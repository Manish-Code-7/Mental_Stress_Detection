{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c2c3abc-b440-449a-b347-c4e6ebcdfd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 20:24:06,819 | INFO | üöÄ Loading dataset for stress detection...\n",
      "2025-11-10 20:24:06,849 | INFO | ‚úÖ Loaded dataset successfully using encoding: utf-8\n",
      "2025-11-10 20:24:06,849 | INFO | üìä Dataset Shape: (2838, 116)\n",
      "2025-11-10 20:24:06,849 | INFO | üìë Columns: ['subreddit', 'post_id', 'sentence_range', 'text', 'id', 'label', 'confidence', 'social_timestamp', 'social_karma', 'syntax_ari', 'lex_liwc_WC', 'lex_liwc_Analytic', 'lex_liwc_Clout', 'lex_liwc_Authentic', 'lex_liwc_Tone', 'lex_liwc_WPS', 'lex_liwc_Sixltr', 'lex_liwc_Dic', 'lex_liwc_function', 'lex_liwc_pronoun', 'lex_liwc_ppron', 'lex_liwc_i', 'lex_liwc_we', 'lex_liwc_you', 'lex_liwc_shehe', 'lex_liwc_they', 'lex_liwc_ipron', 'lex_liwc_article', 'lex_liwc_prep', 'lex_liwc_auxverb', 'lex_liwc_adverb', 'lex_liwc_conj', 'lex_liwc_negate', 'lex_liwc_verb', 'lex_liwc_adj', 'lex_liwc_compare', 'lex_liwc_interrog', 'lex_liwc_number', 'lex_liwc_quant', 'lex_liwc_affect', 'lex_liwc_posemo', 'lex_liwc_negemo', 'lex_liwc_anx', 'lex_liwc_anger', 'lex_liwc_sad', 'lex_liwc_social', 'lex_liwc_family', 'lex_liwc_friend', 'lex_liwc_female', 'lex_liwc_male', 'lex_liwc_cogproc', 'lex_liwc_insight', 'lex_liwc_cause', 'lex_liwc_discrep', 'lex_liwc_tentat', 'lex_liwc_certain', 'lex_liwc_differ', 'lex_liwc_percept', 'lex_liwc_see', 'lex_liwc_hear', 'lex_liwc_feel', 'lex_liwc_bio', 'lex_liwc_body', 'lex_liwc_health', 'lex_liwc_sexual', 'lex_liwc_ingest', 'lex_liwc_drives', 'lex_liwc_affiliation', 'lex_liwc_achieve', 'lex_liwc_power', 'lex_liwc_reward', 'lex_liwc_risk', 'lex_liwc_focuspast', 'lex_liwc_focuspresent', 'lex_liwc_focusfuture', 'lex_liwc_relativ', 'lex_liwc_motion', 'lex_liwc_space', 'lex_liwc_time', 'lex_liwc_work', 'lex_liwc_leisure', 'lex_liwc_home', 'lex_liwc_money', 'lex_liwc_relig', 'lex_liwc_death', 'lex_liwc_informal', 'lex_liwc_swear', 'lex_liwc_netspeak', 'lex_liwc_assent', 'lex_liwc_nonflu', 'lex_liwc_filler', 'lex_liwc_AllPunc', 'lex_liwc_Period', 'lex_liwc_Comma', 'lex_liwc_Colon', 'lex_liwc_SemiC', 'lex_liwc_QMark', 'lex_liwc_Exclam', 'lex_liwc_Dash', 'lex_liwc_Quote', 'lex_liwc_Apostro', 'lex_liwc_Parenth', 'lex_liwc_OtherP', 'lex_dal_max_pleasantness', 'lex_dal_max_activation', 'lex_dal_max_imagery', 'lex_dal_min_pleasantness', 'lex_dal_min_activation', 'lex_dal_min_imagery', 'lex_dal_avg_activation', 'lex_dal_avg_imagery', 'lex_dal_avg_pleasantness', 'social_upvote_ratio', 'social_num_comments', 'syntax_fk_grade', 'sentiment']\n",
      "2025-11-10 20:24:06,865 | INFO | üîç Missing Values: 0 | Duplicates: 0\n",
      "2025-11-10 20:24:06,886 | INFO | üìÅ Dataset profile saved at: preprocessors/dataset_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      " MENTAL STRESS DETECTION - DATA SUMMARY \n",
      "============================================================\n",
      "Shape: (2838, 116)\n",
      "Possible Label Column: subreddit\n",
      "Text Columns: ['post_id', 'sentence_range', 'text']\n",
      "Duplicate Rows: 0\n",
      "Missing Values: 0\n",
      "\n",
      "‚úÖ Dataset successfully loaded and validated. Ready for preprocessing.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# üì¶ STANDARDIZED SETUP - MENTAL STRESS DETECTION PROJECT\n",
    "# ==========================================\n",
    "\n",
    "# --- Core Imports ---\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Visualization (optional for EDA) ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# --- Text & NLP Utilities ---\n",
    "import re\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "\n",
    "# --- Machine Learning ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# --- System Utilities ---\n",
    "import joblib\n",
    "from typing import Dict, Any\n",
    "\n",
    "# --- Warnings ---\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==========================================\n",
    "# üöÄ LOGGING CONFIGURATION\n",
    "# ==========================================\n",
    "LOG_DIR = Path(\"logs\")\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "from logging.handlers import RotatingFileHandler\n",
    "\n",
    "log_file = LOG_DIR / \"stress_detection.log\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    handlers=[\n",
    "        RotatingFileHandler(log_file, maxBytes=1_000_000, backupCount=3),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ==========================================\n",
    "# üóÇÔ∏è DIRECTORY STRUCTURE (MODELS / REPORTS / FRONTEND)\n",
    "# ==========================================\n",
    "for folder in [\"models\", \"preprocessors\", \"reports\", \"artifacts\"]:\n",
    "    Path(folder).mkdir(exist_ok=True)\n",
    "\n",
    "# ==========================================\n",
    "# üß† DATA LOADING FUNCTION\n",
    "# ==========================================\n",
    "def load_dataset(file_path: str, encoding: str = \"utf-8\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load dataset with multiple fallback encodings and detailed validation.\n",
    "    Returns: pandas DataFrame\n",
    "    \"\"\"\n",
    "    encodings = [encoding, \"utf-8\", \"latin-1\", \"cp1252\"]\n",
    "    dataset = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            dataset = pd.read_csv(file_path, encoding=enc, on_bad_lines=\"skip\", low_memory=False)\n",
    "            logger.info(f\"‚úÖ Loaded dataset successfully using encoding: {enc}\")\n",
    "            break\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"‚ùå File not found at {file_path}\")\n",
    "            return None\n",
    "    if dataset is None:\n",
    "        raise ValueError(f\"‚ùå Failed to load dataset using all encodings: {encodings}\")\n",
    "    \n",
    "    logger.info(f\"üìä Dataset Shape: {dataset.shape}\")\n",
    "    logger.info(f\"üìë Columns: {list(dataset.columns)}\")\n",
    "    logger.info(f\"üîç Missing Values: {dataset.isnull().sum().sum()} | Duplicates: {dataset.duplicated().sum()}\")\n",
    "    return dataset\n",
    "\n",
    "# ==========================================\n",
    "# üîé VALIDATION FUNCTION\n",
    "# ==========================================\n",
    "def validate_stress_dataset(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate dataset structure for stress detection tasks.\n",
    "    Identifies potential text and label columns, missing data, duplicates, etc.\n",
    "    \"\"\"\n",
    "    validation = {\n",
    "        \"total_samples\": len(df),\n",
    "        \"missing_values\": df.isnull().sum().to_dict(),\n",
    "        \"duplicate_rows\": int(df.duplicated().sum()),\n",
    "        \"text_columns\": [],\n",
    "        \"label_column\": None,\n",
    "        \"issues\": []\n",
    "    }\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == \"object\":\n",
    "            avg_len = df[col].dropna().astype(str).str.len().mean()\n",
    "            unique_vals = df[col].nunique(dropna=True)\n",
    "\n",
    "            if avg_len > 15 or unique_vals > 30:\n",
    "                validation[\"text_columns\"].append(col)\n",
    "            elif unique_vals <= 10:\n",
    "                validation[\"label_column\"] = col\n",
    "\n",
    "    for text_col in validation[\"text_columns\"]:\n",
    "        short_count = df[text_col].astype(str).str.len().lt(5).sum()\n",
    "        if short_count > 0:\n",
    "            validation[\"issues\"].append(f\"Column '{text_col}' has {short_count} very short entries\")\n",
    "\n",
    "    return validation\n",
    "\n",
    "# ==========================================\n",
    "# üíæ SAVE DATASET PROFILE\n",
    "# ==========================================\n",
    "def save_dataset_profile(df: pd.DataFrame, validation: Dict[str, Any]):\n",
    "    profile = {\n",
    "        \"dataset_info\": {\n",
    "            \"shape\": df.shape,\n",
    "            \"columns\": list(df.columns),\n",
    "            \"text_columns\": validation[\"text_columns\"],\n",
    "            \"label_column\": validation[\"label_column\"],\n",
    "            \"duplicates\": validation[\"duplicate_rows\"],\n",
    "            \"missing\": sum(validation[\"missing_values\"].values())\n",
    "        },\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    config_path = Path(\"preprocessors/dataset_config.json\")\n",
    "    with open(config_path, \"w\") as f:\n",
    "        json.dump(profile, f, indent=2)\n",
    "    logger.info(f\"üìÅ Dataset profile saved at: {config_path}\")\n",
    "    return profile\n",
    "\n",
    "# ==========================================\n",
    "# ‚öôÔ∏è EXECUTION - LOAD AND VALIDATE\n",
    "# ==========================================\n",
    "logger.info(\"üöÄ Loading dataset for stress detection...\")\n",
    "DATA_PATH = \"stress.csv\"   # modify if needed\n",
    "\n",
    "try:\n",
    "    df_raw = load_dataset(DATA_PATH)\n",
    "    if df_raw is None:\n",
    "        raise FileNotFoundError(f\"Dataset not found at {DATA_PATH}. Please ensure it's placed in the project root.\")\n",
    "        \n",
    "    df = df_raw.copy()\n",
    "    validation = validate_stress_dataset(df)\n",
    "    profile = save_dataset_profile(df, validation)\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\" MENTAL STRESS DETECTION - DATA SUMMARY \")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Possible Label Column: {validation['label_column']}\")\n",
    "    print(f\"Text Columns: {validation['text_columns']}\")\n",
    "    print(f\"Duplicate Rows: {validation['duplicate_rows']}\")\n",
    "    print(f\"Missing Values: {sum(validation['missing_values'].values())}\")\n",
    "    if validation['issues']:\n",
    "        print(\"\\n‚ö†Ô∏è Issues Found:\")\n",
    "        for issue in validation['issues']:\n",
    "            print(f\"  - {issue}\")\n",
    "    print(\"\\n‚úÖ Dataset successfully loaded and validated. Ready for preprocessing.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"‚ùå Data loading failed: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5dd8c1e-5e2e-4d76-b8bb-6ad959456b94",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.pyplot' has no attribute 'use'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m plt\u001b[38;5;241m.\u001b[39muse(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAgg\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# ‚úÖ Required for Docker/frontend\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# üß≠ SMART COLUMN DETECTION\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_text_column\u001b[39m(df: pd\u001b[38;5;241m.\u001b[39mDataFrame):\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib.pyplot' has no attribute 'use'"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# üåø ADVANCED PROJECT-SPECIFIC EDA ‚Äì MENTAL STRESS DETECTION\n",
    "# ===============================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "plt.use('Agg')  # ‚úÖ Required for Docker/frontend\n",
    "\n",
    "# ------------------------------------------------\n",
    "# üß≠ SMART COLUMN DETECTION\n",
    "# ------------------------------------------------\n",
    "def get_text_column(df: pd.DataFrame):\n",
    "    candidates = [\"clean_text\", \"text\", \"message\", \"content\", \"post\", \"body\"]\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    text_like = [c for c in df.columns if df[c].dtype == \"object\"]\n",
    "    return text_like[0] if text_like else None\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# üß† SENTIMENT & EMOTION ANALYSIS VISUALS\n",
    "# ------------------------------------------------\n",
    "def sentiment_summary(df, text_col):\n",
    "    df[\"sentiment_polarity\"] = df[text_col].astype(str).apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    df[\"sentiment_subjectivity\"] = df[text_col].astype(str).apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.histplot(df[\"sentiment_polarity\"], kde=True, bins=40, color=\"purple\")\n",
    "    plt.title(\"üí¨ Sentiment Polarity Distribution (-1 ‚Üí +1)\")\n",
    "    plt.xlabel(\"Polarity\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"reports/sentiment_polarity.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.histplot(df[\"sentiment_subjectivity\"], kde=True, bins=40, color=\"orange\")\n",
    "    plt.title(\"üß≠ Sentiment Subjectivity (0 Objective ‚Üí 1 Subjective)\")\n",
    "    plt.xlabel(\"Subjectivity\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"reports/sentiment_subjectivity.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    return {\n",
    "        \"avg_polarity\": round(df[\"sentiment_polarity\"].mean(), 4),\n",
    "        \"avg_subjectivity\": round(df[\"sentiment_subjectivity\"].mean(), 4),\n",
    "    }\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# üí¨ EMOTIONAL WORD INTENSITY VISUALIZATION\n",
    "# ------------------------------------------------\n",
    "def emotion_intensity_map(df, text_col):\n",
    "    from textblob import Word\n",
    "    emotion_words = [\"happy\", \"sad\", \"angry\", \"tired\", \"worried\", \"relaxed\", \"peaceful\", \"anxious\", \"overwhelmed\", \"calm\"]\n",
    "    counts = {w: 0 for w in emotion_words}\n",
    "    for text in df[text_col].astype(str):\n",
    "        for w in emotion_words:\n",
    "            if w in text.lower():\n",
    "                counts[w] += 1\n",
    "    emo_df = pd.DataFrame(list(counts.items()), columns=[\"emotion\", \"count\"])\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    sns.barplot(x=\"count\", y=\"emotion\", data=emo_df, palette=\"coolwarm\")\n",
    "    plt.title(\"üí° Emotion Word Frequency Map\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"reports/emotion_intensity.png\", dpi=300)\n",
    "    plt.close()\n",
    "    return emo_df\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# üßÆ WORD COUNT & COMPLEXITY\n",
    "# ------------------------------------------------\n",
    "def text_complexity_analysis(df, text_col):\n",
    "    df[\"word_count\"] = df[text_col].astype(str).apply(lambda x: len(x.split()))\n",
    "    df[\"avg_word_length\"] = df[text_col].astype(str).apply(lambda x: np.mean([len(w) for w in x.split()]) if len(x.split())>0 else 0)\n",
    "    df[\"punctuation_density\"] = df[text_col].astype(str).apply(lambda x: sum([1 for c in x if c in \"!?.\"]) / max(len(x),1))\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    sns.histplot(df[\"word_count\"], bins=40, ax=axs[0], color=\"teal\")\n",
    "    axs[0].set_title(\"üìò Word Count Distribution\")\n",
    "\n",
    "    sns.histplot(df[\"avg_word_length\"], bins=40, ax=axs[1], color=\"olive\")\n",
    "    axs[1].set_title(\"üßÆ Avg Word Length Distribution\")\n",
    "\n",
    "    sns.histplot(df[\"punctuation_density\"], bins=40, ax=axs[2], color=\"crimson\")\n",
    "    axs[2].set_title(\"üî£ Punctuation Density\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"reports/text_complexity.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    return df[[\"word_count\",\"avg_word_length\",\"punctuation_density\"]].describe().to_dict()\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# üî° MOST COMMON WORDS BY CLASS\n",
    "# ------------------------------------------------\n",
    "def most_common_words_by_class(df, label_col, text_col, n=15):\n",
    "    if label_col not in df.columns:\n",
    "        return None\n",
    "    df[label_col] = df[label_col].astype(str)\n",
    "    class_labels = df[label_col].unique()\n",
    "    all_freqs = {}\n",
    "    for lbl in class_labels:\n",
    "        subset = df[df[label_col]==lbl]\n",
    "        words = \" \".join(subset[text_col].astype(str)).lower().split()\n",
    "        common = Counter(words).most_common(n)\n",
    "        all_freqs[lbl] = common\n",
    "        common_df = pd.DataFrame(common, columns=[\"word\", \"freq\"])\n",
    "        plt.figure(figsize=(8,5))\n",
    "        sns.barplot(y=\"word\", x=\"freq\", data=common_df, palette=\"viridis\")\n",
    "        plt.title(f\"üó£Ô∏è Top {n} Words for '{lbl}' Class\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"reports/top_words_{lbl}.png\", dpi=300)\n",
    "        plt.close()\n",
    "    return all_freqs\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# üß© LABEL BALANCE & CORRELATIONS\n",
    "# ------------------------------------------------\n",
    "def plot_label_balance(df, label_col=\"label\"):\n",
    "    if label_col not in df.columns: return None\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.countplot(x=label_col, data=df, palette=\"pastel\")\n",
    "    plt.title(\"‚öñÔ∏è Label Balance\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"reports/label_balance.png\", dpi=300)\n",
    "    plt.close()\n",
    "    return \"reports/label_balance.png\"\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# üîÅ FINAL EXECUTION FOR EDA\n",
    "# ------------------------------------------------\n",
    "print(\"=\"*80)\n",
    "print(\"üåø Running ADVANCED EDA for MENTAL STRESS DETECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "text_col = get_text_column(df)\n",
    "label_col = \"label\" if \"label\" in df.columns else df.columns[-1]\n",
    "\n",
    "print(f\"‚úÖ Using text column: {text_col}, label column: {label_col}\")\n",
    "\n",
    "sent_summary = sentiment_summary(df, text_col)\n",
    "emo_map = emotion_intensity_map(df, text_col)\n",
    "complexity_stats = text_complexity_analysis(df, text_col)\n",
    "label_plot = plot_label_balance(df, label_col)\n",
    "word_freqs = most_common_words_by_class(df, label_col, text_col)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# üìä EXPORT JSON FOR FRONTEND DASHBOARD\n",
    "# ------------------------------------------------\n",
    "eda_summary = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"sentiment_summary\": sent_summary,\n",
    "    \"complexity_stats\": complexity_stats,\n",
    "    \"emotion_counts\": emo_map.to_dict(),\n",
    "    \"top_words_by_class\": word_freqs,\n",
    "}\n",
    "with open(\"reports/advanced_eda_summary.json\", \"w\") as f:\n",
    "    json.dump(eda_summary, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úÖ Advanced EDA Completed\")\n",
    "print(\"üìÅ Visual reports saved in 'reports/'\")\n",
    "print(\"üß† Ready for frontend integration or deployment container.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcea8e1b-cf90-4ca0-80d1-cc1814cb9d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† ORIGINAL: I'm feeling soooo stressed üò© about my exams and deadlines!!! Can't sleep at all.\n",
      "üîß CLEANED: feel soooo stress weary face exam deadline sleep stress_focus\n",
      "üìä FEATURES: {'char_len': 80, 'word_count': 14, 'avg_word_len': 4.785714285714286, 'sentiment_polarity': 0.0, 'sentiment_subjectivity': 0.0, 'stress_word_count': 0}\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# üß† ENHANCED NLP PREPROCESSING - MENTAL STRESS DETECTION\n",
    "# ==========================================\n",
    "\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Ensure NLTK assets are available (for Docker environments)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ‚öôÔ∏è HELPER FUNCTIONS\n",
    "# ------------------------------------------------------------\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"Map POS tag to WordNet format.\"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# üß© ADVANCED TEXT PREPROCESSOR CLASS\n",
    "# ------------------------------------------------------------\n",
    "class AdvancedTextPreprocessor:\n",
    "    \"\"\"\n",
    "    Handles advanced text preprocessing for stress detection.\n",
    "    Includes normalization, lemmatization, emoji decoding,\n",
    "    and stress-aware keyword augmentation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, extra_stopwords=None):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        if extra_stopwords:\n",
    "            self.stop_words.update(extra_stopwords)\n",
    "\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stress_keywords = [\n",
    "            \"stress\", \"anxiety\", \"depression\", \"panic\", \"fear\",\n",
    "            \"pressure\", \"worry\", \"burnout\", \"tired\", \"sad\", \"angry\",\n",
    "            \"hopeless\", \"calm\", \"peaceful\", \"happy\", \"relaxed\"\n",
    "        ]\n",
    "\n",
    "        # Contraction map\n",
    "        self.contraction_map = {\n",
    "            \"can't\": \"cannot\", \"won't\": \"will not\", \"n't\": \" not\",\n",
    "            \"'re\": \" are\", \"'s\": \" is\", \"'d\": \" would\",\n",
    "            \"'ll\": \" will\", \"'t\": \" not\", \"'ve\": \" have\", \"'m\": \" am\"\n",
    "        }\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    def clean_contractions(self, text):\n",
    "        for c, expanded in self.contraction_map.items():\n",
    "            text = text.replace(c, expanded)\n",
    "        return text\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    def handle_negations(self, text):\n",
    "        \"\"\"\n",
    "        Keep negations close to context words\n",
    "        (e.g., 'not happy' ‚Üí 'not_happy')\n",
    "        \"\"\"\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        negation_words = {'not', 'no', 'never', \"n't\"}\n",
    "        new_tokens = []\n",
    "        skip_next = False\n",
    "        for i in range(len(tokens) - 1):\n",
    "            if tokens[i].lower() in negation_words and tokens[i + 1].isalpha():\n",
    "                new_tokens.append(tokens[i] + \"_\" + tokens[i + 1])\n",
    "                skip_next = True\n",
    "            elif not skip_next:\n",
    "                new_tokens.append(tokens[i])\n",
    "            else:\n",
    "                skip_next = False\n",
    "        if not skip_next:\n",
    "            new_tokens.append(tokens[-1])\n",
    "        return \" \".join(new_tokens)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    def replace_emojis(self, text):\n",
    "        \"\"\"Convert emojis to text descriptions (üôÇ ‚Üí smiley_face).\"\"\"\n",
    "        return emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    def remove_noise(self, text):\n",
    "        \"\"\"Remove URLs, mentions, hashtags, numbers, and symbols.\"\"\"\n",
    "        text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "        text = re.sub(r\"@\\w+|#\\w+\", \"\", text)\n",
    "        text = re.sub(r\"\\d+\", \"\", text)\n",
    "        text = re.sub(r\"[%s]\" % re.escape(string.punctuation), \" \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        return text.strip()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    def lemmatize_text(self, text):\n",
    "        \"\"\"POS-based lemmatization.\"\"\"\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        pos_tags = nltk.pos_tag(tokens)\n",
    "        lemmatized = [\n",
    "            self.lemmatizer.lemmatize(word.lower(), get_wordnet_pos(pos))\n",
    "            for word, pos in pos_tags\n",
    "        ]\n",
    "        return \" \".join(lemmatized)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    def add_stress_boost(self, text):\n",
    "        \"\"\"\n",
    "        Enrich text by amplifying known stress indicators.\n",
    "        Adds a small frequency boost to stress-related words.\n",
    "        \"\"\"\n",
    "        for kw in self.stress_keywords:\n",
    "            if kw in text:\n",
    "                text += f\" {kw}_focus\"\n",
    "        return text\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    def clean_text_advanced(self, text):\n",
    "        \"\"\"Apply full pipeline: normalize ‚Üí clean ‚Üí lemmatize.\"\"\"\n",
    "        text = str(text).lower()\n",
    "        text = unicodedata.normalize(\"NFKD\", text)\n",
    "        text = self.clean_contractions(text)\n",
    "        text = self.replace_emojis(text)\n",
    "        text = self.handle_negations(text)\n",
    "        text = self.remove_noise(text)\n",
    "        text = self.lemmatize_text(text)\n",
    "        text = self.add_stress_boost(text)\n",
    "        tokens = [w for w in text.split() if w not in self.stop_words and len(w) > 2]\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    def extract_advanced_features(self, text):\n",
    "        \"\"\"Extract linguistic & sentiment features for ML models.\"\"\"\n",
    "        blob = TextBlob(str(text))\n",
    "        return {\n",
    "            \"char_len\": len(text),\n",
    "            \"word_count\": len(text.split()),\n",
    "            \"avg_word_len\": np.mean([len(w) for w in text.split()]) if text else 0,\n",
    "            \"sentiment_polarity\": blob.sentiment.polarity,\n",
    "            \"sentiment_subjectivity\": blob.sentiment.subjectivity,\n",
    "            \"stress_word_count\": sum(1 for w in text.split() if w in self.stress_keywords)\n",
    "        }\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ‚úÖ TEST THE PREPROCESSOR PIPELINE\n",
    "# ------------------------------------------------------------\n",
    "sample_text = \"I'm feeling soooo stressed üò© about my exams and deadlines!!! Can't sleep at all.\"\n",
    "pre = AdvancedTextPreprocessor()\n",
    "print(\"\\nüß† ORIGINAL:\", sample_text)\n",
    "print(\"üîß CLEANED:\", pre.clean_text_advanced(sample_text))\n",
    "print(\"üìä FEATURES:\", pre.extract_advanced_features(sample_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37799ce-3297-479a-aa5b-9b2310db8c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f7ef739-2cc5-435e-b94f-653720201461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ spaCy model 'en_core_web_sm' loaded successfully.\n",
      "üñ•Ô∏è  Compute Device: CPU only\n",
      "\n",
      "üìÇ Verified project structure:\n",
      "   ‚îî‚îÄ‚îÄ models/ ‚úÖ\n",
      "   ‚îî‚îÄ‚îÄ preprocessors/ ‚úÖ\n",
      "   ‚îî‚îÄ‚îÄ reports/ ‚úÖ\n",
      "   ‚îî‚îÄ‚îÄ artifacts/ ‚úÖ\n",
      "üí¨ TextBlob polarity test: -0.25\n",
      "üóÇÔ∏è Stopwords loaded: 198 words\n",
      "\n",
      "üì¶ ENVIRONMENT SUMMARY\n",
      "========================================\n",
      "python_version      : 3.12.2\n",
      "system              : Darwin\n",
      "device_info         : {'gpu_available': False, 'device_name': 'CPU Mode'}\n",
      "nltk_version        : 3.9.1\n",
      "spacy_version       : 3.8.7\n",
      "textblob_version    : 0.19.0\n",
      "emoji_version       : 2.15.0\n",
      "========================================\n",
      "‚úÖ NLP environment fully configured and deployment-ready!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# ‚öôÔ∏è ENVIRONMENT SETUP - NLP DEPENDENCIES\n",
    "# ==========================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import importlib\n",
    "import platform\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# üß© UTILITY: Safe Import with Auto-Install\n",
    "# ------------------------------------------------\n",
    "def install_and_import(package_name, import_name=None):\n",
    "    \"\"\"Safely import a package, auto-install if missing.\"\"\"\n",
    "    try:\n",
    "        return importlib.import_module(import_name or package_name)\n",
    "    except ImportError:\n",
    "        print(f\"üì¶ Installing missing package: {package_name} ...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
    "        return importlib.import_module(import_name or package_name)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# üß† CORE NLP LIBRARIES (Auto-managed)\n",
    "# ------------------------------------------------\n",
    "nltk = install_and_import(\"nltk\")\n",
    "spacy = install_and_import(\"spacy\")\n",
    "emoji = install_and_import(\"emoji\")\n",
    "textblob = install_and_import(\"textblob\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# üî§ SPACY MODEL SETUP\n",
    "# ------------------------------------------------\n",
    "def load_spacy_model(model_name=\"en_core_web_sm\"):\n",
    "    \"\"\"Ensure spaCy English model is available.\"\"\"\n",
    "    try:\n",
    "        nlp = spacy.load(model_name)\n",
    "        print(f\"‚úÖ spaCy model '{model_name}' loaded successfully.\")\n",
    "    except OSError:\n",
    "        print(f\"‚öôÔ∏è Downloading spaCy model: {model_name} ...\")\n",
    "        subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", model_name], check=True)\n",
    "        nlp = spacy.load(model_name)\n",
    "    return nlp\n",
    "\n",
    "nlp = load_spacy_model(\"en_core_web_sm\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# ‚ö° GPU / CPU CHECK\n",
    "# ------------------------------------------------\n",
    "def check_gpu_status():\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    gpu_name = torch.cuda.get_device_name(0) if gpu_available else \"CPU Mode\"\n",
    "    print(f\"üñ•Ô∏è  Compute Device: {'GPU - ' + gpu_name if gpu_available else 'CPU only'}\")\n",
    "    return {\"gpu_available\": gpu_available, \"device_name\": gpu_name}\n",
    "\n",
    "device_info = check_gpu_status()\n",
    "\n",
    "# ------------------------------------------------\n",
    "# üìÅ PROJECT DIRECTORY VALIDATION\n",
    "# ------------------------------------------------\n",
    "for folder in [\"models\", \"preprocessors\", \"reports\", \"artifacts\"]:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "print(\"\\nüìÇ Verified project structure:\")\n",
    "for f in [\"models\", \"preprocessors\", \"reports\", \"artifacts\"]:\n",
    "    print(f\"   ‚îî‚îÄ‚îÄ {f}/ ‚úÖ\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# üß† TEXTBLOB / NLTK SANITY CHECK\n",
    "# ------------------------------------------------\n",
    "try:\n",
    "    from textblob import TextBlob\n",
    "    from nltk.corpus import stopwords\n",
    "    nltk.download(\"punkt\", quiet=True)\n",
    "    nltk.download(\"stopwords\", quiet=True)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    sample_blob = TextBlob(\"I am feeling a bit anxious about tomorrow.\")\n",
    "    print(f\"üí¨ TextBlob polarity test: {sample_blob.sentiment.polarity:.2f}\")\n",
    "    print(f\"üóÇÔ∏è Stopwords loaded: {len(stop_words)} words\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error verifying NLP dependencies: {e}\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# üîç ENVIRONMENT SUMMARY (Safe Version Fetch)\n",
    "# ------------------------------------------------\n",
    "from importlib.metadata import version, PackageNotFoundError\n",
    "\n",
    "def get_pkg_version(pkg_name, fallback=\"Unknown\"):\n",
    "    \"\"\"Safely fetch package version even if __version__ missing.\"\"\"\n",
    "    try:\n",
    "        return version(pkg_name)\n",
    "    except PackageNotFoundError:\n",
    "        try:\n",
    "            pkg = importlib.import_module(pkg_name)\n",
    "            return getattr(pkg, \"__version__\", fallback)\n",
    "        except Exception:\n",
    "            return fallback\n",
    "\n",
    "env_summary = {\n",
    "    \"python_version\": platform.python_version(),\n",
    "    \"system\": platform.system(),\n",
    "    \"device_info\": device_info,\n",
    "    \"nltk_version\": get_pkg_version(\"nltk\"),\n",
    "    \"spacy_version\": get_pkg_version(\"spacy\"),\n",
    "    \"textblob_version\": get_pkg_version(\"textblob\"),\n",
    "    \"emoji_version\": get_pkg_version(\"emoji\"),\n",
    "}\n",
    "\n",
    "print(\"\\nüì¶ ENVIRONMENT SUMMARY\")\n",
    "print(\"=\" * 40)\n",
    "for key, val in env_summary.items():\n",
    "    print(f\"{key:20s}: {val}\")\n",
    "print(\"=\" * 40)\n",
    "print(\"‚úÖ NLP environment fully configured and deployment-ready!\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ac130e-f7cb-4d38-a6ee-15beca4941a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d17899d0-7f77-4bb2-a49f-f1a62e2fe9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Preprocessing 4 texts using 4 threads...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 132.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Sample Output\n",
      "\n",
      "üß† Original: I‚Äôm completely stressed about my project deadlines üò©\n",
      "üîß Cleaned:  work pressure unbearable manage\n",
      "üìä Features: {'char_len': 31, 'word_count': 4, 'sentiment': 0.0, 'subjectivity': 0.0, 'stress_words': 1}\n",
      "\n",
      "üß† Original: Feeling super calm and relaxed after yoga üßò\n",
      "üîß Cleaned:  anxiety level go day exam\n",
      "üìä Features: {'char_len': 25, 'word_count': 5, 'sentiment': 0.0, 'subjectivity': 0.0, 'stress_words': 1}\n",
      "\n",
      "üß† Original: Anxiety levels are going up every day with these exams!\n",
      "üîß Cleaned:  completely stress project deadline weary face\n",
      "üìä Features: {'char_len': 45, 'word_count': 6, 'sentiment': 0.1, 'subjectivity': 0.4, 'stress_words': 1}\n",
      "\n",
      "üß† Original: Work pressure is unbearable, but I‚Äôll manage somehow.\n",
      "üîß Cleaned:  feel super calm relax yoga person lotus position\n",
      "üìä Features: {'char_len': 48, 'word_count': 8, 'sentiment': 0.31666666666666665, 'subjectivity': 0.7083333333333333, 'stress_words': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# ‚öôÔ∏è OPTIMIZED HIGH-PERFORMANCE TEXT PREPROCESSOR\n",
    "# ==========================================\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from textblob import TextBlob\n",
    "import emoji\n",
    "import nltk\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ‚úÖ Ensure spacy model loaded\n",
    "# ---------------------------------------------------------\n",
    "try:\n",
    "    nlp\n",
    "except NameError:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# üîß Optimized Preprocessor Class\n",
    "# ---------------------------------------------------------\n",
    "class HighPerformancePreprocessor:\n",
    "    \"\"\"spaCy + cache accelerated text preprocessor for large-scale inference.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "        self.stress_lexicon = {\n",
    "            \"stress\", \"anxiety\", \"panic\", \"pressure\", \"fear\", \"depression\",\n",
    "            \"tension\", \"worry\", \"sad\", \"angry\", \"hopeless\", \"relaxed\",\n",
    "            \"peaceful\", \"calm\", \"happy\", \"grateful\"\n",
    "        }\n",
    "        self.stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    def normalize_text(self, text):\n",
    "        text = emoji.demojize(str(text).lower(), delimiters=(\" \", \" \"))\n",
    "        text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "        text = re.sub(r\"@\\w+|#\\w+\", \"\", text)\n",
    "        text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        return text\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    def process_doc(self, text):\n",
    "        if text in self.cache:\n",
    "            return self.cache[text]\n",
    "        doc = nlp(text)\n",
    "        tokens = [\n",
    "            tok.lemma_.lower()\n",
    "            for tok in doc\n",
    "            if not tok.is_stop and tok.is_alpha and len(tok) > 2\n",
    "        ]\n",
    "        cleaned = \" \".join(tokens)\n",
    "        self.cache[text] = cleaned\n",
    "        return cleaned\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    def extract_features(self, text):\n",
    "        blob = TextBlob(text)\n",
    "        return {\n",
    "            \"char_len\": len(text),\n",
    "            \"word_count\": len(text.split()),\n",
    "            \"sentiment\": blob.sentiment.polarity,\n",
    "            \"subjectivity\": blob.sentiment.subjectivity,\n",
    "            \"stress_words\": sum(w in self.stress_lexicon for w in text.split())\n",
    "        }\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    def batch_process(self, texts, max_workers=4):\n",
    "        cleaned, features = [], []\n",
    "        print(f\"üöÄ Preprocessing {len(texts)} texts using {max_workers} threads...\")\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_text = {executor.submit(self.pipeline, t): t for t in texts}\n",
    "            for future in tqdm(as_completed(future_to_text), total=len(future_to_text)):\n",
    "                clean_text, feats = future.result()\n",
    "                cleaned.append(clean_text)\n",
    "                features.append(feats)\n",
    "\n",
    "        return cleaned, features\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    def pipeline(self, text):\n",
    "        text = self.normalize_text(text)\n",
    "        clean_text = self.process_doc(text)\n",
    "        feats = self.extract_features(clean_text)\n",
    "        return clean_text, feats\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ‚úÖ Test Run (example)\n",
    "# ---------------------------------------------------------\n",
    "sample_batch = [\n",
    "    \"I‚Äôm completely stressed about my project deadlines üò©\",\n",
    "    \"Feeling super calm and relaxed after yoga üßò\",\n",
    "    \"Anxiety levels are going up every day with these exams!\",\n",
    "    \"Work pressure is unbearable, but I‚Äôll manage somehow.\",\n",
    "]\n",
    "\n",
    "hp = HighPerformancePreprocessor()\n",
    "cleaned, feats = hp.batch_process(sample_batch, max_workers=4)\n",
    "\n",
    "print(\"\\nüîç Sample Output\")\n",
    "for i in range(len(sample_batch)):\n",
    "    print(f\"\\nüß† Original: {sample_batch[i]}\")\n",
    "    print(f\"üîß Cleaned:  {cleaned[i]}\")\n",
    "    print(f\"üìä Features: {feats[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b95b85ab-4c09-4655-b15d-54e7b3f20597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created 10 vectorizers:\n",
      "  - bow_binary\n",
      "  - bow_freq\n",
      "  - tfidf_unigram\n",
      "  - tfidf_bigram\n",
      "  - tfidf_char\n",
      "  - hybrid_char_word\n",
      "  - mental_health_vocab\n",
      "  - hashing\n",
      "  - sentence_embeddings\n",
      "  - ensemble_vectorizer\n",
      "üìÅ Saved vectorizer metadata ‚Üí preprocessors/vectorizer_index.json\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# üß© CELL 6 ‚Äî EXPANDED ADVANCED VECTORIZER CREATOR\n",
    "# ==========================================\n",
    "\n",
    "import os, json, numpy as np\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    ")\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# optional transformer embeddings\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    SBT_AVAILABLE = True\n",
    "except Exception:\n",
    "    SBT_AVAILABLE = False\n",
    "\n",
    "# spaCy embeddings fallback\n",
    "try:\n",
    "    _ = nlp\n",
    "    SPACY_AVAILABLE = True\n",
    "except Exception:\n",
    "    SPACY_AVAILABLE = False\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# üî† Mental-health vocabulary builder\n",
    "# ---------------------------------------------------------\n",
    "def build_mental_health_vocab(extra_terms=None):\n",
    "    vocab = {\n",
    "        \"stress\",\"stressed\",\"anxiety\",\"anxious\",\"panic\",\"pressure\",\"tension\",\n",
    "        \"depression\",\"depressed\",\"hopeless\",\"helpless\",\"sad\",\"cry\",\"worry\",\n",
    "        \"worried\",\"fear\",\"afraid\",\"angry\",\"frustrated\",\"burnout\",\"tired\",\n",
    "        \"exhausted\",\"insomnia\",\"sleep\",\"therapy\",\"counseling\",\"help\",\"relief\",\n",
    "        \"relaxed\",\"calm\",\"peace\",\"happy\",\"joy\",\"gratitude\"\n",
    "    }\n",
    "    if extra_terms: vocab.update(set(extra_terms))\n",
    "    return sorted(vocab)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# üß† Embedding wrapper (SentenceTransformer / spaCy)\n",
    "# ---------------------------------------------------------\n",
    "class SentenceEmbeddingVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\"):\n",
    "        self.model_name, self.model, self.backend = model_name, None, None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if SBT_AVAILABLE:\n",
    "            try:\n",
    "                self.model = SentenceTransformer(self.model_name)\n",
    "                self.backend = \"sentence-transformers\"\n",
    "            except Exception:\n",
    "                self.model = None\n",
    "        if self.model is None and SPACY_AVAILABLE:\n",
    "            self.model, self.backend = nlp, \"spacy\"\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"No embedding backend available.\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.backend == \"sentence-transformers\":\n",
    "            return self.model.encode(list(X), show_progress_bar=False)\n",
    "        elif self.backend == \"spacy\":\n",
    "            return np.vstack([doc.vector for doc in self.model.pipe(list(X), disable=[\"parser\",\"ner\"])])\n",
    "        else:\n",
    "            raise RuntimeError(\"No backend initialized.\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ‚öôÔ∏è Build feature unions / specialized vectorizers\n",
    "# ---------------------------------------------------------\n",
    "def build_hybrid_char_word():\n",
    "    return FeatureUnion([\n",
    "        ('word_tfidf', TfidfVectorizer(max_features=8000, ngram_range=(1,2),\n",
    "                                       min_df=2, stop_words=\"english\")),\n",
    "        ('char_tfidf', TfidfVectorizer(max_features=4000, analyzer='char_wb',\n",
    "                                       ngram_range=(3,5), min_df=2))\n",
    "    ])\n",
    "\n",
    "def mental_health_tfidf(vocab=None):\n",
    "    if vocab is None: vocab = build_mental_health_vocab()\n",
    "    return TfidfVectorizer(vocabulary=vocab, ngram_range=(1,2), sublinear_tf=True)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ‚öôÔ∏è Ensemble wrapper (combine sparse + dense)\n",
    "# ---------------------------------------------------------\n",
    "class EnsembleVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Combine a sparse TF-IDF and dense embedding space.\"\"\"\n",
    "    def __init__(self, word_tfidf=None, embed_vec=None):\n",
    "        self.word_tfidf = word_tfidf or TfidfVectorizer(max_features=10000)\n",
    "        self.embed_vec = embed_vec or SentenceEmbeddingVectorizer()\n",
    "        self.scaler = StandardScaler(with_mean=False)\n",
    "    def fit(self, X, y=None):\n",
    "        self.word_tfidf.fit(X)\n",
    "        self.embed_vec.fit(X)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        tfidf_matrix = self.word_tfidf.transform(X)\n",
    "        dense_matrix = self.embed_vec.transform(X)\n",
    "        dense_scaled = self.scaler.fit_transform(dense_matrix)\n",
    "        # horizontally stack sparse + dense\n",
    "        from scipy.sparse import hstack, csr_matrix\n",
    "        return hstack([tfidf_matrix, csr_matrix(dense_scaled)])\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# üìö Create complete vectorizer suite\n",
    "# ---------------------------------------------------------\n",
    "def create_advanced_vectorizers(extra_vocab_terms=None):\n",
    "    vocab = build_mental_health_vocab(extra_vocab_terms)\n",
    "    vects = {\n",
    "        # Bag-of-Words\n",
    "        \"bow_binary\": CountVectorizer(max_features=10000, binary=True, ngram_range=(1,2)),\n",
    "        \"bow_freq\": CountVectorizer(max_features=12000, binary=False, ngram_range=(1,2)),\n",
    "\n",
    "        # TF-IDF variants\n",
    "        \"tfidf_unigram\": TfidfVectorizer(max_features=10000, ngram_range=(1,1), sublinear_tf=True),\n",
    "        \"tfidf_bigram\":  TfidfVectorizer(max_features=15000, ngram_range=(1,2), sublinear_tf=True),\n",
    "        \"tfidf_char\":    TfidfVectorizer(max_features=8000, analyzer='char_wb', ngram_range=(3,5)),\n",
    "\n",
    "        # Hybrid & vocab\n",
    "        \"hybrid_char_word\": build_hybrid_char_word(),\n",
    "        \"mental_health_vocab\": mental_health_tfidf(vocab),\n",
    "\n",
    "        # Hashing\n",
    "        \"hashing\": HashingVectorizer(n_features=2**15, alternate_sign=False, norm=None)\n",
    "    }\n",
    "\n",
    "    # Add embeddings if available\n",
    "    if SBT_AVAILABLE or SPACY_AVAILABLE:\n",
    "        vects[\"sentence_embeddings\"] = SentenceEmbeddingVectorizer()\n",
    "\n",
    "    # Add ensemble combination\n",
    "    vects[\"ensemble_vectorizer\"] = EnsembleVectorizer()\n",
    "\n",
    "    return vects\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# üöÄ Build & summarize\n",
    "# ---------------------------------------------------------\n",
    "vectorizers = create_advanced_vectorizers()\n",
    "print(f\"‚úÖ Created {len(vectorizers)} vectorizers:\")\n",
    "for k in vectorizers: print(f\"  - {k}\")\n",
    "\n",
    "meta = {\n",
    "    \"vectorizers\": list(vectorizers.keys()),\n",
    "    \"sentence_transformers\": SBT_AVAILABLE,\n",
    "    \"spacy_available\": SPACY_AVAILABLE,\n",
    "    \"mental_health_vocab_size\": len(build_mental_health_vocab())\n",
    "}\n",
    "os.makedirs(\"preprocessors\", exist_ok=True)\n",
    "with open(\"preprocessors/vectorizer_index.json\", \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "print(\"üìÅ Saved vectorizer metadata ‚Üí preprocessors/vectorizer_index.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f53f6b9-e6dc-4b0e-808a-a88453364cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model suite initialized (lazy loading)\n",
      "üìã Classical models: 8\n",
      "üß† Deep models: 3\n",
      "\n",
      "üí° Usage:\n",
      "   model = get_model('LogisticRegression')\n",
      "   model = build_ann(input_dim=512)\n",
      "\n",
      "‚ö†Ô∏è  Models are created on-demand to prevent memory issues\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# ü§ñ MEMORY-SAFE MODEL SUITE\n",
    "# ==========================================\n",
    "# Load models on-demand to prevent kernel crash\n",
    "\n",
    "import warnings, os, numpy as np\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, ExtraTreesClassifier, \n",
    "    GradientBoostingClassifier, AdaBoostClassifier\n",
    ")\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "# ------------------------------\n",
    "# 1Ô∏è‚É£ Classical ML models (LAZY LOADING)\n",
    "# ------------------------------\n",
    "def get_model(model_name):\n",
    "    \"\"\"Get a single model on-demand to save memory\"\"\"\n",
    "    models_config = {\n",
    "        \"LogisticRegression\": lambda: LogisticRegression(\n",
    "            max_iter=2000, C=1.0, class_weight='balanced', random_state=42\n",
    "        ),\n",
    "        \"SVM_Linear\": lambda: LinearSVC(C=1.0, max_iter=2000, random_state=42),\n",
    "        \"RandomForest\": lambda: RandomForestClassifier(\n",
    "            n_estimators=300, max_depth=20, random_state=42, n_jobs=-1\n",
    "        ),\n",
    "        \"ExtraTrees\": lambda: ExtraTreesClassifier(\n",
    "            n_estimators=300, random_state=42, n_jobs=-1\n",
    "        ),\n",
    "        \"GradientBoosting\": lambda: GradientBoostingClassifier(\n",
    "            n_estimators=150, learning_rate=0.1, random_state=42\n",
    "        ),\n",
    "        \"AdaBoost\": lambda: AdaBoostClassifier(\n",
    "            n_estimators=100, learning_rate=1.0, random_state=42\n",
    "        ),\n",
    "        \"NaiveBayes\": lambda: MultinomialNB(alpha=0.1),\n",
    "        \"MLPClassifier\": lambda: MLPClassifier(\n",
    "            hidden_layer_sizes=(128, 64), activation='relu', \n",
    "            max_iter=500, random_state=42\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    if model_name not in models_config:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "    \n",
    "    return models_config[model_name]()\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 2Ô∏è‚É£ Deep Neural Models (ON-DEMAND)\n",
    "# ------------------------------\n",
    "def build_ann(input_dim):\n",
    "    \"\"\"Build ANN only when needed\"\"\"\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.keras.models import Sequential\n",
    "        from tensorflow.keras.layers import Dense, Dropout\n",
    "        from tensorflow.keras.optimizers import Adam\n",
    "        \n",
    "        # Clear previous sessions\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        model = Sequential([\n",
    "            Dense(512, activation='relu', input_dim=input_dim),\n",
    "            Dropout(0.4),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(\n",
    "            optimizer=Adam(1e-3), \n",
    "            loss='binary_crossentropy', \n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è TensorFlow not installed\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def build_cnn(vocab_size=10000, max_len=100, embed_dim=128):\n",
    "    \"\"\"Build CNN only when needed\"\"\"\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.keras.models import Sequential\n",
    "        from tensorflow.keras.layers import (\n",
    "            Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "        )\n",
    "        from tensorflow.keras.optimizers import Adam\n",
    "        \n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        model = Sequential([\n",
    "            Embedding(vocab_size, embed_dim, input_length=max_len),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            GlobalMaxPooling1D(),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(\n",
    "            optimizer=Adam(1e-3), \n",
    "            loss='binary_crossentropy', \n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è TensorFlow not installed\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def build_bilstm(vocab_size=10000, max_len=100, embed_dim=128):\n",
    "    \"\"\"Build BiLSTM only when needed\"\"\"\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.keras.models import Sequential\n",
    "        from tensorflow.keras.layers import (\n",
    "            Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "        )\n",
    "        from tensorflow.keras.optimizers import Adam\n",
    "        \n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        model = Sequential([\n",
    "            Embedding(vocab_size, embed_dim, input_length=max_len),\n",
    "            Bidirectional(LSTM(128)),\n",
    "            Dropout(0.4),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(\n",
    "            optimizer=Adam(1e-3), \n",
    "            loss='binary_crossentropy', \n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è TensorFlow not installed\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 3Ô∏è‚É£ Model Registry (NO PRE-LOADING)\n",
    "# ------------------------------\n",
    "def list_available_models():\n",
    "    \"\"\"List all available models without loading them\"\"\"\n",
    "    classical = [\n",
    "        \"LogisticRegression\", \"SVM_Linear\", \"RandomForest\", \n",
    "        \"ExtraTrees\", \"GradientBoosting\", \"AdaBoost\", \n",
    "        \"NaiveBayes\", \"MLPClassifier\"\n",
    "    ]\n",
    "    deep = [\"ANN\", \"CNN\", \"BiLSTM\"]\n",
    "    return {\n",
    "        \"classical\": classical,\n",
    "        \"deep\": deep,\n",
    "        \"all\": classical + deep\n",
    "    }\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# üöÄ Safe Execution\n",
    "# ------------------------------\n",
    "print(\"‚úÖ Model suite initialized (lazy loading)\")\n",
    "available = list_available_models()\n",
    "print(f\"üìã Classical models: {len(available['classical'])}\")\n",
    "print(f\"üß† Deep models: {len(available['deep'])}\")\n",
    "print(\"\\nüí° Usage:\")\n",
    "print(\"   model = get_model('LogisticRegression')\")\n",
    "print(\"   model = build_ann(input_dim=512)\")\n",
    "print(\"\\n‚ö†Ô∏è  Models are created on-demand to prevent memory issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0120858-31aa-4b5c-9051-f81bedf92dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd8461b-3a5a-49a1-b29d-378bb5180ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
