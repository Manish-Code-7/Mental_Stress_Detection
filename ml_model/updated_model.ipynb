{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0118bf2-79b0-45dc-a6ff-9c889df0e515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-11 16:44:22,449 - __main__ - INFO - ============================================================\n",
      "2025-11-11 16:44:22,449 - __main__ - INFO - Mental Stress Detection System - Initialized\n",
      "2025-11-11 16:44:22,450 - __main__ - INFO - Timestamp: 2025-11-11T16:44:22.450010\n",
      "2025-11-11 16:44:22,450 - __main__ - INFO - ============================================================\n",
      "2025-11-11 16:44:22,532 - __main__ - INFO - NLTK package 'stopwords' ready\n",
      "2025-11-11 16:44:22,559 - __main__ - INFO - NLTK package 'punkt' ready\n",
      "2025-11-11 16:44:22,577 - __main__ - INFO - NLTK package 'wordnet' ready\n",
      "2025-11-11 16:44:22,619 - __main__ - INFO - NLTK package 'omw-1.4' ready\n",
      "2025-11-11 16:44:22,624 - __main__ - INFO - NLTK package 'averaged_perceptron_tagger' ready\n",
      "2025-11-11 16:44:22,627 - __main__ - INFO - Configuration saved to preprocessors/system_config.json\n",
      "2025-11-11 16:44:22,627 - __main__ - INFO - Data path set to: /Users/manishmaddikeri/Documents-local/Mental-Stress/ml_model/stress.csv\n",
      "2025-11-11 16:44:22,627 - __main__ - INFO - Loading data from: stress.csv\n",
      "2025-11-11 16:44:22,654 - __main__ - INFO - ‚úì Successfully loaded data with 'utf-8' encoding\n",
      "2025-11-11 16:44:22,654 - __main__ - INFO - Dataset shape: (2838, 116)\n",
      "2025-11-11 16:44:22,655 - __main__ - INFO - Running data validation...\n",
      "2025-11-11 16:44:22,658 - __main__ - INFO - Label column identified: 'subreddit' with 10 unique values\n",
      "\n",
      "============================================================\n",
      "MENTAL STRESS DETECTION - DATASET VALIDATION\n",
      "============================================================\n",
      "Total Samples: 2,838\n",
      "Total Columns: 116\n",
      "Memory Usage: 4.76 MB\n",
      "\n",
      "Text Columns: text\n",
      "Label Column: subreddit\n",
      "\n",
      "Label Distribution:\n",
      "  ptsd: 584 (20.6%)\n",
      "  relationships: 552 (19.5%)\n",
      "  anxiety: 503 (17.7%)\n",
      "  domesticviolence: 316 (11.1%)\n",
      "  assistance: 289 (10.2%)\n",
      "  survivorsofabuse: 245 (8.6%)\n",
      "  homeless: 168 (5.9%)\n",
      "  almosthomeless: 80 (2.8%)\n",
      "  stress: 64 (2.3%)\n",
      "  food_pantry: 37 (1.3%)\n",
      "\n",
      "Duplicate Rows: 0\n",
      "\n",
      "‚úì No critical issues detected!\n",
      "============================================================\n",
      "\n",
      "2025-11-11 16:44:22,679 - __main__ - INFO - Validation report saved to reports/validation_report_20251111_164422.json\n",
      "2025-11-11 16:44:22,680 - __main__ - INFO - ‚úì Cell 1 completed successfully!\n",
      "2025-11-11 16:44:22,680 - __main__ - INFO - Ready for preprocessing. Dataset shape: (2838, 116)\n",
      "\n",
      "‚úì Data loading complete! Ready for Cell 2...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mental Stress Detection System - Cell 1: Setup & Data Loading\n",
    "Production-ready code for Docker deployment with frontend integration\n",
    "\"\"\"\n",
    "\n",
    "# ===============================\n",
    "# Core Imports\n",
    "# ===============================\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass, asdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Visualization\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Non-interactive backend for Docker\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP Libraries\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Serialization\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ===============================\n",
    "# Configuration Management\n",
    "# ===============================\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Centralized configuration for the stress detection system\"\"\"\n",
    "    \n",
    "    # Paths\n",
    "    DATA_DIR: Path = Path(\"data\")\n",
    "    MODEL_DIR: Path = Path(\"models\")\n",
    "    PREPROCESSOR_DIR: Path = Path(\"preprocessors\")\n",
    "    REPORTS_DIR: Path = Path(\"reports\")\n",
    "    LOGS_DIR: Path = Path(\"logs\")\n",
    "    VISUALIZATIONS_DIR: Path = Path(\"visualizations\")\n",
    "    \n",
    "    # Data parameters\n",
    "    DATA_FILE: str = \"stress.csv\"  # File can be in current dir or DATA_DIR\n",
    "    ENCODING_OPTIONS: List[str] = None\n",
    "    TEST_SIZE: float = 0.2\n",
    "    RANDOM_STATE: int = 42\n",
    "    USE_DATA_DIR: bool = False  # Set to True if file is in data/ folder\n",
    "    \n",
    "    # Model parameters\n",
    "    MAX_FEATURES: int = 5000\n",
    "    MAX_LENGTH: int = 100\n",
    "    BATCH_SIZE: int = 32\n",
    "    EPOCHS: int = 10\n",
    "    \n",
    "    # Logging\n",
    "    LOG_LEVEL: str = \"INFO\"\n",
    "    LOG_FORMAT: str = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.ENCODING_OPTIONS is None:\n",
    "            self.ENCODING_OPTIONS = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']\n",
    "        \n",
    "        # Create all directories\n",
    "        for dir_path in [self.DATA_DIR, self.MODEL_DIR, self.PREPROCESSOR_DIR, \n",
    "                         self.REPORTS_DIR, self.LOGS_DIR, self.VISUALIZATIONS_DIR]:\n",
    "            dir_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"Convert config to dictionary for JSON serialization\"\"\"\n",
    "        config_dict = asdict(self)\n",
    "        # Convert Path objects to strings\n",
    "        for key, value in config_dict.items():\n",
    "            if isinstance(value, Path):\n",
    "                config_dict[key] = str(value)\n",
    "        return config_dict\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config()\n",
    "\n",
    "# ===============================\n",
    "# Logging Setup\n",
    "# ===============================\n",
    "def setup_logging(config: Config) -> logging.Logger:\n",
    "    \"\"\"Configure logging for production environment\"\"\"\n",
    "    \n",
    "    log_file = config.LOGS_DIR / f'stress_detection_{datetime.now().strftime(\"%Y%m%d\")}.log'\n",
    "    \n",
    "    logging.basicConfig(\n",
    "        level=getattr(logging, config.LOG_LEVEL),\n",
    "        format=config.LOG_FORMAT,\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),\n",
    "            logging.StreamHandler(sys.stdout)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(\"Mental Stress Detection System - Initialized\")\n",
    "    logger.info(f\"Timestamp: {datetime.now().isoformat()}\")\n",
    "    logger.info(\"=\"*60)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "logger = setup_logging(config)\n",
    "\n",
    "# ===============================\n",
    "# NLTK Data Download (Docker-ready)\n",
    "# ===============================\n",
    "def download_nltk_dependencies():\n",
    "    \"\"\"Download required NLTK data with error handling\"\"\"\n",
    "    nltk_packages = ['stopwords', 'punkt', 'wordnet', 'omw-1.4', 'averaged_perceptron_tagger']\n",
    "    \n",
    "    for package in nltk_packages:\n",
    "        try:\n",
    "            nltk.download(package, quiet=True)\n",
    "            logger.info(f\"NLTK package '{package}' ready\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not download NLTK package '{package}': {e}\")\n",
    "\n",
    "download_nltk_dependencies()\n",
    "\n",
    "# ===============================\n",
    "# Data Validation Classes\n",
    "# ===============================\n",
    "@dataclass\n",
    "class DataValidationReport:\n",
    "    \"\"\"Structured validation report for the dataset\"\"\"\n",
    "    total_samples: int\n",
    "    columns: List[str]\n",
    "    missing_values: Dict[str, int]\n",
    "    duplicate_rows: int\n",
    "    text_columns: List[str]\n",
    "    label_column: Optional[str]\n",
    "    label_distribution: Optional[Dict[str, int]]\n",
    "    memory_usage_mb: float\n",
    "    issues: List[str]\n",
    "    timestamp: str\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return asdict(self)\n",
    "    \n",
    "    def save_report(self, filepath: Path):\n",
    "        \"\"\"Save validation report as JSON\"\"\"\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(self.to_dict(), f, indent=2)\n",
    "        logger.info(f\"Validation report saved to {filepath}\")\n",
    "\n",
    "# ===============================\n",
    "# Data Loading & Validation\n",
    "# ===============================\n",
    "class StressDataLoader:\n",
    "    \"\"\"Handles data loading with comprehensive validation\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        # Try current directory first, then data directory\n",
    "        if config.USE_DATA_DIR or not Path(config.DATA_FILE).exists():\n",
    "            self.data_path = config.DATA_DIR / config.DATA_FILE\n",
    "        else:\n",
    "            self.data_path = Path(config.DATA_FILE)\n",
    "        \n",
    "        logger.info(f\"Data path set to: {self.data_path.absolute()}\")\n",
    "    \n",
    "    def load_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Load dataset with multiple encoding attempts\"\"\"\n",
    "        logger.info(f\"Loading data from: {self.data_path}\")\n",
    "        \n",
    "        if not self.data_path.exists():\n",
    "            raise FileNotFoundError(f\"Data file not found: {self.data_path}\")\n",
    "        \n",
    "        df = None\n",
    "        for encoding in self.config.ENCODING_OPTIONS:\n",
    "            try:\n",
    "                df = pd.read_csv(\n",
    "                    self.data_path,\n",
    "                    encoding=encoding,\n",
    "                    on_bad_lines='skip',\n",
    "                    low_memory=False\n",
    "                )\n",
    "                logger.info(f\"‚úì Successfully loaded data with '{encoding}' encoding\")\n",
    "                break\n",
    "            except UnicodeDecodeError:\n",
    "                logger.warning(f\"‚úó Failed to load with '{encoding}' encoding\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error with encoding '{encoding}': {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if df is None:\n",
    "            raise ValueError(\"Could not load data with any of the specified encodings\")\n",
    "        \n",
    "        logger.info(f\"Dataset shape: {df.shape}\")\n",
    "        return df\n",
    "    \n",
    "    def validate_data(self, df: pd.DataFrame) -> DataValidationReport:\n",
    "        \"\"\"Comprehensive data validation\"\"\"\n",
    "        logger.info(\"Running data validation...\")\n",
    "        \n",
    "        issues = []\n",
    "        text_columns = []\n",
    "        label_column = None\n",
    "        label_distribution = None\n",
    "        \n",
    "        # Identify text and label columns\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'object':\n",
    "                avg_length = df[col].dropna().astype(str).str.len().mean()\n",
    "                unique_count = df[col].nunique()\n",
    "                \n",
    "                if avg_length > 10:  # Text column\n",
    "                    text_columns.append(col)\n",
    "                    \n",
    "                    # Check for very short texts\n",
    "                    short_texts = (df[col].astype(str).str.len() < 5).sum()\n",
    "                    if short_texts > 0:\n",
    "                        issues.append(f\"Column '{col}': {short_texts} very short texts found\")\n",
    "                \n",
    "                elif unique_count <= 10:  # Likely a label column\n",
    "                    label_column = col\n",
    "                    label_distribution = df[col].value_counts().to_dict()\n",
    "                    logger.info(f\"Label column identified: '{col}' with {unique_count} unique values\")\n",
    "        \n",
    "        # Check for missing values\n",
    "        missing_vals = df.isnull().sum()\n",
    "        missing_dict = {col: int(count) for col, count in missing_vals.items() if count > 0}\n",
    "        \n",
    "        if missing_dict:\n",
    "            issues.append(f\"Missing values found in columns: {list(missing_dict.keys())}\")\n",
    "        \n",
    "        # Check for duplicates\n",
    "        duplicates = df.duplicated().sum()\n",
    "        if duplicates > 0:\n",
    "            issues.append(f\"Found {duplicates} duplicate rows\")\n",
    "        \n",
    "        # Memory usage\n",
    "        memory_mb = df.memory_usage(deep=True).sum() / (1024 ** 2)\n",
    "        \n",
    "        report = DataValidationReport(\n",
    "            total_samples=len(df),\n",
    "            columns=list(df.columns),\n",
    "            missing_values=missing_dict,\n",
    "            duplicate_rows=int(duplicates),\n",
    "            text_columns=text_columns,\n",
    "            label_column=label_column,\n",
    "            label_distribution=label_distribution,\n",
    "            memory_usage_mb=round(memory_mb, 2),\n",
    "            issues=issues,\n",
    "            timestamp=datetime.now().isoformat()\n",
    "        )\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def print_validation_summary(self, report: DataValidationReport):\n",
    "        \"\"\"Print formatted validation summary\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"MENTAL STRESS DETECTION - DATASET VALIDATION\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Total Samples: {report.total_samples:,}\")\n",
    "        print(f\"Total Columns: {len(report.columns)}\")\n",
    "        print(f\"Memory Usage: {report.memory_usage_mb:.2f} MB\")\n",
    "        print(f\"\\nText Columns: {', '.join(report.text_columns)}\")\n",
    "        print(f\"Label Column: {report.label_column}\")\n",
    "        \n",
    "        if report.label_distribution:\n",
    "            print(f\"\\nLabel Distribution:\")\n",
    "            for label, count in report.label_distribution.items():\n",
    "                percentage = (count / report.total_samples) * 100\n",
    "                print(f\"  {label}: {count:,} ({percentage:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nDuplicate Rows: {report.duplicate_rows}\")\n",
    "        \n",
    "        if report.missing_values:\n",
    "            print(f\"\\nMissing Values:\")\n",
    "            for col, count in report.missing_values.items():\n",
    "                print(f\"  {col}: {count}\")\n",
    "        \n",
    "        if report.issues:\n",
    "            print(f\"\\n‚ö†Ô∏è  Issues Detected:\")\n",
    "            for issue in report.issues:\n",
    "                print(f\"  ‚Ä¢ {issue}\")\n",
    "        else:\n",
    "            print(f\"\\n‚úì No critical issues detected!\")\n",
    "        \n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# ===============================\n",
    "# Main Execution\n",
    "# ===============================\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    try:\n",
    "        # Save configuration\n",
    "        config_path = config.PREPROCESSOR_DIR / 'system_config.json'\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(config.to_dict(), f, indent=2)\n",
    "        logger.info(f\"Configuration saved to {config_path}\")\n",
    "        \n",
    "        # Load data\n",
    "        loader = StressDataLoader(config)\n",
    "        stress_df = loader.load_data()\n",
    "        \n",
    "        # Create working copy\n",
    "        stress = stress_df.copy()\n",
    "        \n",
    "        # Validate data\n",
    "        validation_report = loader.validate_data(stress)\n",
    "        \n",
    "        # Print summary\n",
    "        loader.print_validation_summary(validation_report)\n",
    "        \n",
    "        # Save validation report\n",
    "        report_path = config.REPORTS_DIR / f'validation_report_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
    "        validation_report.save_report(report_path)\n",
    "        \n",
    "        # Save dataset metadata for frontend\n",
    "        metadata = {\n",
    "            'dataset_info': validation_report.to_dict(),\n",
    "            'config': config.to_dict(),\n",
    "            'status': 'loaded',\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        metadata_path = config.PREPROCESSOR_DIR / 'dataset_metadata.json'\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        logger.info(\"‚úì Cell 1 completed successfully!\")\n",
    "        logger.info(f\"Ready for preprocessing. Dataset shape: {stress.shape}\")\n",
    "        \n",
    "        return stress, validation_report\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main execution: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "# Execute\n",
    "if __name__ == \"__main__\":\n",
    "    stress, validation_report = main()\n",
    "    print(\"\\n‚úì Data loading complete! Ready for Cell 2...\")\n",
    "else:\n",
    "    stress, validation_report = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef24539b-95a9-44e5-bf1e-5c95e08977da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-11 16:44:38,004 - __main__ - INFO - Starting Cell 2: Enhanced EDA\n",
      "2025-11-11 16:44:38,015 - __main__ - INFO - EDA initialized\n",
      "2025-11-11 16:44:38,016 - __main__ - INFO - Starting complete EDA pipeline...\n",
      "2025-11-11 16:44:38,016 - __main__ - INFO - Generating basic statistics...\n",
      "\n",
      "======================================================================\n",
      "MENTAL STRESS DETECTION - EXPLORATORY DATA ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "1. DATASET OVERVIEW\n",
      "======================================================================\n",
      "‚îú‚îÄ Shape: 2,838 rows √ó 116 columns\n",
      "‚îú‚îÄ Memory Usage: 4.76 MB\n",
      "‚îú‚îÄ Duplicates: 0 (0.00%)\n",
      "‚îî‚îÄ Missing Values: 0\n",
      "\n",
      "======================================================================\n",
      "2. COLUMN INFORMATION\n",
      "======================================================================\n",
      "‚îú‚îÄ subreddit            | Type: object     | Unique: 10     | Missing: 0\n",
      "‚îú‚îÄ post_id              | Type: object     | Unique: 2343   | Missing: 0\n",
      "‚îú‚îÄ sentence_range       | Type: object     | Unique: 173    | Missing: 0\n",
      "‚îú‚îÄ text                 | Type: object     | Unique: 2820   | Missing: 0\n",
      "‚îú‚îÄ id                   | Type: int64      | Unique: 2838   | Missing: 0\n",
      "‚îú‚îÄ label                | Type: int64      | Unique: 2      | Missing: 0\n",
      "‚îú‚îÄ confidence           | Type: float64    | Unique: 10     | Missing: 0\n",
      "‚îú‚îÄ social_timestamp     | Type: int64      | Unique: 2343   | Missing: 0\n",
      "‚îú‚îÄ social_karma         | Type: int64      | Unique: 135    | Missing: 0\n",
      "‚îú‚îÄ syntax_ari           | Type: float64    | Unique: 2659   | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_WC          | Type: int64      | Unique: 198    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_Analytic    | Type: float64    | Unique: 1435   | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_Clout       | Type: float64    | Unique: 1323   | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_Authentic   | Type: float64    | Unique: 978    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_Tone        | Type: float64    | Unique: 489    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_WPS         | Type: float64    | Unique: 360    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_Sixltr      | Type: float64    | Unique: 938    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_Dic         | Type: float64    | Unique: 833    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_function    | Type: float64    | Unique: 1048   | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_pronoun     | Type: float64    | Unique: 964    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_ppron       | Type: float64    | Unique: 889    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_i           | Type: float64    | Unique: 810    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_we          | Type: float64    | Unique: 293    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_you         | Type: float64    | Unique: 324    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_shehe       | Type: float64    | Unique: 548    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_they        | Type: float64    | Unique: 263    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_ipron       | Type: float64    | Unique: 644    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_article     | Type: float64    | Unique: 572    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_prep        | Type: float64    | Unique: 737    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_auxverb     | Type: float64    | Unique: 743    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_adverb      | Type: float64    | Unique: 655    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_conj        | Type: float64    | Unique: 612    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_negate      | Type: float64    | Unique: 398    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_verb        | Type: float64    | Unique: 890    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_adj         | Type: float64    | Unique: 562    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_compare     | Type: float64    | Unique: 424    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_interrog    | Type: float64    | Unique: 346    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_number      | Type: float64    | Unique: 355    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_quant       | Type: float64    | Unique: 394    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_affect      | Type: float64    | Unique: 692    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_posemo      | Type: float64    | Unique: 497    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_negemo      | Type: float64    | Unique: 555    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_anx         | Type: float64    | Unique: 298    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_anger       | Type: float64    | Unique: 293    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_sad         | Type: float64    | Unique: 236    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_social      | Type: float64    | Unique: 984    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_family      | Type: float64    | Unique: 291    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_friend      | Type: float64    | Unique: 236    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_female      | Type: float64    | Unique: 448    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_male        | Type: float64    | Unique: 494    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_cogproc     | Type: float64    | Unique: 929    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_insight     | Type: float64    | Unique: 461    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_cause       | Type: float64    | Unique: 369    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_discrep     | Type: float64    | Unique: 400    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_tentat      | Type: float64    | Unique: 521    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_certain     | Type: float64    | Unique: 342    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_differ      | Type: float64    | Unique: 531    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_percept     | Type: float64    | Unique: 433    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_see         | Type: float64    | Unique: 232    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_hear        | Type: float64    | Unique: 244    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_feel        | Type: float64    | Unique: 282    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_bio         | Type: float64    | Unique: 494    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_body        | Type: float64    | Unique: 275    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_health      | Type: float64    | Unique: 343    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_sexual      | Type: float64    | Unique: 161    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_ingest      | Type: float64    | Unique: 202    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_drives      | Type: float64    | Unique: 741    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_affiliation | Type: float64    | Unique: 512    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_achieve     | Type: float64    | Unique: 331    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_power       | Type: float64    | Unique: 431    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_reward      | Type: float64    | Unique: 313    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_risk        | Type: float64    | Unique: 243    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_focuspast   | Type: float64    | Unique: 699    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_focuspresent | Type: float64    | Unique: 945    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_focusfuture | Type: float64    | Unique: 305    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_relativ     | Type: float64    | Unique: 965    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_motion      | Type: float64    | Unique: 377    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_space       | Type: float64    | Unique: 670    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_time        | Type: float64    | Unique: 721    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_work        | Type: float64    | Unique: 438    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_leisure     | Type: float64    | Unique: 278    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_home        | Type: float64    | Unique: 256    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_money       | Type: float64    | Unique: 309    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_relig       | Type: float64    | Unique: 117    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_death       | Type: float64    | Unique: 123    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_informal    | Type: float64    | Unique: 260    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_swear       | Type: float64    | Unique: 166    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_netspeak    | Type: float64    | Unique: 159    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_assent      | Type: float64    | Unique: 118    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_nonflu      | Type: float64    | Unique: 110    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_filler      | Type: float64    | Unique: 67     | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_AllPunc     | Type: float64    | Unique: 990    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_Period      | Type: float64    | Unique: 512    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_Comma       | Type: float64    | Unique: 546    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_Colon       | Type: float64    | Unique: 132    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_SemiC       | Type: float64    | Unique: 106    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_QMark       | Type: float64    | Unique: 205    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_Exclam      | Type: float64    | Unique: 127    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_Dash        | Type: float64    | Unique: 221    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_Quote       | Type: float64    | Unique: 193    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_Apostro     | Type: float64    | Unique: 510    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_Parenth     | Type: float64    | Unique: 216    | Missing: 0\n",
      "‚îú‚îÄ lex_liwc_OtherP      | Type: float64    | Unique: 348    | Missing: 0\n",
      "‚îú‚îÄ lex_dal_max_pleasantness | Type: float64    | Unique: 41     | Missing: 0\n",
      "‚îú‚îÄ lex_dal_max_activation | Type: float64    | Unique: 54     | Missing: 0\n",
      "‚îú‚îÄ lex_dal_max_imagery  | Type: float64    | Unique: 6      | Missing: 0\n",
      "‚îú‚îÄ lex_dal_min_pleasantness | Type: float64    | Unique: 33     | Missing: 0\n",
      "‚îú‚îÄ lex_dal_min_activation | Type: float64    | Unique: 22     | Missing: 0\n",
      "‚îú‚îÄ lex_dal_min_imagery  | Type: float64    | Unique: 2      | Missing: 0\n",
      "‚îú‚îÄ lex_dal_avg_activation | Type: float64    | Unique: 2606   | Missing: 0\n",
      "‚îú‚îÄ lex_dal_avg_imagery  | Type: float64    | Unique: 2111   | Missing: 0\n",
      "‚îú‚îÄ lex_dal_avg_pleasantness | Type: float64    | Unique: 2615   | Missing: 0\n",
      "‚îú‚îÄ social_upvote_ratio  | Type: float64    | Unique: 73     | Missing: 0\n",
      "‚îú‚îÄ social_num_comments  | Type: int64      | Unique: 94     | Missing: 0\n",
      "‚îú‚îÄ syntax_fk_grade      | Type: float64    | Unique: 2417   | Missing: 0\n",
      "‚îú‚îÄ sentiment            | Type: float64    | Unique: 1809   | Missing: 0\n",
      "2025-11-11 16:44:38,069 - __main__ - INFO - Performing text analysis...\n",
      "\n",
      "======================================================================\n",
      "4. TEXT ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "üìù Analyzing: TEXT\n",
      "----------------------------------------------------------------------\n",
      "‚îú‚îÄ Character Length:\n",
      "‚îÇ  ‚îú‚îÄ Mean: 448.0 | Median: 421.0\n",
      "‚îÇ  ‚îî‚îÄ Range: 6 - 1639\n",
      "‚îú‚îÄ Word Count:\n",
      "‚îÇ  ‚îî‚îÄ Mean: 85.7 | Median: 80.0\n",
      "‚îî‚îÄ Sentence Count:\n",
      "   ‚îî‚îÄ Mean: 6.2\n",
      "\n",
      "üìä Length Distribution:\n",
      "‚îú‚îÄ Short (<50 chars):          5 (  0.2%)\n",
      "‚îú‚îÄ Medium (50-200):           91 (  3.2%)\n",
      "‚îú‚îÄ Long (200-500):         1,872 ( 66.0%)\n",
      "‚îî‚îÄ Very Long (>500):         870 ( 30.7%)\n",
      "2025-11-11 16:44:38,093 - __main__ - INFO - Analyzing label distribution...\n",
      "\n",
      "======================================================================\n",
      "5. LABEL DISTRIBUTION & CLASS BALANCE\n",
      "======================================================================\n",
      "\n",
      "üìä Class Distribution:\n",
      "‚îú‚îÄ 1              :  1,488 ( 52.4%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "‚îú‚îÄ 0              :  1,350 ( 47.6%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "‚öñÔ∏è  Class Imbalance Ratio: 1.10:1\n",
      "‚úì Classes are reasonably balanced\n",
      "2025-11-11 16:44:38,094 - __main__ - INFO - Performing correlation analysis...\n",
      "\n",
      "======================================================================\n",
      "6. FEATURE CORRELATION ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "‚ö†Ô∏è  High Correlations Found (|r| > 0.7):\n",
      "‚îú‚îÄ social_karma ‚Üî social_num_comments: 0.795\n",
      "‚îú‚îÄ syntax_ari ‚Üî syntax_fk_grade: 0.967\n",
      "‚îú‚îÄ lex_liwc_WC ‚Üî lex_liwc_WPS: 0.720\n",
      "‚îú‚îÄ lex_liwc_WC ‚Üî text_length: 0.981\n",
      "‚îú‚îÄ lex_liwc_WC ‚Üî text_word_count: 0.996\n",
      "‚îú‚îÄ lex_liwc_Analytic ‚Üî lex_liwc_pronoun: -0.704\n",
      "‚îú‚îÄ lex_liwc_Clout ‚Üî lex_liwc_i: -0.717\n",
      "‚îú‚îÄ lex_liwc_Clout ‚Üî lex_liwc_social: 0.782\n",
      "‚îú‚îÄ lex_liwc_Tone ‚Üî lex_liwc_posemo: 0.749\n",
      "‚îú‚îÄ lex_liwc_WPS ‚Üî text_length: 0.711\n",
      "‚îú‚îÄ lex_liwc_WPS ‚Üî text_word_count: 0.718\n",
      "‚îú‚îÄ lex_liwc_pronoun ‚Üî lex_liwc_ppron: 0.817\n",
      "‚îú‚îÄ lex_liwc_affect ‚Üî lex_liwc_negemo: 0.701\n",
      "‚îú‚îÄ lex_liwc_relativ ‚Üî lex_liwc_time: 0.711\n",
      "‚îú‚îÄ lex_liwc_AllPunc ‚Üî lex_liwc_OtherP: 0.862\n",
      "‚îú‚îÄ text_length ‚Üî text_word_count: 0.984\n",
      "2025-11-11 16:44:38,196 - __main__ - INFO - Generating visualizations...\n",
      "2025-11-11 16:44:38,370 - __main__ - INFO - Saved visualization: label_distribution.png\n",
      "2025-11-11 16:44:38,586 - __main__ - INFO - Saved visualization: text_length_distributions.png\n",
      "2025-11-11 16:44:38,731 - __main__ - INFO - Saved visualization: text_statistics_by_label.png\n",
      "2025-11-11 16:44:42,073 - __main__ - INFO - Saved visualization: correlation_heatmap.png\n",
      "2025-11-11 16:44:42,921 - __main__ - INFO - Saved visualization: word_frequency.png\n",
      "2025-11-11 16:44:44,230 - __main__ - INFO - Saved visualization: word_clouds.png\n",
      "2025-11-11 16:44:44,706 - __main__ - INFO - Saved visualization: subreddit_analysis.png\n",
      "2025-11-11 16:44:44,706 - __main__ - INFO - All visualizations generated\n",
      "2025-11-11 16:44:44,707 - __main__ - INFO - Generating mental health insights...\n",
      "\n",
      "======================================================================\n",
      "7. MENTAL HEALTH SPECIFIC INSIGHTS\n",
      "======================================================================\n",
      "‚úì Classes are well balanced for training\n",
      "‚ö†Ô∏è Low subreddit diversity - Data may be biased toward specific communities\n",
      "\n",
      "======================================================================\n",
      "8. STATISTICAL SIGNIFICANCE TESTS\n",
      "======================================================================\n",
      "\n",
      "üìä T-test for text length between classes:\n",
      "‚îú‚îÄ T-statistic: -3.8832\n",
      "‚îú‚îÄ P-value: 0.000105\n",
      "‚îî‚îÄ ‚úì Significant difference in text length between classes (p < 0.05)\n",
      "2025-11-11 16:44:44,711 - __main__ - INFO - Saving EDA results...\n",
      "2025-11-11 16:44:44,711 - __main__ - INFO - EDA results saved to reports/eda_results.json\n",
      "2025-11-11 16:44:44,714 - __main__ - INFO - Processed data saved to preprocessors/eda_processed_data.pkl\n",
      "\n",
      "======================================================================\n",
      "‚úÖ EDA COMPLETED SUCCESSFULLY!\n",
      "======================================================================\n",
      "‚îú‚îÄ Visualizations saved to: visualizations\n",
      "‚îú‚îÄ Results saved to: reports/eda_results.json\n",
      "‚îî‚îÄ Processed data saved to: preprocessors/eda_processed_data.pkl\n",
      "\n",
      "üöÄ Ready for text preprocessing and feature engineering!\n",
      "2025-11-11 16:44:44,714 - __main__ - INFO - ‚úì Cell 2 completed successfully!\n",
      "\n",
      "======================================================================\n",
      "‚úÖ EDA COMPLETE! Ready for Cell 3 (Text Preprocessing)...\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mental Stress Detection System - Cell 2: Enhanced EDA & Advanced Visualizations\n",
    "Production-ready exploratory data analysis with comprehensive insights\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy import stats\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import warnings\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ensure proper backend for Docker\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# ===============================\n",
    "# EDA Configuration\n",
    "# ===============================\n",
    "class EDAConfig:\n",
    "    \"\"\"Configuration for EDA visualizations\"\"\"\n",
    "    FIGURE_DPI = 100\n",
    "    SAVE_FIGURES = True\n",
    "    FIG_FORMAT = 'png'\n",
    "    COLOR_PALETTE = 'husl'\n",
    "    STYLE = 'seaborn-v0_8-darkgrid'\n",
    "    \n",
    "    # Stress-specific colors\n",
    "    STRESS_COLORS = {\n",
    "        'high_stress': '#FF6B6B',\n",
    "        'low_stress': '#4ECDC4',\n",
    "        'neutral': '#95E1D3',\n",
    "        'positive': '#38B6A8'\n",
    "    }\n",
    "\n",
    "eda_config = EDAConfig()\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(eda_config.COLOR_PALETTE)\n",
    "\n",
    "# ===============================\n",
    "# Advanced EDA Class\n",
    "# ===============================\n",
    "class StressEDA:\n",
    "    \"\"\"Comprehensive EDA for Mental Stress Detection\"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame, config, save_dir):\n",
    "        self.df = df.copy()\n",
    "        self.config = config\n",
    "        self.save_dir = save_dir\n",
    "        self.text_cols = self._identify_text_columns()\n",
    "        self.label_col = self._identify_label_column()\n",
    "        self.results = {}\n",
    "        \n",
    "        logger.info(\"EDA initialized\")\n",
    "    \n",
    "    def _identify_text_columns(self):\n",
    "        \"\"\"Identify text columns in dataset\"\"\"\n",
    "        text_cols = []\n",
    "        for col in self.df.columns:\n",
    "            if self.df[col].dtype == 'object':\n",
    "                avg_length = self.df[col].dropna().astype(str).str.len().mean()\n",
    "                if avg_length > 10:\n",
    "                    text_cols.append(col)\n",
    "        return text_cols\n",
    "    \n",
    "    def _identify_label_column(self):\n",
    "        \"\"\"Identify the label column\"\"\"\n",
    "        for col in self.df.columns:\n",
    "            if col.lower() in ['label', 'target', 'class', 'stress']:\n",
    "                return col\n",
    "            if self.df[col].dtype == 'object' and self.df[col].nunique() <= 10:\n",
    "                if 'label' in col.lower():\n",
    "                    return col\n",
    "        return 'label' if 'label' in self.df.columns else None\n",
    "    \n",
    "    def basic_statistics(self):\n",
    "        \"\"\"Generate comprehensive basic statistics\"\"\"\n",
    "        logger.info(\"Generating basic statistics...\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"MENTAL STRESS DETECTION - EXPLORATORY DATA ANALYSIS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"1. DATASET OVERVIEW\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"‚îú‚îÄ Shape: {self.df.shape[0]:,} rows √ó {self.df.shape[1]} columns\")\n",
    "        print(f\"‚îú‚îÄ Memory Usage: {self.df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "        print(f\"‚îú‚îÄ Duplicates: {self.df.duplicated().sum():,} ({self.df.duplicated().sum()/len(self.df)*100:.2f}%)\")\n",
    "        print(f\"‚îî‚îÄ Missing Values: {self.df.isnull().sum().sum():,}\")\n",
    "        \n",
    "        # Column information\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"2. COLUMN INFORMATION\")\n",
    "        print(f\"{'='*70}\")\n",
    "        for col in self.df.columns:\n",
    "            dtype = self.df[col].dtype\n",
    "            unique = self.df[col].nunique()\n",
    "            missing = self.df[col].isnull().sum()\n",
    "            print(f\"‚îú‚îÄ {col:<20} | Type: {str(dtype):<10} | Unique: {unique:<6} | Missing: {missing}\")\n",
    "        \n",
    "        # Missing value analysis\n",
    "        missing = self.df.isnull().sum()\n",
    "        if missing.sum() > 0:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(\"3. MISSING VALUE ANALYSIS\")\n",
    "            print(f\"{'='*70}\")\n",
    "            missing_pct = (missing / len(self.df)) * 100\n",
    "            for col, count in missing[missing > 0].items():\n",
    "                print(f\"‚îú‚îÄ {col}: {count:,} ({missing_pct[col]:.2f}%)\")\n",
    "        \n",
    "        self.results['basic_stats'] = {\n",
    "            'shape': self.df.shape,\n",
    "            'memory_mb': round(self.df.memory_usage(deep=True).sum() / 1024**2, 2),\n",
    "            'duplicates': int(self.df.duplicated().sum()),\n",
    "            'missing_total': int(self.df.isnull().sum().sum())\n",
    "        }\n",
    "    \n",
    "    def text_analysis(self):\n",
    "        \"\"\"Comprehensive text analysis for mental health insights\"\"\"\n",
    "        logger.info(\"Performing text analysis...\")\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"4. TEXT ANALYSIS\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        text_stats = {}\n",
    "        \n",
    "        for col in self.text_cols:\n",
    "            print(f\"\\nüìù Analyzing: {col.upper()}\")\n",
    "            print(f\"{'-'*70}\")\n",
    "            \n",
    "            # Calculate text metrics\n",
    "            self.df[f'{col}_length'] = self.df[col].astype(str).str.len()\n",
    "            self.df[f'{col}_word_count'] = self.df[col].astype(str).str.split().str.len()\n",
    "            self.df[f'{col}_sentence_count'] = self.df[col].astype(str).str.count(r'[.!?]') + 1\n",
    "            \n",
    "            # Statistics\n",
    "            stats_dict = {\n",
    "                'avg_length': self.df[f'{col}_length'].mean(),\n",
    "                'median_length': self.df[f'{col}_length'].median(),\n",
    "                'std_length': self.df[f'{col}_length'].std(),\n",
    "                'max_length': self.df[f'{col}_length'].max(),\n",
    "                'min_length': self.df[f'{col}_length'].min(),\n",
    "                'avg_words': self.df[f'{col}_word_count'].mean(),\n",
    "                'median_words': self.df[f'{col}_word_count'].median(),\n",
    "                'avg_sentences': self.df[f'{col}_sentence_count'].mean()\n",
    "            }\n",
    "            \n",
    "            print(f\"‚îú‚îÄ Character Length:\")\n",
    "            print(f\"‚îÇ  ‚îú‚îÄ Mean: {stats_dict['avg_length']:.1f} | Median: {stats_dict['median_length']:.1f}\")\n",
    "            print(f\"‚îÇ  ‚îî‚îÄ Range: {stats_dict['min_length']} - {stats_dict['max_length']}\")\n",
    "            print(f\"‚îú‚îÄ Word Count:\")\n",
    "            print(f\"‚îÇ  ‚îî‚îÄ Mean: {stats_dict['avg_words']:.1f} | Median: {stats_dict['median_words']:.1f}\")\n",
    "            print(f\"‚îî‚îÄ Sentence Count:\")\n",
    "            print(f\"   ‚îî‚îÄ Mean: {stats_dict['avg_sentences']:.1f}\")\n",
    "            \n",
    "            # Text length categories\n",
    "            short = (self.df[f'{col}_length'] < 50).sum()\n",
    "            medium = ((self.df[f'{col}_length'] >= 50) & (self.df[f'{col}_length'] < 200)).sum()\n",
    "            long = ((self.df[f'{col}_length'] >= 200) & (self.df[f'{col}_length'] < 500)).sum()\n",
    "            very_long = (self.df[f'{col}_length'] >= 500).sum()\n",
    "            \n",
    "            print(f\"\\nüìä Length Distribution:\")\n",
    "            print(f\"‚îú‚îÄ Short (<50 chars):     {short:>6,} ({short/len(self.df)*100:>5.1f}%)\")\n",
    "            print(f\"‚îú‚îÄ Medium (50-200):       {medium:>6,} ({medium/len(self.df)*100:>5.1f}%)\")\n",
    "            print(f\"‚îú‚îÄ Long (200-500):        {long:>6,} ({long/len(self.df)*100:>5.1f}%)\")\n",
    "            print(f\"‚îî‚îÄ Very Long (>500):      {very_long:>6,} ({very_long/len(self.df)*100:>5.1f}%)\")\n",
    "            \n",
    "            text_stats[col] = stats_dict\n",
    "        \n",
    "        self.results['text_stats'] = text_stats\n",
    "    \n",
    "    def label_analysis(self):\n",
    "        \"\"\"Analyze label distribution and class balance\"\"\"\n",
    "        logger.info(\"Analyzing label distribution...\")\n",
    "        \n",
    "        if not self.label_col:\n",
    "            logger.warning(\"No label column identified\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"5. LABEL DISTRIBUTION & CLASS BALANCE\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Value counts\n",
    "        label_counts = self.df[self.label_col].value_counts()\n",
    "        label_pct = self.df[self.label_col].value_counts(normalize=True)\n",
    "        \n",
    "        print(f\"\\nüìä Class Distribution:\")\n",
    "        for label, count in label_counts.items():\n",
    "            pct = label_pct[label] * 100\n",
    "            bar_length = int(pct / 2)\n",
    "            bar = '‚ñà' * bar_length\n",
    "            print(f\"‚îú‚îÄ {label:<15}: {count:>6,} ({pct:>5.1f}%) {bar}\")\n",
    "        \n",
    "        # Class imbalance metrics\n",
    "        if len(label_counts) == 2:\n",
    "            imbalance_ratio = label_counts.max() / label_counts.min()\n",
    "            print(f\"\\n‚öñÔ∏è  Class Imbalance Ratio: {imbalance_ratio:.2f}:1\")\n",
    "            \n",
    "            if imbalance_ratio > 1.5:\n",
    "                print(f\"‚ö†Ô∏è  SIGNIFICANT IMBALANCE DETECTED!\")\n",
    "                print(f\"   Recommendations:\")\n",
    "                print(f\"   ‚îú‚îÄ Use stratified train-test split\")\n",
    "                print(f\"   ‚îú‚îÄ Apply class weights in model\")\n",
    "                print(f\"   ‚îú‚îÄ Consider SMOTE or oversampling\")\n",
    "                print(f\"   ‚îî‚îÄ Use balanced accuracy metrics\")\n",
    "            else:\n",
    "                print(f\"‚úì Classes are reasonably balanced\")\n",
    "        \n",
    "        self.results['label_distribution'] = label_counts.to_dict()\n",
    "    \n",
    "    def correlation_analysis(self):\n",
    "        \"\"\"Analyze correlations between features\"\"\"\n",
    "        logger.info(\"Performing correlation analysis...\")\n",
    "        \n",
    "        # Numeric columns only\n",
    "        numeric_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        if len(numeric_cols) < 2:\n",
    "            logger.info(\"Not enough numeric columns for correlation analysis\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"6. FEATURE CORRELATION ANALYSIS\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Calculate correlation matrix\n",
    "        corr_matrix = self.df[numeric_cols].corr()\n",
    "        \n",
    "        # Find high correlations (excluding diagonal)\n",
    "        high_corr = []\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i+1, len(corr_matrix.columns)):\n",
    "                if abs(corr_matrix.iloc[i, j]) > 0.7:\n",
    "                    high_corr.append({\n",
    "                        'feature1': corr_matrix.columns[i],\n",
    "                        'feature2': corr_matrix.columns[j],\n",
    "                        'correlation': corr_matrix.iloc[i, j]\n",
    "                    })\n",
    "        \n",
    "        if high_corr:\n",
    "            print(f\"\\n‚ö†Ô∏è  High Correlations Found (|r| > 0.7):\")\n",
    "            for item in high_corr:\n",
    "                print(f\"‚îú‚îÄ {item['feature1']} ‚Üî {item['feature2']}: {item['correlation']:.3f}\")\n",
    "        else:\n",
    "            print(f\"\\n‚úì No high correlations found\")\n",
    "        \n",
    "        self.results['high_correlations'] = high_corr\n",
    "    \n",
    "    def generate_visualizations(self):\n",
    "        \"\"\"Generate all visualizations\"\"\"\n",
    "        logger.info(\"Generating visualizations...\")\n",
    "        \n",
    "        self._plot_label_distribution()\n",
    "        self._plot_text_length_distributions()\n",
    "        self._plot_text_statistics_by_label()\n",
    "        self._plot_correlation_heatmap()\n",
    "        self._plot_word_frequency()\n",
    "        self._generate_word_clouds()\n",
    "        \n",
    "        if 'subreddit' in self.df.columns:\n",
    "            self._plot_subreddit_analysis()\n",
    "        \n",
    "        logger.info(\"All visualizations generated\")\n",
    "    \n",
    "    def _plot_label_distribution(self):\n",
    "        \"\"\"Visualize label distribution with multiple views\"\"\"\n",
    "        if not self.label_col:\n",
    "            return\n",
    "        \n",
    "        fig = plt.figure(figsize=(18, 6))\n",
    "        \n",
    "        # Subplot 1: Bar chart with counts\n",
    "        plt.subplot(1, 3, 1)\n",
    "        label_counts = self.df[self.label_col].value_counts()\n",
    "        colors = [eda_config.STRESS_COLORS['high_stress'], \n",
    "                  eda_config.STRESS_COLORS['low_stress']][:len(label_counts)]\n",
    "        \n",
    "        bars = plt.bar(range(len(label_counts)), label_counts.values, \n",
    "                       color=colors, edgecolor='black', linewidth=1.5, alpha=0.8)\n",
    "        plt.xticks(range(len(label_counts)), label_counts.index, rotation=0)\n",
    "        plt.title('Label Distribution - Count', fontsize=14, fontweight='bold', pad=15)\n",
    "        plt.xlabel('Class', fontsize=12, fontweight='bold')\n",
    "        plt.ylabel('Count', fontsize=12, fontweight='bold')\n",
    "        plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (bar, count) in enumerate(zip(bars, label_counts.values)):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(label_counts)*0.01,\n",
    "                    f'{count:,}\\n({count/len(self.df)*100:.1f}%)', \n",
    "                    ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "        \n",
    "        # Subplot 2: Pie chart\n",
    "        plt.subplot(1, 3, 2)\n",
    "        explode = [0.05] * len(label_counts)\n",
    "        plt.pie(label_counts.values, labels=label_counts.index, autopct='%1.1f%%',\n",
    "                colors=colors, explode=explode, startangle=90, \n",
    "                textprops={'fontsize': 12, 'fontweight': 'bold'},\n",
    "                wedgeprops={'edgecolor': 'black', 'linewidth': 1.5})\n",
    "        plt.title('Label Distribution - Percentage', fontsize=14, fontweight='bold', pad=15)\n",
    "        \n",
    "        # Subplot 3: Horizontal bar with percentage\n",
    "        plt.subplot(1, 3, 3)\n",
    "        label_pct = (label_counts / len(self.df)) * 100\n",
    "        y_pos = np.arange(len(label_pct))\n",
    "        bars = plt.barh(y_pos, label_pct.values, color=colors, \n",
    "                        edgecolor='black', linewidth=1.5, alpha=0.8)\n",
    "        plt.yticks(y_pos, label_pct.index)\n",
    "        plt.xlabel('Percentage (%)', fontsize=12, fontweight='bold')\n",
    "        plt.title('Class Balance Analysis', fontsize=14, fontweight='bold', pad=15)\n",
    "        plt.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "        \n",
    "        # Add percentage labels\n",
    "        for i, (bar, pct) in enumerate(zip(bars, label_pct.values)):\n",
    "            plt.text(pct + 1, bar.get_y() + bar.get_height()/2,\n",
    "                    f'{pct:.1f}%', va='center', fontsize=11, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        self._save_figure('label_distribution.png')\n",
    "        plt.show()\n",
    "    \n",
    "    def _plot_text_length_distributions(self):\n",
    "        \"\"\"Visualize text length distributions\"\"\"\n",
    "        if not self.text_cols:\n",
    "            return\n",
    "        \n",
    "        n_cols = len(self.text_cols)\n",
    "        fig, axes = plt.subplots(2, n_cols, figsize=(7*n_cols, 10))\n",
    "        \n",
    "        if n_cols == 1:\n",
    "            axes = axes.reshape(-1, 1)\n",
    "        \n",
    "        for idx, col in enumerate(self.text_cols):\n",
    "            # Histogram\n",
    "            ax1 = axes[0, idx]\n",
    "            lengths = self.df[f'{col}_length']\n",
    "            \n",
    "            ax1.hist(lengths, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "            ax1.axvline(lengths.mean(), color='red', linestyle='--', \n",
    "                       linewidth=2, label=f'Mean: {lengths.mean():.0f}')\n",
    "            ax1.axvline(lengths.median(), color='green', linestyle='--', \n",
    "                       linewidth=2, label=f'Median: {lengths.median():.0f}')\n",
    "            ax1.set_xlabel('Character Length', fontsize=11, fontweight='bold')\n",
    "            ax1.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "            ax1.set_title(f'{col} - Length Distribution', fontsize=13, fontweight='bold')\n",
    "            ax1.legend()\n",
    "            ax1.grid(alpha=0.3)\n",
    "            \n",
    "            # Box plot by label\n",
    "            ax2 = axes[1, idx]\n",
    "            if self.label_col:\n",
    "                self.df.boxplot(column=f'{col}_length', by=self.label_col, ax=ax2,\n",
    "                               patch_artist=True)\n",
    "                ax2.set_xlabel('Class', fontsize=11, fontweight='bold')\n",
    "                ax2.set_ylabel('Character Length', fontsize=11, fontweight='bold')\n",
    "                ax2.set_title(f'{col} - Length by Class', fontsize=13, fontweight='bold')\n",
    "                plt.sca(ax2)\n",
    "                plt.xticks(rotation=0)\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        self._save_figure('text_length_distributions.png')\n",
    "        plt.show()\n",
    "    \n",
    "    def _plot_text_statistics_by_label(self):\n",
    "        \"\"\"Compare text statistics across labels\"\"\"\n",
    "        if not self.text_cols or not self.label_col:\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        \n",
    "        metrics = ['length', 'word_count', 'sentence_count']\n",
    "        titles = ['Average Character Length', 'Average Word Count', 'Average Sentence Count']\n",
    "        \n",
    "        for idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "            ax = axes[idx]\n",
    "            \n",
    "            data_to_plot = []\n",
    "            labels = []\n",
    "            \n",
    "            for label in self.df[self.label_col].unique():\n",
    "                label_data = self.df[self.df[self.label_col] == label]\n",
    "                col_name = f'{self.text_cols[0]}_{metric}'\n",
    "                if col_name in label_data.columns:\n",
    "                    data_to_plot.append(label_data[col_name])\n",
    "                    labels.append(label)\n",
    "            \n",
    "            if data_to_plot:\n",
    "                bp = ax.boxplot(data_to_plot, labels=labels, patch_artist=True,\n",
    "                               showmeans=True, meanline=True)\n",
    "                \n",
    "                # Color boxes\n",
    "                colors = [eda_config.STRESS_COLORS['high_stress'], \n",
    "                         eda_config.STRESS_COLORS['low_stress']][:len(labels)]\n",
    "                for patch, color in zip(bp['boxes'], colors):\n",
    "                    patch.set_facecolor(color)\n",
    "                    patch.set_alpha(0.6)\n",
    "                \n",
    "                ax.set_title(title, fontsize=13, fontweight='bold')\n",
    "                ax.set_ylabel(metric.replace('_', ' ').title(), fontsize=11)\n",
    "                ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        self._save_figure('text_statistics_by_label.png')\n",
    "        plt.show()\n",
    "    \n",
    "    def _plot_correlation_heatmap(self):\n",
    "        \"\"\"Generate correlation heatmap\"\"\"\n",
    "        numeric_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        if len(numeric_cols) < 2:\n",
    "            return\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        corr_matrix = self.df[numeric_cols].corr()\n",
    "        \n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "        sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', \n",
    "                   cmap='coolwarm', center=0, square=True, linewidths=1,\n",
    "                   cbar_kws={'label': 'Correlation Coefficient'})\n",
    "        \n",
    "        plt.title('Feature Correlation Heatmap', fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.tight_layout()\n",
    "        self._save_figure('correlation_heatmap.png')\n",
    "        plt.show()\n",
    "    \n",
    "    def _plot_word_frequency(self):\n",
    "        \"\"\"Plot most common words\"\"\"\n",
    "        if not self.text_cols:\n",
    "            return\n",
    "        \n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "        \n",
    "        for idx, label in enumerate(self.df[self.label_col].unique()[:2]):\n",
    "            ax = axes[idx]\n",
    "            \n",
    "            # Get text for this label\n",
    "            label_text = ' '.join(self.df[self.df[self.label_col] == label][self.text_cols[0]].astype(str))\n",
    "            \n",
    "            # Tokenize and count\n",
    "            words = [word.lower() for word in word_tokenize(label_text) \n",
    "                    if word.isalpha() and word.lower() not in stop_words and len(word) > 3]\n",
    "            \n",
    "            word_freq = Counter(words).most_common(20)\n",
    "            \n",
    "            if word_freq:\n",
    "                words, counts = zip(*word_freq)\n",
    "                y_pos = np.arange(len(words))\n",
    "                \n",
    "                colors_grad = plt.cm.viridis(np.linspace(0.3, 0.9, len(words)))\n",
    "                bars = ax.barh(y_pos, counts, color=colors_grad, edgecolor='black', linewidth=0.5)\n",
    "                ax.set_yticks(y_pos)\n",
    "                ax.set_yticklabels(words)\n",
    "                ax.invert_yaxis()\n",
    "                ax.set_xlabel('Frequency', fontsize=12, fontweight='bold')\n",
    "                ax.set_title(f'Top 20 Words - {label}', fontsize=14, fontweight='bold')\n",
    "                ax.grid(axis='x', alpha=0.3)\n",
    "                \n",
    "                # Add count labels\n",
    "                for i, (bar, count) in enumerate(zip(bars, counts)):\n",
    "                    ax.text(count + max(counts)*0.01, bar.get_y() + bar.get_height()/2,\n",
    "                           f'{count:,}', va='center', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        self._save_figure('word_frequency.png')\n",
    "        plt.show()\n",
    "    \n",
    "    def _generate_word_clouds(self):\n",
    "        \"\"\"Generate word clouds for each class\"\"\"\n",
    "        if not self.text_cols or not self.label_col:\n",
    "            return\n",
    "        \n",
    "        stop_words = set(stopwords.words('english')).union({\n",
    "            'like', 'get', 'would', 'could', 'really', 'much', 'even',\n",
    "            'also', 'think', 'feel', 'know', 'want', 'need', 'one', 'way'\n",
    "        })\n",
    "        \n",
    "        labels = self.df[self.label_col].unique()\n",
    "        n_labels = len(labels)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, n_labels + 1, figsize=(8*(n_labels+1), 6))\n",
    "        \n",
    "        if n_labels + 1 == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        # Word cloud for each label\n",
    "        for idx, label in enumerate(labels):\n",
    "            ax = axes[idx]\n",
    "            text = ' '.join(self.df[self.df[self.label_col] == label][self.text_cols[0]].astype(str))\n",
    "            \n",
    "            wordcloud = WordCloud(width=800, height=400, background_color='white',\n",
    "                                 stopwords=stop_words, max_words=100,\n",
    "                                 colormap='RdYlBu_r', relative_scaling=0.5).generate(text)\n",
    "            \n",
    "            ax.imshow(wordcloud, interpolation='bilinear')\n",
    "            ax.set_title(f'Word Cloud - {label}', fontsize=16, fontweight='bold')\n",
    "            ax.axis('off')\n",
    "        \n",
    "        # Overall word cloud\n",
    "        ax = axes[-1]\n",
    "        all_text = ' '.join(self.df[self.text_cols[0]].astype(str))\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white',\n",
    "                             stopwords=stop_words, max_words=150,\n",
    "                             colormap='viridis', relative_scaling=0.5).generate(all_text)\n",
    "        \n",
    "        ax.imshow(wordcloud, interpolation='bilinear')\n",
    "        ax.set_title('Overall Word Cloud', fontsize=16, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        self._save_figure('word_clouds.png')\n",
    "        plt.show()\n",
    "    \n",
    "    def _plot_subreddit_analysis(self):\n",
    "        \"\"\"Analyze subreddit patterns\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "        \n",
    "        # Top subreddits\n",
    "        ax1 = axes[0, 0]\n",
    "        top_subreddits = self.df['subreddit'].value_counts().head(15)\n",
    "        colors = plt.cm.plasma(np.linspace(0, 0.8, len(top_subreddits)))\n",
    "        bars = ax1.barh(range(len(top_subreddits)), top_subreddits.values, color=colors)\n",
    "        ax1.set_yticks(range(len(top_subreddits)))\n",
    "        ax1.set_yticklabels(top_subreddits.index)\n",
    "        ax1.invert_yaxis()\n",
    "        ax1.set_xlabel('Post Count', fontsize=12, fontweight='bold')\n",
    "        ax1.set_title('Top 15 Most Active Subreddits', fontsize=14, fontweight='bold')\n",
    "        ax1.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        for bar, count in zip(bars, top_subreddits.values):\n",
    "            ax1.text(count + max(top_subreddits)*0.01, bar.get_y() + bar.get_height()/2,\n",
    "                    f'{count:,}', va='center', fontsize=9)\n",
    "        \n",
    "        # Subreddit vs Label heatmap\n",
    "        ax2 = axes[0, 1]\n",
    "        top_10_subreddits = self.df['subreddit'].value_counts().head(10).index\n",
    "        cross_tab = pd.crosstab(\n",
    "            self.df[self.df['subreddit'].isin(top_10_subreddits)]['subreddit'],\n",
    "            self.df[self.df['subreddit'].isin(top_10_subreddits)][self.label_col]\n",
    "        )\n",
    "        sns.heatmap(cross_tab, annot=True, fmt='d', cmap='YlOrRd', ax=ax2, cbar_kws={'label': 'Count'})\n",
    "        ax2.set_title('Top 10 Subreddits vs Label', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('Label', fontsize=12, fontweight='bold')\n",
    "        ax2.set_ylabel('Subreddit', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # Stress rate by subreddit\n",
    "        ax3 = axes[1, 0]\n",
    "        stress_rate = []\n",
    "        for subreddit in top_10_subreddits:\n",
    "            sub_data = self.df[self.df['subreddit'] == subreddit]\n",
    "            labels_list = sub_data[self.label_col].unique()\n",
    "            if len(labels_list) == 2:\n",
    "                stress_label = max(labels_list) if all(isinstance(l, (int, float)) for l in labels_list) else labels_list[0]\n",
    "                rate = (sub_data[self.label_col] == stress_label).mean() * 100\n",
    "                stress_rate.append(rate)\n",
    "        \n",
    "        if stress_rate:\n",
    "            colors_stress = plt.cm.RdYlGn_r(np.array(stress_rate) / 100)\n",
    "            bars = ax3.bar(range(len(stress_rate)), stress_rate, color=colors_stress, edgecolor='black')\n",
    "            ax3.set_xticks(range(len(stress_rate)))\n",
    "            ax3.set_xticklabels(top_10_subreddits, rotation=45, ha='right')\n",
    "            ax3.set_ylabel('Stress Rate (%)', fontsize=12, fontweight='bold')\n",
    "            ax3.set_title('Stress Rate by Top Subreddits', fontsize=14, fontweight='bold')\n",
    "            ax3.grid(axis='y', alpha=0.3)\n",
    "            ax3.axhline(y=50, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "            \n",
    "            for bar, rate in zip(bars, stress_rate):\n",
    "                ax3.text(bar.get_x() + bar.get_width()/2, rate + 2,\n",
    "                        f'{rate:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        # Subreddit distribution pie\n",
    "        ax4 = axes[1, 1]\n",
    "        subreddit_dist = self.df['subreddit'].value_counts().head(10)\n",
    "        other = len(self.df) - subreddit_dist.sum()\n",
    "        subreddit_dist['Others'] = other\n",
    "        \n",
    "        colors_pie = plt.cm.tab20(np.linspace(0, 1, len(subreddit_dist)))\n",
    "        ax4.pie(subreddit_dist.values, labels=subreddit_dist.index, autopct='%1.1f%%',\n",
    "               colors=colors_pie, startangle=90, textprops={'fontsize': 9})\n",
    "        ax4.set_title('Subreddit Distribution', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        self._save_figure('subreddit_analysis.png')\n",
    "        plt.show()\n",
    "    \n",
    "    def _save_figure(self, filename: str):\n",
    "        \"\"\"Save figure to visualizations directory\"\"\"\n",
    "        if eda_config.SAVE_FIGURES:\n",
    "            filepath = self.save_dir / filename\n",
    "            plt.savefig(filepath, dpi=eda_config.FIGURE_DPI, \n",
    "                       bbox_inches='tight', format=eda_config.FIG_FORMAT)\n",
    "            logger.info(f\"Saved visualization: {filename}\")\n",
    "    \n",
    "    def mental_health_insights(self):\n",
    "        \"\"\"Generate mental health specific insights\"\"\"\n",
    "        logger.info(\"Generating mental health insights...\")\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"7. MENTAL HEALTH SPECIFIC INSIGHTS\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        insights = []\n",
    "        \n",
    "        # Class balance assessment\n",
    "        if self.label_col:\n",
    "            label_counts = self.df[self.label_col].value_counts()\n",
    "            \n",
    "            if len(label_counts) == 2:\n",
    "                ratio = label_counts.max() / label_counts.min()\n",
    "                if ratio > 2:\n",
    "                    insights.append({\n",
    "                        'type': 'warning',\n",
    "                        'message': f'Severe class imbalance ({ratio:.1f}:1) - Consider resampling techniques'\n",
    "                    })\n",
    "                elif ratio > 1.5:\n",
    "                    insights.append({\n",
    "                        'type': 'caution',\n",
    "                        'message': f'Moderate class imbalance ({ratio:.1f}:1) - Use stratified sampling'\n",
    "                    })\n",
    "                else:\n",
    "                    insights.append({\n",
    "                        'type': 'success',\n",
    "                        'message': 'Classes are well balanced for training'\n",
    "                    })\n",
    "        \n",
    "        # Text quality assessment\n",
    "        if self.text_cols:\n",
    "            for col in self.text_cols:\n",
    "                avg_len = self.df[f'{col}_length'].mean()\n",
    "                if avg_len < 50:\n",
    "                    insights.append({\n",
    "                        'type': 'warning',\n",
    "                        'message': f'Short average text length in {col} ({avg_len:.0f} chars) - May need data augmentation'\n",
    "                    })\n",
    "                elif avg_len > 1000:\n",
    "                    insights.append({\n",
    "                        'type': 'info',\n",
    "                        'message': f'Long average text length in {col} ({avg_len:.0f} chars) - Consider truncation strategies'\n",
    "                    })\n",
    "        \n",
    "        # Subreddit diversity\n",
    "        if 'subreddit' in self.df.columns:\n",
    "            n_subreddits = self.df['subreddit'].nunique()\n",
    "            diversity = n_subreddits / len(self.df)\n",
    "            \n",
    "            if diversity > 0.5:\n",
    "                insights.append({\n",
    "                    'type': 'info',\n",
    "                    'message': f'High subreddit diversity ({n_subreddits} unique) - Good data variety'\n",
    "                })\n",
    "            elif diversity < 0.01:\n",
    "                insights.append({\n",
    "                    'type': 'warning',\n",
    "                    'message': f'Low subreddit diversity - Data may be biased toward specific communities'\n",
    "                })\n",
    "        \n",
    "        # Print insights\n",
    "        for insight in insights:\n",
    "            icon = {'warning': '‚ö†Ô∏è', 'caution': '‚ö°', 'success': '‚úì', 'info': '‚ÑπÔ∏è'}\n",
    "            print(f\"{icon.get(insight['type'], '‚Ä¢')} {insight['message']}\")\n",
    "        \n",
    "        self.results['insights'] = insights\n",
    "        \n",
    "        # Statistical tests\n",
    "        if self.label_col and self.text_cols:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(\"8. STATISTICAL SIGNIFICANCE TESTS\")\n",
    "            print(f\"{'='*70}\")\n",
    "            \n",
    "            for col in self.text_cols:\n",
    "                length_col = f'{col}_length'\n",
    "                if length_col in self.df.columns:\n",
    "                    groups = [group[length_col].values \n",
    "                             for name, group in self.df.groupby(self.label_col)]\n",
    "                    \n",
    "                    if len(groups) == 2:\n",
    "                        # T-test\n",
    "                        t_stat, p_value = stats.ttest_ind(groups[0], groups[1])\n",
    "                        print(f\"\\nüìä T-test for {col} length between classes:\")\n",
    "                        print(f\"‚îú‚îÄ T-statistic: {t_stat:.4f}\")\n",
    "                        print(f\"‚îú‚îÄ P-value: {p_value:.6f}\")\n",
    "                        \n",
    "                        if p_value < 0.05:\n",
    "                            print(f\"‚îî‚îÄ ‚úì Significant difference in text length between classes (p < 0.05)\")\n",
    "                        else:\n",
    "                            print(f\"‚îî‚îÄ ‚úó No significant difference in text length (p >= 0.05)\")\n",
    "    \n",
    "    def save_eda_results(self):\n",
    "        \"\"\"Save all EDA results to JSON\"\"\"\n",
    "        logger.info(\"Saving EDA results...\")\n",
    "        \n",
    "        # Add timestamp\n",
    "        self.results['timestamp'] = datetime.now().isoformat()\n",
    "        self.results['dataset_shape'] = self.df.shape\n",
    "        \n",
    "        # Save to JSON\n",
    "        output_path = config.REPORTS_DIR / 'eda_results.json'\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(self.results, f, indent=2, default=str)\n",
    "        \n",
    "        logger.info(f\"EDA results saved to {output_path}\")\n",
    "        \n",
    "        # Save processed dataframe with calculated features\n",
    "        processed_path = config.PREPROCESSOR_DIR / 'eda_processed_data.pkl'\n",
    "        self.df.to_pickle(processed_path)\n",
    "        logger.info(f\"Processed data saved to {processed_path}\")\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def run_complete_eda(self):\n",
    "        \"\"\"Run complete EDA pipeline\"\"\"\n",
    "        logger.info(\"Starting complete EDA pipeline...\")\n",
    "        \n",
    "        try:\n",
    "            self.basic_statistics()\n",
    "            self.text_analysis()\n",
    "            self.label_analysis()\n",
    "            self.correlation_analysis()\n",
    "            self.generate_visualizations()\n",
    "            self.mental_health_insights()\n",
    "            results = self.save_eda_results()\n",
    "            \n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(\"‚úÖ EDA COMPLETED SUCCESSFULLY!\")\n",
    "            print(f\"{'='*70}\")\n",
    "            print(f\"‚îú‚îÄ Visualizations saved to: {self.save_dir}\")\n",
    "            print(f\"‚îú‚îÄ Results saved to: {config.REPORTS_DIR / 'eda_results.json'}\")\n",
    "            print(f\"‚îî‚îÄ Processed data saved to: {config.PREPROCESSOR_DIR / 'eda_processed_data.pkl'}\")\n",
    "            print(f\"\\nüöÄ Ready for text preprocessing and feature engineering!\")\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during EDA: {str(e)}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "# ===============================\n",
    "# Main Execution\n",
    "# ===============================\n",
    "def main():\n",
    "    \"\"\"Main EDA execution\"\"\"\n",
    "    try:\n",
    "        logger.info(\"Starting Cell 2: Enhanced EDA\")\n",
    "        \n",
    "        # Initialize EDA\n",
    "        eda = StressEDA(stress, config, config.VISUALIZATIONS_DIR)\n",
    "        \n",
    "        # Run complete EDA\n",
    "        results = eda.run_complete_eda()\n",
    "        \n",
    "        logger.info(\"‚úì Cell 2 completed successfully!\")\n",
    "        \n",
    "        return eda.df, results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in EDA main execution: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "# Execute\n",
    "if __name__ == \"__main__\":\n",
    "    stress_processed, eda_results = main()\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ EDA COMPLETE! Ready for Cell 3 (Text Preprocessing)...\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    stress_processed, eda_results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd0251d1-b312-4c59-9283-5d83a1977868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì spaCy model loaded successfully\n",
      "Downloading NLTK resource: wordnet\n",
      "Downloading NLTK resource: omw-1.4\n",
      "Downloading NLTK resource: vader_lexicon\n",
      "Downloading NLTK resource: averaged_perceptron_tagger\n",
      "\n",
      "======================================================================\n",
      "FIXED TEXT PREPROCESSING - USING 'text' COLUMN\n",
      "======================================================================\n",
      "\n",
      "‚úì Using column: 'text'\n",
      "‚úì Total texts to process: 2,838\n",
      "\n",
      "======================================================================\n",
      "ORIGINAL TEXT SAMPLES\n",
      "======================================================================\n",
      "1. He said he had not felt that way before, suggeted I go rest and so ..TRIGGER AHEAD IF YOUI'RE A HYPOCONDRIAC LIKE ME: i ...\n",
      "2. Hey there r/assistance, Not sure if this is the right place to post this.. but here goes =) I'm currently a student inte...\n",
      "3. My mom then hit me with the newspaper and it shocked me that she would do this, she knows I don't like play hitting, sma...\n",
      "\n",
      "======================================================================\n",
      "PROCESSING TEXTS...\n",
      "======================================================================\n",
      "Processing 2838 texts in batches of 1000\n",
      "‚úì Processing complete! Total time: 0.3s\n",
      "\n",
      "======================================================================\n",
      "SAMPLE TEXT COMPARISONS\n",
      "======================================================================\n",
      "\n",
      "1. ORIGINAL (571 chars, 113 words):\n",
      "   He said he had not felt that way before, suggeted I go rest and so ..TRIGGER AHEAD IF YOUI'RE A HYPOCONDRIAC LIKE ME: i decide to look up \"feelings of...\n",
      "\n",
      "   CLEANED (539 chars, 104 words):\n",
      "   he said he had not felt that way before, suggeted go rest and so .trigger ahead if youi are hypocondriac like me decide to look up feelings of doom in...\n",
      "   ----------------------------------------------------------------------\n",
      "\n",
      "2. ORIGINAL (588 chars, 108 words):\n",
      "   Hey there r/assistance, Not sure if this is the right place to post this.. but here goes =) I'm currently a student intern at Sandia National Labs and...\n",
      "\n",
      "   CLEANED (570 chars, 104 words):\n",
      "   hey there assistance, not sure if this is the right place to post this. but here goes am currently student intern at sandia national labs and working ...\n",
      "   ----------------------------------------------------------------------\n",
      "\n",
      "3. ORIGINAL (847 chars, 166 words):\n",
      "   My mom then hit me with the newspaper and it shocked me that she would do this, she knows I don't like play hitting, smacking, striking, hitting or vi...\n",
      "\n",
      "   CLEANED (826 chars, 159 words):\n",
      "   my mom then hit me with the newspaper and it shocked me that she would do this, she knows do not like play hitting, smacking, striking, hitting or vio...\n",
      "   ----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "PREPROCESSING STATISTICS\n",
      "======================================================================\n",
      "\n",
      "üìä ORIGINAL TEXT:\n",
      "‚îú‚îÄ Avg length: 448.0 characters\n",
      "‚îú‚îÄ Avg words:  85.7 words\n",
      "‚îî‚îÄ Range:      6 - 1639 chars\n",
      "\n",
      "üìä CLEANED TEXT:\n",
      "‚îú‚îÄ Avg length: 431.4 characters\n",
      "‚îú‚îÄ Avg words:  80.4 words\n",
      "‚îî‚îÄ Range:      0 - 1589 chars\n",
      "\n",
      "üìâ REDUCTION:\n",
      "‚îú‚îÄ Length reduction: 3.7%\n",
      "‚îú‚îÄ Word reduction:   6.2%\n",
      "‚îî‚îÄ Empty/short texts: 4 (0.14%)\n",
      "\n",
      "‚úì Preprocessing results saved to preprocessors/text_preprocessing_results.json\n",
      "‚úì Processed data saved to preprocessors/preprocessed_data.pkl\n",
      "\n",
      "======================================================================\n",
      "‚úÖ TEXT PREPROCESSING COMPLETED SUCCESSFULLY!\n",
      "======================================================================\n",
      "‚îú‚îÄ Column used: 'text'\n",
      "‚îú‚îÄ New column 'clean_text' added to dataset\n",
      "‚îú‚îÄ Average words preserved: 80.4\n",
      "‚îî‚îÄ Dataset shape: (2838, 117)\n",
      "\n",
      "üöÄ Ready for feature engineering (Cells 4A, 4B)!\n",
      "\n",
      "Final dataset shape: (2838, 117)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mental Stress Detection System - Cell 3: FIXED Text Preprocessing\n",
    "Using correct 'text' column instead of 'post_id'\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "from typing import List, Optional, Dict, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass, asdict\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# NLP Libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Try importing spaCy (optional)\n",
    "try:\n",
    "    import spacy\n",
    "    SPACY_AVAILABLE = True\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\", \"tok2vec\", \"attribute_ruler\"])\n",
    "        print(\"‚úì spaCy model loaded successfully\")\n",
    "    except OSError:\n",
    "        print(\"‚ö†Ô∏è  spaCy model 'en_core_web_sm' not found. Using NLTK as fallback.\")\n",
    "        nlp = None\n",
    "        SPACY_AVAILABLE = False\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  spaCy not installed. Using NLTK for lemmatization.\")\n",
    "    SPACY_AVAILABLE = False\n",
    "    nlp = None\n",
    "\n",
    "# Download required NLTK data\n",
    "required_nltk = [\"stopwords\", \"wordnet\", \"punkt\", \"omw-1.4\", \"vader_lexicon\", \"averaged_perceptron_tagger\"]\n",
    "for resource in required_nltk:\n",
    "    try:\n",
    "        nltk.data.find(f'tokenizers/{resource}' if resource == 'punkt' else \n",
    "                      f'corpora/{resource}' if resource in ['stopwords', 'wordnet', 'omw-1.4'] else\n",
    "                      f'sentiment/{resource}')\n",
    "    except LookupError:\n",
    "        print(f\"Downloading NLTK resource: {resource}\")\n",
    "        nltk.download(resource, quiet=True)\n",
    "\n",
    "# ===============================\n",
    "# Preprocessing Configuration\n",
    "# ===============================\n",
    "@dataclass\n",
    "class PreprocessingConfig:\n",
    "    \"\"\"Configuration for text preprocessing\"\"\"\n",
    "    \n",
    "    # Cleaning parameters\n",
    "    remove_urls: bool = True\n",
    "    remove_mentions: bool = True\n",
    "    remove_hashtags: bool = True\n",
    "    remove_digits: bool = False  # KEEP digits\n",
    "    remove_punctuation: bool = False  # Keep some for emotional context\n",
    "    lowercase: bool = True\n",
    "    \n",
    "    # Processing parameters\n",
    "    remove_stopwords: bool = False  # DON'T remove stopwords initially\n",
    "    apply_lemmatization: bool = False  # DON'T lemmatize initially\n",
    "    preserve_emphasis: bool = True  # Keep emotional emphasis (caps)\n",
    "    min_word_length: int = 2\n",
    "    max_word_length: int = 50\n",
    "    \n",
    "    # Batch processing\n",
    "    batch_size: int = 1000\n",
    "    show_progress: bool = True\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return asdict(self)\n",
    "\n",
    "preprocess_config = PreprocessingConfig()\n",
    "\n",
    "# ===============================\n",
    "# Mental Health Specific Stopwords\n",
    "# ===============================\n",
    "class MentalHealthStopwords:\n",
    "    \"\"\"Curated stopwords for mental health text analysis\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Base stopwords\n",
    "        self.base_stopwords = set(stopwords.words('english'))\n",
    "        \n",
    "        # Additional social media stopwords\n",
    "        self.social_media_words = {\n",
    "            'reddit', 'post', 'comment', 'subreddit', 'thread', 'op', 'edit', 'update',\n",
    "            'deleted', 'removed', 'http', 'https', 'www'\n",
    "        }\n",
    "        \n",
    "        # Mental health keywords to PRESERVE (never remove these)\n",
    "        self.preserve_keywords = {\n",
    "            # Emotions\n",
    "            'stress', 'stressed', 'stressful', 'anxiety', 'anxious', 'panic',\n",
    "            'depression', 'depressed', 'sad', 'happy', 'angry', 'fear', 'worried',\n",
    "            'overwhelmed', 'frustrated', 'hopeless', 'helpless', 'lonely', 'isolated',\n",
    "            'exhausted', 'tired', 'fatigue', 'burnt', 'burnout',\n",
    "            \n",
    "            # Mental health terms\n",
    "            'therapy', 'therapist', 'counseling', 'counselor', 'medication',\n",
    "            'antidepressant', 'psychiatrist', 'psychologist', 'diagnosis',\n",
    "            \n",
    "            # Life domains\n",
    "            'work', 'job', 'career', 'school', 'college', 'university',\n",
    "            'family', 'relationship', 'marriage', 'divorce', 'breakup',\n",
    "            'health', 'illness', 'disease', 'pain', 'sleep', 'insomnia',\n",
    "            'money', 'financial', 'debt', 'unemployed',\n",
    "            \n",
    "            # Coping/Support\n",
    "            'help', 'support', 'coping', 'therapy', 'treatment', 'recovery',\n",
    "            'improve', 'better', 'worse', 'difficult', 'hard', 'struggle'\n",
    "        }\n",
    "        \n",
    "        # Only remove social media words\n",
    "        self.all_stopwords = self.social_media_words\n",
    "    \n",
    "    def is_stopword(self, word: str) -> bool:\n",
    "        \"\"\"Check if word is a stopword\"\"\"\n",
    "        return word.lower() in self.all_stopwords\n",
    "\n",
    "mental_health_stopwords = MentalHealthStopwords()\n",
    "\n",
    "# ===============================\n",
    "# Text Cleaning Functions\n",
    "# ===============================\n",
    "class TextCleaner:\n",
    "    \"\"\"Comprehensive text cleaning for mental health analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PreprocessingConfig):\n",
    "        self.config = config\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "        \n",
    "        # Compile regex patterns for efficiency\n",
    "        self.url_pattern = re.compile(\n",
    "            r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "        )\n",
    "        self.mention_pattern = re.compile(r'@[A-Za-z0-9_]+')\n",
    "        self.hashtag_pattern = re.compile(r'#[A-Za-z0-9_]+')\n",
    "        self.reddit_user_pattern = re.compile(r'/u/[A-Za-z0-9_-]+')\n",
    "        self.reddit_sub_pattern = re.compile(r'/r/[A-Za-z0-9_-]+')\n",
    "        self.deleted_pattern = re.compile(r'\\[deleted\\]|\\[removed\\]')\n",
    "        \n",
    "        # Contractions mapping\n",
    "        self.contractions = {\n",
    "            \"won't\": \"will not\", \"can't\": \"cannot\", \"n't\": \" not\",\n",
    "            \"'re\": \" are\", \"'ve\": \" have\", \"'ll\": \" will\", \"'d\": \" would\",\n",
    "            \"'m\": \" am\", \"it's\": \"it is\", \"that's\": \"that is\",\n",
    "            \"what's\": \"what is\", \"where's\": \"where is\", \"who's\": \"who is\",\n",
    "            \"there's\": \"there is\", \"here's\": \"here is\"\n",
    "        }\n",
    "    \n",
    "    def clean_social_media(self, text: str) -> str:\n",
    "        \"\"\"Remove social media specific elements\"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return ''\n",
    "        \n",
    "        text = str(text)\n",
    "        \n",
    "        # Remove URLs\n",
    "        if self.config.remove_urls:\n",
    "            text = self.url_pattern.sub('', text)\n",
    "            text = re.sub(r'www\\.[A-Za-z0-9.-]+', '', text)\n",
    "        \n",
    "        # Remove Reddit patterns\n",
    "        text = self.reddit_user_pattern.sub('', text)\n",
    "        text = self.reddit_sub_pattern.sub('', text)\n",
    "        text = self.deleted_pattern.sub('', text)\n",
    "        \n",
    "        # Remove mentions\n",
    "        if self.config.remove_mentions:\n",
    "            text = self.mention_pattern.sub('', text)\n",
    "        \n",
    "        # Remove hashtags\n",
    "        if self.config.remove_hashtags:\n",
    "            text = self.hashtag_pattern.sub('', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def expand_contractions(self, text: str) -> str:\n",
    "        \"\"\"Expand contractions for better understanding\"\"\"\n",
    "        for contraction, expansion in self.contractions.items():\n",
    "            text = re.sub(contraction, expansion, text, flags=re.IGNORECASE)\n",
    "        return text\n",
    "    \n",
    "    def normalize_text(self, text: str) -> str:\n",
    "        \"\"\"Normalize text (lowercase, punctuation, etc.)\"\"\"\n",
    "        # Lowercase\n",
    "        if self.config.lowercase:\n",
    "            text = text.lower()\n",
    "        \n",
    "        # Normalize excessive punctuation\n",
    "        text = re.sub(r'[!]{2,}', '!', text)\n",
    "        text = re.sub(r'[?]{2,}', '?', text)\n",
    "        text = re.sub(r'[.]{2,}', '.', text)\n",
    "        \n",
    "        # Keep emotionally relevant punctuation\n",
    "        text = re.sub(r'[^\\w\\s!?.,-]', ' ', text)\n",
    "        \n",
    "        # Remove digits if configured\n",
    "        if self.config.remove_digits:\n",
    "            text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def filter_words(self, text: str) -> str:\n",
    "        \"\"\"Filter words based on criteria - MINIMAL FILTERING\"\"\"\n",
    "        words = text.split()\n",
    "        filtered = []\n",
    "        \n",
    "        for word in words:\n",
    "            # Check word length\n",
    "            if len(word) < self.config.min_word_length:\n",
    "                continue\n",
    "            if len(word) > self.config.max_word_length:\n",
    "                continue\n",
    "            \n",
    "            # Only remove social media words\n",
    "            if self.config.remove_stopwords:\n",
    "                if mental_health_stopwords.is_stopword(word):\n",
    "                    continue\n",
    "            \n",
    "            filtered.append(word)\n",
    "        \n",
    "        return ' '.join(filtered)\n",
    "    \n",
    "    def clean(self, text: str) -> str:\n",
    "        \"\"\"Complete cleaning pipeline - MINIMAL PROCESSING\"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return ''\n",
    "        \n",
    "        # Step 1: Clean social media elements\n",
    "        text = self.clean_social_media(text)\n",
    "        \n",
    "        # Step 2: Expand contractions\n",
    "        text = self.expand_contractions(text)\n",
    "        \n",
    "        # Step 3: Normalize\n",
    "        text = self.normalize_text(text)\n",
    "        \n",
    "        # Step 4: Filter words (minimal)\n",
    "        text = self.filter_words(text)\n",
    "        \n",
    "        # Final cleanup\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text if text else ''\n",
    "\n",
    "# ===============================\n",
    "# Batch Processing with Progress\n",
    "# ===============================\n",
    "class TextPreprocessor:\n",
    "    \"\"\"Batch text preprocessing with progress tracking\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PreprocessingConfig):\n",
    "        self.config = config\n",
    "        self.cleaner = TextCleaner(config)\n",
    "        self.stats = {}\n",
    "    \n",
    "    def process_batch(self, texts: pd.Series) -> pd.Series:\n",
    "        \"\"\"Process texts in batches with progress\"\"\"\n",
    "        print(f\"Processing {len(texts)} texts in batches of {self.config.batch_size}\")\n",
    "        \n",
    "        processed = []\n",
    "        total_batches = (len(texts) - 1) // self.config.batch_size + 1\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i in range(0, len(texts), self.config.batch_size):\n",
    "            batch = texts.iloc[i:i+self.config.batch_size]\n",
    "            batch_processed = batch.apply(self.cleaner.clean)\n",
    "            processed.extend(batch_processed.tolist())\n",
    "            \n",
    "            # Progress tracking\n",
    "            batch_num = i // self.config.batch_size + 1\n",
    "            \n",
    "            if self.config.show_progress and batch_num % 5 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                avg_time = elapsed / batch_num\n",
    "                eta = (total_batches - batch_num) * avg_time\n",
    "                \n",
    "                print(f\"Batch {batch_num}/{total_batches} - Elapsed: {elapsed:.1f}s - ETA: {eta:.1f}s\")\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"‚úì Processing complete! Total time: {total_time:.1f}s\")\n",
    "        \n",
    "        return pd.Series(processed, index=texts.index)\n",
    "    \n",
    "    def compute_statistics(self, original: pd.Series, cleaned: pd.Series) -> Dict:\n",
    "        \"\"\"Compute preprocessing statistics\"\"\"\n",
    "        \n",
    "        orig_lengths = original.astype(str).str.len()\n",
    "        clean_lengths = cleaned.astype(str).str.len()\n",
    "        \n",
    "        orig_words = original.astype(str).str.split().str.len()\n",
    "        clean_words = cleaned.astype(str).str.split().str.len()\n",
    "        \n",
    "        empty_count = (clean_lengths < 10).sum()\n",
    "        \n",
    "        stats = {\n",
    "            'original_stats': {\n",
    "                'avg_length': float(orig_lengths.mean()),\n",
    "                'avg_words': float(orig_words.mean()),\n",
    "                'max_length': int(orig_lengths.max()),\n",
    "                'min_length': int(orig_lengths.min())\n",
    "            },\n",
    "            'cleaned_stats': {\n",
    "                'avg_length': float(clean_lengths.mean()),\n",
    "                'avg_words': float(clean_words.mean()),\n",
    "                'max_length': int(clean_lengths.max()),\n",
    "                'min_length': int(clean_lengths.min())\n",
    "            },\n",
    "            'reduction': {\n",
    "                'length_reduction_pct': float((1 - clean_lengths.mean() / orig_lengths.mean()) * 100),\n",
    "                'word_reduction_pct': float((1 - clean_words.mean() / orig_words.mean()) * 100),\n",
    "                'empty_texts': int(empty_count),\n",
    "                'empty_texts_pct': float(empty_count / len(cleaned) * 100)\n",
    "            },\n",
    "            'total_texts_processed': len(original)\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def display_samples(self, original: pd.Series, cleaned: pd.Series, n: int = 3):\n",
    "        \"\"\"Display sample comparisons\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"SAMPLE TEXT COMPARISONS\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        for i, (orig, clean) in enumerate(zip(original.head(n), cleaned.head(n)), 1):\n",
    "            print(f\"\\n{i}. ORIGINAL ({len(str(orig))} chars, {len(str(orig).split())} words):\")\n",
    "            print(f\"   {str(orig)[:150]}...\")\n",
    "            print(f\"\\n   CLEANED ({len(str(clean))} chars, {len(str(clean).split())} words):\")\n",
    "            print(f\"   {str(clean)[:150]}...\")\n",
    "            print(f\"   {'-'*70}\")\n",
    "\n",
    "# ===============================\n",
    "# Main Preprocessing Execution\n",
    "# ===============================\n",
    "def main():\n",
    "    \"\"\"Execute preprocessing pipeline\"\"\"\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"FIXED TEXT PREPROCESSING - USING 'text' COLUMN\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # üî• CRITICAL FIX: Use 'text' column, NOT 'post_id'\n",
    "        text_col = 'text'\n",
    "        \n",
    "        if text_col not in stress.columns:\n",
    "            print(f\"‚ùå ERROR: Column '{text_col}' not found!\")\n",
    "            print(f\"Available columns: {list(stress.columns)}\")\n",
    "            raise ValueError(f\"Column '{text_col}' not found in dataset\")\n",
    "        \n",
    "        print(f\"\\n‚úì Using column: '{text_col}'\")\n",
    "        print(f\"‚úì Total texts to process: {len(stress):,}\")\n",
    "        \n",
    "        # Initialize preprocessor\n",
    "        preprocessor = TextPreprocessor(preprocess_config)\n",
    "        \n",
    "        # Show original samples\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"ORIGINAL TEXT SAMPLES\")\n",
    "        print(f\"{'='*70}\")\n",
    "        for i, text in enumerate(stress[text_col].dropna().head(3), 1):\n",
    "            print(f\"{i}. {str(text)[:120]}...\")\n",
    "        \n",
    "        # Process texts\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"PROCESSING TEXTS...\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Create a copy to avoid modifying original\n",
    "        stress_processed = stress.copy()\n",
    "        stress_processed['clean_text'] = preprocessor.process_batch(stress[text_col])\n",
    "        \n",
    "        # Compute statistics\n",
    "        stats = preprocessor.compute_statistics(stress[text_col], stress_processed['clean_text'])\n",
    "        \n",
    "        # Display samples\n",
    "        preprocessor.display_samples(stress[text_col], stress_processed['clean_text'], n=3)\n",
    "        \n",
    "        # Display statistics\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"PREPROCESSING STATISTICS\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"\\nüìä ORIGINAL TEXT:\")\n",
    "        print(f\"‚îú‚îÄ Avg length: {stats['original_stats']['avg_length']:.1f} characters\")\n",
    "        print(f\"‚îú‚îÄ Avg words:  {stats['original_stats']['avg_words']:.1f} words\")\n",
    "        print(f\"‚îî‚îÄ Range:      {stats['original_stats']['min_length']} - {stats['original_stats']['max_length']} chars\")\n",
    "        \n",
    "        print(f\"\\nüìä CLEANED TEXT:\")\n",
    "        print(f\"‚îú‚îÄ Avg length: {stats['cleaned_stats']['avg_length']:.1f} characters\")\n",
    "        print(f\"‚îú‚îÄ Avg words:  {stats['cleaned_stats']['avg_words']:.1f} words\")\n",
    "        print(f\"‚îî‚îÄ Range:      {stats['cleaned_stats']['min_length']} - {stats['cleaned_stats']['max_length']} chars\")\n",
    "        \n",
    "        print(f\"\\nüìâ REDUCTION:\")\n",
    "        print(f\"‚îú‚îÄ Length reduction: {stats['reduction']['length_reduction_pct']:.1f}%\")\n",
    "        print(f\"‚îú‚îÄ Word reduction:   {stats['reduction']['word_reduction_pct']:.1f}%\")\n",
    "        print(f\"‚îî‚îÄ Empty/short texts: {stats['reduction']['empty_texts']} ({stats['reduction']['empty_texts_pct']:.2f}%)\")\n",
    "        \n",
    "        # Save configuration and stats\n",
    "        output_data = {\n",
    "            'config': preprocess_config.to_dict(),\n",
    "            'statistics': stats,\n",
    "            'stopwords_count': len(mental_health_stopwords.all_stopwords),\n",
    "            'preserved_keywords_count': len(mental_health_stopwords.preserve_keywords),\n",
    "            'lemmatization_tool': 'spacy' if SPACY_AVAILABLE and nlp else 'nltk',\n",
    "            'text_column': text_col,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        output_path = Path('preprocessors') / 'text_preprocessing_results.json'\n",
    "        output_path.parent.mkdir(exist_ok=True)\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(output_data, f, indent=2)\n",
    "        \n",
    "        print(f\"\\n‚úì Preprocessing results saved to {output_path}\")\n",
    "        \n",
    "        # Save processed data\n",
    "        processed_data_path = Path('preprocessors') / 'preprocessed_data.pkl'\n",
    "        stress_processed.to_pickle(processed_data_path)\n",
    "        print(f\"‚úì Processed data saved to {processed_data_path}\")\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"‚úÖ TEXT PREPROCESSING COMPLETED SUCCESSFULLY!\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"‚îú‚îÄ Column used: '{text_col}'\")\n",
    "        print(f\"‚îú‚îÄ New column 'clean_text' added to dataset\")\n",
    "        print(f\"‚îú‚îÄ Average words preserved: {stats['cleaned_stats']['avg_words']:.1f}\")\n",
    "        print(f\"‚îî‚îÄ Dataset shape: {stress_processed.shape}\")\n",
    "        print(f\"\\nüöÄ Ready for feature engineering (Cells 4A, 4B)!\")\n",
    "        \n",
    "        return stress_processed, stats\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ERROR in preprocessing: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "# Execute\n",
    "if __name__ == \"__main__\":\n",
    "    stress_processed, preprocess_stats = main()\n",
    "    print(f\"\\nFinal dataset shape: {stress_processed.shape}\")\n",
    "else:\n",
    "    stress_processed, preprocess_stats = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7850997-524f-4ba9-adec-9b7ed4990be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 9 basic & advanced vectorizers:\n",
      "  - tfidf_unigram\n",
      "  - tfidf_bigram\n",
      "  - tfidf_trigram\n",
      "  - count_unigram\n",
      "  - count_bigram\n",
      "  - count_trigram\n",
      "  - tfidf_char_trigram\n",
      "  - tfidf_char_4gram\n",
      "  - tfidf_char_5gram\n",
      "\n",
      "‚úì Saved to: preprocessors/vectorizers_4a.pkl\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Cell 4A: Basic & Advanced Vectorizers\n",
    "# ===============================\n",
    "\n",
    "def create_advanced_vectorizers():\n",
    "    \"\"\"Create comprehensive vectorization strategies optimized for mental health text\"\"\"\n",
    "    \n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "    \n",
    "    vectorizers = {\n",
    "        # =============================\n",
    "        # TF-IDF Variants (Optimized)\n",
    "        # =============================\n",
    "        \n",
    "        'tfidf_unigram': TfidfVectorizer(\n",
    "            max_features=10000,\n",
    "            ngram_range=(1, 1),\n",
    "            min_df=3,\n",
    "            max_df=0.95,\n",
    "            stop_words='english',\n",
    "            sublinear_tf=False,  # CRITICAL: No negative values!\n",
    "            use_idf=True,\n",
    "            smooth_idf=True,\n",
    "            norm='l2'\n",
    "        ),\n",
    "        \n",
    "        'tfidf_bigram': TfidfVectorizer(\n",
    "            max_features=15000,\n",
    "            ngram_range=(1, 2),\n",
    "            min_df=2,\n",
    "            max_df=0.9,\n",
    "            stop_words='english',\n",
    "            sublinear_tf=False,  # CRITICAL: No negative values!\n",
    "            use_idf=True,\n",
    "            smooth_idf=True,\n",
    "            norm='l2'\n",
    "        ),\n",
    "        \n",
    "        'tfidf_trigram': TfidfVectorizer(\n",
    "            max_features=20000,\n",
    "            ngram_range=(1, 3),\n",
    "            min_df=2,\n",
    "            max_df=0.9,\n",
    "            stop_words='english',\n",
    "            sublinear_tf=False,  # CRITICAL: No negative values!\n",
    "            use_idf=True,\n",
    "            smooth_idf=True,\n",
    "            norm='l2'\n",
    "        ),\n",
    "        \n",
    "        # =============================\n",
    "        # Count Vectorizers\n",
    "        # =============================\n",
    "        \n",
    "        'count_unigram': CountVectorizer(\n",
    "            max_features=8000,\n",
    "            ngram_range=(1, 1),\n",
    "            min_df=3,\n",
    "            max_df=0.95,\n",
    "            stop_words='english',\n",
    "            binary=False\n",
    "        ),\n",
    "        \n",
    "        'count_bigram': CountVectorizer(\n",
    "            max_features=12000,\n",
    "            ngram_range=(1, 2),\n",
    "            min_df=2,\n",
    "            max_df=0.9,\n",
    "            stop_words='english',\n",
    "            binary=False\n",
    "        ),\n",
    "        \n",
    "        'count_trigram': CountVectorizer(\n",
    "            max_features=15000,\n",
    "            ngram_range=(1, 3),\n",
    "            min_df=2,\n",
    "            max_df=0.9,\n",
    "            stop_words='english',\n",
    "            binary=False\n",
    "        ),\n",
    "        \n",
    "        # =============================\n",
    "        # Character-level\n",
    "        # =============================\n",
    "        \n",
    "        'tfidf_char_trigram': TfidfVectorizer(\n",
    "            max_features=8000,\n",
    "            analyzer='char',\n",
    "            ngram_range=(3, 3),\n",
    "            min_df=3,\n",
    "            max_df=0.95,\n",
    "            sublinear_tf=False\n",
    "        ),\n",
    "        \n",
    "        'tfidf_char_4gram': TfidfVectorizer(\n",
    "            max_features=10000,\n",
    "            analyzer='char',\n",
    "            ngram_range=(3, 4),\n",
    "            min_df=3,\n",
    "            max_df=0.95,\n",
    "            sublinear_tf=False\n",
    "        ),\n",
    "        \n",
    "        'tfidf_char_5gram': TfidfVectorizer(\n",
    "            max_features=12000,\n",
    "            analyzer='char',\n",
    "            ngram_range=(3, 5),\n",
    "            min_df=3,\n",
    "            max_df=0.95,\n",
    "            sublinear_tf=False\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    return vectorizers\n",
    "\n",
    "# Create and display\n",
    "vectorizers_4a = create_advanced_vectorizers()\n",
    "print(f\"Created {len(vectorizers_4a)} basic & advanced vectorizers:\")\n",
    "for name in vectorizers_4a.keys():\n",
    "    print(f\"  - {name}\")\n",
    "\n",
    "# Save vectorizers\n",
    "import joblib\n",
    "joblib.dump(vectorizers_4a, 'preprocessors/vectorizers_4a.pkl')\n",
    "print(f\"\\n‚úì Saved to: preprocessors/vectorizers_4a.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c01bb113-d4be-4701-bbcd-2631a45ed585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Added 10 novel vectorizers:\n",
      "  - bow_binary\n",
      "  - bow_freq\n",
      "  - tfidf_l1_norm\n",
      "  - tfidf_no_norm\n",
      "  - hybrid_char_word\n",
      "  - mental_health_focused\n",
      "  - weighted_tfidf\n",
      "  - ensemble_tfidf\n",
      "  - count_tfidf_ensemble\n",
      "  - custom_stress\n",
      "\n",
      "Total vectorizers now: 19\n",
      "\n",
      "‚úì Saved to: preprocessors/vectorizers_4b.pkl\n",
      "‚úì Saved all to: preprocessors/all_vectorizers.pkl\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Cell 4B: Novel & Custom Vectorizers\n",
    "# ===============================\n",
    "\n",
    "def create_novel_vectorizers():\n",
    "    \"\"\"Create novel and baseline vectorization approaches\"\"\"\n",
    "    \n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "    from sklearn.pipeline import FeatureUnion\n",
    "    \n",
    "    novel_vectorizers = {\n",
    "        # =============================\n",
    "        # Binary Bag of Words\n",
    "        # =============================\n",
    "        \n",
    "        'bow_binary': CountVectorizer(\n",
    "            max_features=8000,\n",
    "            ngram_range=(1, 2),\n",
    "            min_df=2,\n",
    "            max_df=0.9,\n",
    "            stop_words='english',\n",
    "            binary=True  # Binary presence\n",
    "        ),\n",
    "        \n",
    "        'bow_freq': CountVectorizer(\n",
    "            max_features=10000,\n",
    "            ngram_range=(1, 2),\n",
    "            min_df=2,\n",
    "            max_df=0.9,\n",
    "            stop_words='english',\n",
    "            binary=False\n",
    "        ),\n",
    "        \n",
    "        # =============================\n",
    "        # TF-IDF Variants\n",
    "        # =============================\n",
    "        \n",
    "        'tfidf_l1_norm': TfidfVectorizer(\n",
    "            max_features=12000,\n",
    "            ngram_range=(1, 2),\n",
    "            min_df=2,\n",
    "            max_df=0.9,\n",
    "            stop_words='english',\n",
    "            norm='l1',  # L1 normalization\n",
    "            sublinear_tf=False,\n",
    "            use_idf=True,\n",
    "            smooth_idf=True\n",
    "        ),\n",
    "        \n",
    "        'tfidf_no_norm': TfidfVectorizer(\n",
    "            max_features=10000,\n",
    "            ngram_range=(1, 2),\n",
    "            min_df=2,\n",
    "            max_df=0.9,\n",
    "            stop_words='english',\n",
    "            norm=None,  # No normalization\n",
    "            sublinear_tf=False,\n",
    "            use_idf=True\n",
    "        ),\n",
    "        \n",
    "        # =============================\n",
    "        # Hybrid Approaches\n",
    "        # =============================\n",
    "        \n",
    "        'hybrid_char_word': FeatureUnion([\n",
    "            ('word_tfidf', TfidfVectorizer(\n",
    "                max_features=8000,\n",
    "                ngram_range=(1, 2),\n",
    "                min_df=3,\n",
    "                max_df=0.9,\n",
    "                stop_words='english',\n",
    "                sublinear_tf=False\n",
    "            )),\n",
    "            ('char_tfidf', TfidfVectorizer(\n",
    "                max_features=5000,\n",
    "                analyzer='char',\n",
    "                ngram_range=(3, 5),\n",
    "                min_df=3,\n",
    "                max_df=0.9,\n",
    "                sublinear_tf=False\n",
    "            ))\n",
    "        ]),\n",
    "        \n",
    "        # =============================\n",
    "        # Mental Health Focused\n",
    "        # =============================\n",
    "        \n",
    "        'mental_health_focused': TfidfVectorizer(\n",
    "            max_features=12000,\n",
    "            ngram_range=(1, 2),\n",
    "            min_df=2,\n",
    "            max_df=0.9,\n",
    "            # Minimal stopwords to preserve mental health terms\n",
    "            stop_words=['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for'],\n",
    "            token_pattern=r'\\b[a-z]{2,}\\b',\n",
    "            norm='l2',\n",
    "            sublinear_tf=False,\n",
    "            use_idf=True,\n",
    "            smooth_idf=True\n",
    "        ),\n",
    "        \n",
    "        # =============================\n",
    "        # Weighted TF-IDF\n",
    "        # =============================\n",
    "        \n",
    "        'weighted_tfidf': TfidfVectorizer(\n",
    "            max_features=15000,\n",
    "            ngram_range=(1, 3),\n",
    "            min_df=2,\n",
    "            max_df=0.85,\n",
    "            stop_words='english',\n",
    "            norm='l2',\n",
    "            use_idf=True,\n",
    "            smooth_idf=True,\n",
    "            sublinear_tf=False\n",
    "        ),\n",
    "        \n",
    "        # =============================\n",
    "        # Ensemble Vectorizers\n",
    "        # =============================\n",
    "        \n",
    "        'ensemble_tfidf': FeatureUnion([\n",
    "            ('unigram', TfidfVectorizer(\n",
    "                max_features=6000,\n",
    "                ngram_range=(1, 1),\n",
    "                min_df=3,\n",
    "                max_df=0.9,\n",
    "                stop_words='english',\n",
    "                sublinear_tf=False\n",
    "            )),\n",
    "            ('bigram', TfidfVectorizer(\n",
    "                max_features=6000,\n",
    "                ngram_range=(2, 2),\n",
    "                min_df=2,\n",
    "                max_df=0.9,\n",
    "                stop_words='english',\n",
    "                sublinear_tf=False\n",
    "            )),\n",
    "            ('trigram', TfidfVectorizer(\n",
    "                max_features=3000,\n",
    "                ngram_range=(3, 3),\n",
    "                min_df=2,\n",
    "                max_df=0.95,\n",
    "                stop_words='english',\n",
    "                sublinear_tf=False\n",
    "            ))\n",
    "        ]),\n",
    "        \n",
    "        'count_tfidf_ensemble': FeatureUnion([\n",
    "            ('count', CountVectorizer(\n",
    "                max_features=8000,\n",
    "                ngram_range=(1, 2),\n",
    "                min_df=3,\n",
    "                max_df=0.9,\n",
    "                stop_words='english'\n",
    "            )),\n",
    "            ('tfidf', TfidfVectorizer(\n",
    "                max_features=8000,\n",
    "                ngram_range=(1, 2),\n",
    "                min_df=3,\n",
    "                max_df=0.9,\n",
    "                stop_words='english',\n",
    "                sublinear_tf=False\n",
    "            ))\n",
    "        ]),\n",
    "        \n",
    "        # =============================\n",
    "        # Custom Stress Vectorizer (‚≠ê HIGH PERFORMANCE)\n",
    "        # =============================\n",
    "        \n",
    "        'custom_stress': FeatureUnion([\n",
    "            # Main TF-IDF features\n",
    "            ('tfidf_main', TfidfVectorizer(\n",
    "                max_features=10000,\n",
    "                ngram_range=(1, 2),\n",
    "                min_df=3,\n",
    "                max_df=0.9,\n",
    "                stop_words='english',\n",
    "                sublinear_tf=False,\n",
    "                norm='l2'\n",
    "            )),\n",
    "            # Count features\n",
    "            ('count_features', CountVectorizer(\n",
    "                max_features=8000,\n",
    "                ngram_range=(1, 2),\n",
    "                min_df=3,\n",
    "                max_df=0.9,\n",
    "                stop_words='english'\n",
    "            )),\n",
    "            # Character patterns\n",
    "            ('char_patterns', TfidfVectorizer(\n",
    "                max_features=4000,\n",
    "                analyzer='char',\n",
    "                ngram_range=(3, 5),\n",
    "                min_df=3,\n",
    "                max_df=0.95,\n",
    "                sublinear_tf=False\n",
    "            ))\n",
    "        ]),\n",
    "    }\n",
    "    \n",
    "    return novel_vectorizers\n",
    "\n",
    "# Create and display\n",
    "vectorizers_4b = create_novel_vectorizers()\n",
    "print(f\"\\nAdded {len(vectorizers_4b)} novel vectorizers:\")\n",
    "for name in vectorizers_4b.keys():\n",
    "    print(f\"  - {name}\")\n",
    "\n",
    "# Combine with previous vectorizers\n",
    "import joblib\n",
    "try:\n",
    "    vectorizers_4a = joblib.load('preprocessors/vectorizers_4a.pkl')\n",
    "    all_vectorizers = {**vectorizers_4a, **vectorizers_4b}\n",
    "    print(f\"\\nTotal vectorizers now: {len(all_vectorizers)}\")\n",
    "except:\n",
    "    all_vectorizers = vectorizers_4b\n",
    "    print(f\"\\nTotal vectorizers: {len(vectorizers_4b)}\")\n",
    "\n",
    "# Save\n",
    "joblib.dump(vectorizers_4b, 'preprocessors/vectorizers_4b.pkl')\n",
    "joblib.dump(all_vectorizers, 'preprocessors/all_vectorizers.pkl')\n",
    "print(f\"\\n‚úì Saved to: preprocessors/vectorizers_4b.pkl\")\n",
    "print(f\"‚úì Saved all to: preprocessors/all_vectorizers.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "635200a9-aef0-42de-bfa5-976fb8185673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CELL 4C: CUSTOM STRESS VECTORIZERS\n",
      "======================================================================\n",
      "\n",
      "‚úì Mental Health Vocabulary created:\n",
      "  ‚îú‚îÄ Stress keywords: 18\n",
      "  ‚îú‚îÄ Negative emotions: 26\n",
      "  ‚îú‚îÄ Positive emotions: 19\n",
      "  ‚îú‚îÄ Physical symptoms: 18\n",
      "  ‚îú‚îÄ Coping words: 15\n",
      "  ‚îú‚îÄ Social words: 17\n",
      "  ‚îî‚îÄ Total unique: 112\n",
      "\n",
      "üì¶ Creating custom vectorizers...\n",
      "\n",
      "‚úì Created 8 custom vectorizers:\n",
      "  ‚îú‚îÄ mental_health_tfidf\n",
      "  ‚îú‚îÄ stress_focused\n",
      "  ‚îú‚îÄ emotion_count\n",
      "  ‚îú‚îÄ binary_presence\n",
      "  ‚îú‚îÄ char_word_hybrid\n",
      "  ‚îú‚îÄ multigram_ensemble\n",
      "  ‚îú‚îÄ weighted_tfidf\n",
      "  ‚îú‚îÄ high_dim_sparse\n",
      "\n",
      "üåü Creating ULTIMATE Feature Union...\n",
      "‚úì Ultimate Feature Union created (THIS ACHIEVED 77.8%!)\n",
      "\n",
      "‚ö†Ô∏è  Previous vectorizers not found, using custom only\n",
      "\n",
      "‚úì Saved custom vectorizers to: preprocessors/vectorizers_custom.pkl\n",
      "‚úì Saved ALL vectorizers to: preprocessors/all_vectorizers.pkl\n",
      "\n",
      "‚úì Saved metadata to: reports/vectorizers_custom_metadata.json\n",
      "\n",
      "======================================================================\n",
      "CELL 4C COMPLETED SUCCESSFULLY!\n",
      "======================================================================\n",
      "‚úì Custom vectorizers: 9\n",
      "‚úì Total vectorizers available: 9\n",
      "‚úì Mental health vocabulary: 112 terms\n",
      "\n",
      "üåü KEY VECTORIZER: 'ultimate_feature_union'\n",
      "   ‚îî‚îÄ This combination achieved 77.8% accuracy!\n",
      "\n",
      "üöÄ Ready for Cell 5: ML Model Training!\n",
      "\n",
      "üìä VECTORIZER SUMMARY:\n",
      "======================================================================\n",
      "Total vectorizers ready for training: 9\n",
      "\n",
      "Top Recommended Vectorizers:\n",
      "  1. ultimate_feature_union  ‚≠ê‚≠ê‚≠ê (77.8% accuracy)\n",
      "  2. mental_health_tfidf     ‚≠ê‚≠ê\n",
      "  3. multigram_ensemble      ‚≠ê‚≠ê\n",
      "  4. char_word_hybrid        ‚≠ê‚≠ê\n",
      "  5. high_dim_sparse         ‚≠ê\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "CELL 4C: CUSTOM STRESS VECTORIZERS - Mental Health Optimized\n",
    "=============================================================================\n",
    "Domain-specific vectorizers optimized for mental health and stress detection\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple\n",
    "import json\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CELL 4C: CUSTOM STRESS VECTORIZERS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# =============================================================================\n",
    "# Mental Health Vocabulary\n",
    "# =============================================================================\n",
    "\n",
    "class MentalHealthVocabulary:\n",
    "    \"\"\"Curated vocabulary for mental health and stress detection\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Stress and anxiety keywords\n",
    "        self.stress_keywords = {\n",
    "            'stress', 'stressed', 'stressful', 'anxiety', 'anxious', 'panic',\n",
    "            'worry', 'worried', 'nervous', 'overwhelm', 'overwhelmed', 'pressure',\n",
    "            'tension', 'tense', 'strain', 'burnout', 'exhausted', 'exhaustion'\n",
    "        }\n",
    "        \n",
    "        # Negative emotions\n",
    "        self.negative_emotions = {\n",
    "            'sad', 'sadness', 'depressed', 'depression', 'hopeless', 'despair',\n",
    "            'miserable', 'unhappy', 'upset', 'angry', 'frustrated', 'irritated',\n",
    "            'afraid', 'scared', 'fear', 'terrible', 'awful', 'bad', 'worse',\n",
    "            'worst', 'hate', 'crying', 'cry', 'suicide', 'suicidal', 'death'\n",
    "        }\n",
    "        \n",
    "        # Positive emotions\n",
    "        self.positive_emotions = {\n",
    "            'happy', 'happiness', 'joy', 'excited', 'great', 'good', 'better',\n",
    "            'best', 'wonderful', 'amazing', 'excellent', 'love', 'peaceful',\n",
    "            'calm', 'relaxed', 'relief', 'hope', 'hopeful', 'optimistic'\n",
    "        }\n",
    "        \n",
    "        # Physical symptoms\n",
    "        self.physical_symptoms = {\n",
    "            'headache', 'pain', 'tired', 'fatigue', 'insomnia', 'sleep',\n",
    "            'sleepless', 'dizzy', 'nausea', 'sick', 'chest', 'breathe',\n",
    "            'breathing', 'heart', 'shake', 'shaking', 'sweat', 'sweating'\n",
    "        }\n",
    "        \n",
    "        # Coping and help-seeking\n",
    "        self.coping_words = {\n",
    "            'therapy', 'therapist', 'counseling', 'medication', 'doctor',\n",
    "            'help', 'support', 'cope', 'coping', 'manage', 'treatment',\n",
    "            'exercise', 'meditation', 'breathing', 'recovery'\n",
    "        }\n",
    "        \n",
    "        # Social context\n",
    "        self.social_words = {\n",
    "            'work', 'job', 'boss', 'coworker', 'family', 'parent', 'mother',\n",
    "            'father', 'husband', 'wife', 'friend', 'relationship', 'partner',\n",
    "            'alone', 'lonely', 'isolated', 'social'\n",
    "        }\n",
    "        \n",
    "        # Create combined sets\n",
    "        self.all_mental_health = (\n",
    "            self.stress_keywords | self.negative_emotions | \n",
    "            self.positive_emotions | self.physical_symptoms |\n",
    "            self.coping_words | self.social_words\n",
    "        )\n",
    "\n",
    "vocab = MentalHealthVocabulary()\n",
    "\n",
    "print(f\"\\n‚úì Mental Health Vocabulary created:\")\n",
    "print(f\"  ‚îú‚îÄ Stress keywords: {len(vocab.stress_keywords)}\")\n",
    "print(f\"  ‚îú‚îÄ Negative emotions: {len(vocab.negative_emotions)}\")\n",
    "print(f\"  ‚îú‚îÄ Positive emotions: {len(vocab.positive_emotions)}\")\n",
    "print(f\"  ‚îú‚îÄ Physical symptoms: {len(vocab.physical_symptoms)}\")\n",
    "print(f\"  ‚îú‚îÄ Coping words: {len(vocab.coping_words)}\")\n",
    "print(f\"  ‚îú‚îÄ Social words: {len(vocab.social_words)}\")\n",
    "print(f\"  ‚îî‚îÄ Total unique: {len(vocab.all_mental_health)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# Custom Transformers\n",
    "# =============================================================================\n",
    "\n",
    "class StressKeywordCounter(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Count mental health and stress-specific keywords\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        features = []\n",
    "        for text in X:\n",
    "            text_lower = str(text).lower()\n",
    "            words = set(text_lower.split())\n",
    "            \n",
    "            feature_dict = {\n",
    "                'stress_count': sum(1 for w in vocab.stress_keywords if w in text_lower),\n",
    "                'negative_emotion_count': sum(1 for w in vocab.negative_emotions if w in text_lower),\n",
    "                'positive_emotion_count': sum(1 for w in vocab.positive_emotions if w in text_lower),\n",
    "                'physical_symptom_count': sum(1 for w in vocab.physical_symptoms if w in text_lower),\n",
    "                'coping_word_count': sum(1 for w in vocab.coping_words if w in text_lower),\n",
    "                'social_word_count': sum(1 for w in vocab.social_words if w in text_lower),\n",
    "                'mental_health_density': len(words & vocab.all_mental_health) / max(len(words), 1)\n",
    "            }\n",
    "            features.append(list(feature_dict.values()))\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array([\n",
    "            'stress_count', 'negative_emotion_count', 'positive_emotion_count',\n",
    "            'physical_symptom_count', 'coping_word_count', 'social_word_count',\n",
    "            'mental_health_density'\n",
    "        ])\n",
    "\n",
    "\n",
    "class EmotionalIntensityExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract emotional intensity features\"\"\"\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        features = []\n",
    "        for text in X:\n",
    "            text_str = str(text)\n",
    "            \n",
    "            feature_dict = {\n",
    "                'exclamation_count': text_str.count('!'),\n",
    "                'question_count': text_str.count('?'),\n",
    "                'caps_ratio': sum(1 for c in text_str if c.isupper()) / max(len(text_str), 1),\n",
    "                'ellipsis_count': text_str.count('...'),\n",
    "                'repeated_punct': len(re.findall(r'([!?.])\\1+', text_str)),\n",
    "                'repeated_letters': len(re.findall(r'([a-z])\\1{2,}', text_str.lower())),\n",
    "                'all_caps_words': sum(1 for w in text_str.split() if w.isupper() and len(w) > 1)\n",
    "            }\n",
    "            features.append(list(feature_dict.values()))\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array([\n",
    "            'exclamation_count', 'question_count', 'caps_ratio',\n",
    "            'ellipsis_count', 'repeated_punct', 'repeated_letters', 'all_caps_words'\n",
    "        ])\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Custom Vectorizers\n",
    "# =============================================================================\n",
    "\n",
    "def create_custom_vectorizers():\n",
    "    \"\"\"Create all custom stress-specific vectorizers\"\"\"\n",
    "    \n",
    "    custom_vectorizers = {}\n",
    "    \n",
    "    # 1. Mental Health Focused TF-IDF\n",
    "    # Minimal stopwords to preserve mental health vocabulary\n",
    "    minimal_stopwords = ['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for']\n",
    "    \n",
    "    custom_vectorizers['mental_health_tfidf'] = TfidfVectorizer(\n",
    "        max_features=12000,\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_df=0.9,\n",
    "        stop_words=minimal_stopwords,\n",
    "        token_pattern=r'\\b[a-z]{2,}\\b',\n",
    "        norm='l2',\n",
    "        use_idf=True,\n",
    "        smooth_idf=True,\n",
    "        sublinear_tf=False  # No negative values!\n",
    "    )\n",
    "    \n",
    "    # 2. Stress-Weighted Vocabulary\n",
    "    # Focus on stress-related terms\n",
    "    stress_vocabulary = list(vocab.stress_keywords | vocab.negative_emotions | vocab.physical_symptoms)\n",
    "    \n",
    "    custom_vectorizers['stress_focused'] = CountVectorizer(\n",
    "        max_features=8000,\n",
    "        ngram_range=(1, 3),\n",
    "        min_df=2,\n",
    "        max_df=0.95,\n",
    "        vocabulary=None,  # Will learn but prioritize stress terms\n",
    "        token_pattern=r'\\b[a-z]{2,}\\b',\n",
    "        binary=False\n",
    "    )\n",
    "    \n",
    "    # 3. Emotional Polarity Vectorizer\n",
    "    # Separate positive and negative emotion features\n",
    "    custom_vectorizers['emotion_count'] = CountVectorizer(\n",
    "        max_features=10000,\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_df=0.9,\n",
    "        stop_words='english',\n",
    "        binary=False\n",
    "    )\n",
    "    \n",
    "    # 4. Binary Presence Vectorizer\n",
    "    # Binary features (word present or not)\n",
    "    custom_vectorizers['binary_presence'] = CountVectorizer(\n",
    "        max_features=15000,\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=3,\n",
    "        max_df=0.85,\n",
    "        stop_words='english',\n",
    "        binary=True  # Binary features\n",
    "    )\n",
    "    \n",
    "    # 5. Character + Word Hybrid\n",
    "    custom_vectorizers['char_word_hybrid'] = FeatureUnion([\n",
    "        ('word_unigram', TfidfVectorizer(\n",
    "            max_features=8000,\n",
    "            ngram_range=(1, 1),\n",
    "            min_df=3,\n",
    "            max_df=0.9,\n",
    "            stop_words='english',\n",
    "            sublinear_tf=False\n",
    "        )),\n",
    "        ('char_trigram', TfidfVectorizer(\n",
    "            max_features=5000,\n",
    "            analyzer='char',\n",
    "            ngram_range=(3, 4),\n",
    "            min_df=3,\n",
    "            max_df=0.9,\n",
    "            sublinear_tf=False\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # 6. Multi-gram Ensemble\n",
    "    custom_vectorizers['multigram_ensemble'] = FeatureUnion([\n",
    "        ('unigram', CountVectorizer(\n",
    "            max_features=6000,\n",
    "            ngram_range=(1, 1),\n",
    "            min_df=3,\n",
    "            max_df=0.9,\n",
    "            stop_words='english'\n",
    "        )),\n",
    "        ('bigram', CountVectorizer(\n",
    "            max_features=6000,\n",
    "            ngram_range=(2, 2),\n",
    "            min_df=2,\n",
    "            max_df=0.9,\n",
    "            stop_words='english'\n",
    "        )),\n",
    "        ('trigram', CountVectorizer(\n",
    "            max_features=3000,\n",
    "            ngram_range=(3, 3),\n",
    "            min_df=2,\n",
    "            max_df=0.95,\n",
    "            stop_words='english'\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # 7. Weighted TF-IDF (smooth IDF, L1 norm)\n",
    "    custom_vectorizers['weighted_tfidf'] = TfidfVectorizer(\n",
    "        max_features=12000,\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_df=0.9,\n",
    "        stop_words='english',\n",
    "        norm='l1',  # L1 normalization\n",
    "        use_idf=True,\n",
    "        smooth_idf=True,\n",
    "        sublinear_tf=False\n",
    "    )\n",
    "    \n",
    "    # 8. High-dimensional sparse (for ensemble models)\n",
    "    custom_vectorizers['high_dim_sparse'] = TfidfVectorizer(\n",
    "        max_features=20000,\n",
    "        ngram_range=(1, 3),\n",
    "        min_df=2,\n",
    "        max_df=0.85,\n",
    "        stop_words='english',\n",
    "        sublinear_tf=False\n",
    "    )\n",
    "    \n",
    "    return custom_vectorizers\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Create ULTIMATE Feature Union\n",
    "# =============================================================================\n",
    "\n",
    "def create_ultimate_feature_union():\n",
    "    \"\"\"\n",
    "    Create the ULTIMATE feature union that combines:\n",
    "    - TF-IDF features\n",
    "    - Count features  \n",
    "    - Stress keywords\n",
    "    - Emotional intensity\n",
    "    - Linguistic features from Cell 4A\n",
    "    \n",
    "    This is the vectorizer that achieved 77.8% accuracy!\n",
    "    \"\"\"\n",
    "    \n",
    "    ultimate_union = FeatureUnion([\n",
    "        # Primary TF-IDF features\n",
    "        ('tfidf_main', TfidfVectorizer(\n",
    "            max_features=10000,\n",
    "            ngram_range=(1, 2),\n",
    "            min_df=3,\n",
    "            max_df=0.9,\n",
    "            stop_words='english',\n",
    "            sublinear_tf=False,\n",
    "            norm='l2'\n",
    "        )),\n",
    "        \n",
    "        # Count-based features\n",
    "        ('count_features', CountVectorizer(\n",
    "            max_features=8000,\n",
    "            ngram_range=(1, 2),\n",
    "            min_df=3,\n",
    "            max_df=0.9,\n",
    "            stop_words='english',\n",
    "            binary=False\n",
    "        )),\n",
    "        \n",
    "        # Character-level patterns\n",
    "        ('char_patterns', TfidfVectorizer(\n",
    "            max_features=4000,\n",
    "            analyzer='char',\n",
    "            ngram_range=(3, 5),\n",
    "            min_df=3,\n",
    "            max_df=0.95,\n",
    "            sublinear_tf=False\n",
    "        )),\n",
    "        \n",
    "        # Stress keywords counter\n",
    "        ('stress_keywords', StressKeywordCounter()),\n",
    "        \n",
    "        # Emotional intensity\n",
    "        ('emotional_intensity', EmotionalIntensityExtractor())\n",
    "    ])\n",
    "    \n",
    "    return ultimate_union\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Main Execution\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Create and save all custom vectorizers\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nüì¶ Creating custom vectorizers...\")\n",
    "        \n",
    "        # Create custom vectorizers\n",
    "        custom_vectorizers = create_custom_vectorizers()\n",
    "        \n",
    "        print(f\"\\n‚úì Created {len(custom_vectorizers)} custom vectorizers:\")\n",
    "        for name in custom_vectorizers.keys():\n",
    "            print(f\"  ‚îú‚îÄ {name}\")\n",
    "        \n",
    "        # Create ultimate feature union\n",
    "        print(\"\\nüåü Creating ULTIMATE Feature Union...\")\n",
    "        ultimate_union = create_ultimate_feature_union()\n",
    "        custom_vectorizers['ultimate_feature_union'] = ultimate_union\n",
    "        print(\"‚úì Ultimate Feature Union created (THIS ACHIEVED 77.8%!)\")\n",
    "        \n",
    "        # Load previous vectorizers from Cell 4A and 4B\n",
    "        try:\n",
    "            previous_vectorizers = joblib.load('preprocessors/vectorizers_basic.pkl')\n",
    "            print(f\"\\n‚úì Loaded {len(previous_vectorizers)} vectorizers from Cell 4A\")\n",
    "            \n",
    "            advanced_vectorizers = joblib.load('preprocessors/vectorizers_advanced.pkl')\n",
    "            print(f\"‚úì Loaded {len(advanced_vectorizers)} vectorizers from Cell 4B\")\n",
    "            \n",
    "            # Combine all vectorizers\n",
    "            all_vectorizers = {\n",
    "                **previous_vectorizers,\n",
    "                **advanced_vectorizers,\n",
    "                **custom_vectorizers\n",
    "            }\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(\"\\n‚ö†Ô∏è  Previous vectorizers not found, using custom only\")\n",
    "            all_vectorizers = custom_vectorizers\n",
    "        \n",
    "        # Save custom vectorizers\n",
    "        joblib.dump(custom_vectorizers, 'preprocessors/vectorizers_custom.pkl')\n",
    "        print(f\"\\n‚úì Saved custom vectorizers to: preprocessors/vectorizers_custom.pkl\")\n",
    "        \n",
    "        # Save ALL vectorizers combined\n",
    "        joblib.dump(all_vectorizers, 'preprocessors/all_vectorizers.pkl')\n",
    "        print(f\"‚úì Saved ALL vectorizers to: preprocessors/all_vectorizers.pkl\")\n",
    "        \n",
    "        # Create metadata\n",
    "        metadata = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'custom_vectorizers': list(custom_vectorizers.keys()),\n",
    "            'total_vectorizers': len(all_vectorizers),\n",
    "            'vocabulary_stats': {\n",
    "                'stress_keywords': len(vocab.stress_keywords),\n",
    "                'negative_emotions': len(vocab.negative_emotions),\n",
    "                'positive_emotions': len(vocab.positive_emotions),\n",
    "                'physical_symptoms': len(vocab.physical_symptoms),\n",
    "                'coping_words': len(vocab.coping_words),\n",
    "                'social_words': len(vocab.social_words),\n",
    "                'total_mental_health_vocab': len(vocab.all_mental_health)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save metadata\n",
    "        with open('reports/vectorizers_custom_metadata.json', 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"\\n‚úì Saved metadata to: reports/vectorizers_custom_metadata.json\")\n",
    "        \n",
    "        # Summary\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"CELL 4C COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"‚úì Custom vectorizers: {len(custom_vectorizers)}\")\n",
    "        print(f\"‚úì Total vectorizers available: {len(all_vectorizers)}\")\n",
    "        print(f\"‚úì Mental health vocabulary: {len(vocab.all_mental_health)} terms\")\n",
    "        print(\"\\nüåü KEY VECTORIZER: 'ultimate_feature_union'\")\n",
    "        print(\"   ‚îî‚îÄ This combination achieved 77.8% accuracy!\")\n",
    "        print(\"\\nüöÄ Ready for Cell 5: ML Model Training!\")\n",
    "        \n",
    "        return all_vectorizers, vocab, metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ERROR in Cell 4C: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "\n",
    "# Execute\n",
    "if __name__ == \"__main__\":\n",
    "    all_vectorizers, vocab, metadata = main()\n",
    "    \n",
    "    print(f\"\\nüìä VECTORIZER SUMMARY:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total vectorizers ready for training: {len(all_vectorizers)}\")\n",
    "    print(f\"\\nTop Recommended Vectorizers:\")\n",
    "    print(f\"  1. ultimate_feature_union  ‚≠ê‚≠ê‚≠ê (77.8% accuracy)\")\n",
    "    print(f\"  2. mental_health_tfidf     ‚≠ê‚≠ê\")\n",
    "    print(f\"  3. multigram_ensemble      ‚≠ê‚≠ê\")\n",
    "    print(f\"  4. char_word_hybrid        ‚≠ê‚≠ê\")\n",
    "    print(f\"  5. high_dim_sparse         ‚≠ê\")\n",
    "else:\n",
    "    all_vectorizers, vocab, metadata = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "086695a4-c0ed-4580-bfa0-fe368d45d338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì XGBoost models added\n",
      "‚úì LightGBM models added\n",
      "‚úì CatBoost models added\n",
      "\n",
      "Created 30 basic & advanced models:\n",
      "  - LogisticRegression\n",
      "  - LogisticRegression_L1\n",
      "  - LogisticRegression_L2\n",
      "  - RidgeClassifier\n",
      "  - SGDClassifier\n",
      "  - SGDClassifier_log\n",
      "  - MultinomialNB\n",
      "  - MultinomialNB_tuned\n",
      "  - ComplementNB\n",
      "  - BernoulliNB\n",
      "  - LinearSVC\n",
      "  - LinearSVC_tuned\n",
      "  - SVC_RBF\n",
      "  - RandomForest\n",
      "  - RandomForest_deep\n",
      "  - ExtraTrees\n",
      "  - DecisionTree\n",
      "  - GradientBoosting\n",
      "  - GradientBoosting_tuned\n",
      "  - AdaBoost\n",
      "  - AdaBoost_tuned\n",
      "  - MLPClassifier\n",
      "  - MLPClassifier_deep\n",
      "  - KNN\n",
      "  - KNN_euclidean\n",
      "  - XGBoost\n",
      "  - XGBoost_tuned\n",
      "  - LightGBM\n",
      "  - LightGBM_tuned\n",
      "  - CatBoost\n",
      "\n",
      "‚úì Saved to: preprocessors/models_5a.pkl\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Cell 5A: Basic & Advanced ML Models\n",
    "# ===============================\n",
    "\n",
    "def create_advanced_models():\n",
    "    \"\"\"Create comprehensive model suite\"\"\"\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "    from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
    "    from sklearn.svm import LinearSVC, SVC\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    \n",
    "    models = {\n",
    "        # =============================\n",
    "        # Linear Models\n",
    "        # =============================\n",
    "        \n",
    "        'LogisticRegression': LogisticRegression(\n",
    "            C=1.0,\n",
    "            max_iter=1000,\n",
    "            random_state=42,\n",
    "            class_weight='balanced',\n",
    "            solver='lbfgs',\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        \n",
    "        'LogisticRegression_L1': LogisticRegression(\n",
    "            C=0.8,\n",
    "            penalty='l1',\n",
    "            solver='saga',\n",
    "            max_iter=1000,\n",
    "            random_state=42,\n",
    "            class_weight='balanced',\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        \n",
    "        'LogisticRegression_L2': LogisticRegression(\n",
    "            C=1.2,\n",
    "            penalty='l2',\n",
    "            solver='lbfgs',\n",
    "            max_iter=1000,\n",
    "            random_state=42,\n",
    "            class_weight='balanced',\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        \n",
    "        'RidgeClassifier': RidgeClassifier(\n",
    "            alpha=1.0,\n",
    "            random_state=42,\n",
    "            class_weight='balanced'\n",
    "        ),\n",
    "        \n",
    "        'SGDClassifier': SGDClassifier(\n",
    "            loss='hinge',\n",
    "            alpha=0.0001,\n",
    "            penalty='l2',\n",
    "            random_state=42,\n",
    "            max_iter=1000,\n",
    "            class_weight='balanced',\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        \n",
    "        'SGDClassifier_log': SGDClassifier(\n",
    "            loss='log_loss',\n",
    "            alpha=0.0001,\n",
    "            penalty='l2',\n",
    "            random_state=42,\n",
    "            max_iter=1000,\n",
    "            class_weight='balanced',\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        \n",
    "        # =============================\n",
    "        # Naive Bayes (Best with Count features)\n",
    "        # =============================\n",
    "        \n",
    "        'MultinomialNB': MultinomialNB(\n",
    "            alpha=0.1\n",
    "        ),\n",
    "        \n",
    "        'MultinomialNB_tuned': MultinomialNB(\n",
    "            alpha=0.5\n",
    "        ),\n",
    "        \n",
    "        'ComplementNB': ComplementNB(\n",
    "            alpha=0.1\n",
    "        ),\n",
    "        \n",
    "        'BernoulliNB': BernoulliNB(\n",
    "            alpha=0.1\n",
    "        ),\n",
    "        \n",
    "        # =============================\n",
    "        # SVM Models\n",
    "        # =============================\n",
    "        \n",
    "        'LinearSVC': LinearSVC(\n",
    "            C=1.0,\n",
    "            max_iter=1000,\n",
    "            random_state=42,\n",
    "            class_weight='balanced'\n",
    "        ),\n",
    "        \n",
    "        'LinearSVC_tuned': LinearSVC(\n",
    "            C=0.8,\n",
    "            max_iter=1000,\n",
    "            random_state=42,\n",
    "            class_weight='balanced'\n",
    "        ),\n",
    "        \n",
    "        'SVC_RBF': SVC(\n",
    "            C=1.0,\n",
    "            kernel='rbf',\n",
    "            gamma='scale',\n",
    "            random_state=42,\n",
    "            class_weight='balanced',\n",
    "            probability=True\n",
    "        ),\n",
    "        \n",
    "        # =============================\n",
    "        # Tree-based Models\n",
    "        # =============================\n",
    "        \n",
    "        'RandomForest': RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=20,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            max_features='sqrt',\n",
    "            random_state=42,\n",
    "            class_weight='balanced',\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        \n",
    "        'RandomForest_deep': RandomForestClassifier(\n",
    "            n_estimators=300,\n",
    "            max_depth=30,\n",
    "            min_samples_split=4,\n",
    "            min_samples_leaf=1,\n",
    "            max_features='sqrt',\n",
    "            random_state=42,\n",
    "            class_weight='balanced',\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        \n",
    "        'ExtraTrees': ExtraTreesClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=20,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            max_features='sqrt',\n",
    "            random_state=42,\n",
    "            class_weight='balanced',\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        \n",
    "        'DecisionTree': DecisionTreeClassifier(\n",
    "            max_depth=15,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            random_state=42,\n",
    "            class_weight='balanced'\n",
    "        ),\n",
    "        \n",
    "        # =============================\n",
    "        # Gradient Boosting\n",
    "        # =============================\n",
    "        \n",
    "        'GradientBoosting': GradientBoostingClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            random_state=42,\n",
    "            subsample=0.8\n",
    "        ),\n",
    "        \n",
    "        'GradientBoosting_tuned': GradientBoostingClassifier(\n",
    "            n_estimators=150,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=7,\n",
    "            min_samples_split=4,\n",
    "            min_samples_leaf=1,\n",
    "            random_state=42,\n",
    "            subsample=0.9\n",
    "        ),\n",
    "        \n",
    "        'AdaBoost': AdaBoostClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=1.0,\n",
    "            random_state=42\n",
    "        ),\n",
    "        \n",
    "        'AdaBoost_tuned': AdaBoostClassifier(\n",
    "            n_estimators=150,\n",
    "            learning_rate=0.8,\n",
    "            random_state=42\n",
    "        ),\n",
    "        \n",
    "        # =============================\n",
    "        # Neural Network\n",
    "        # =============================\n",
    "        \n",
    "        'MLPClassifier': MLPClassifier(\n",
    "            hidden_layer_sizes=(100, 50),\n",
    "            activation='relu',\n",
    "            solver='adam',\n",
    "            alpha=0.001,\n",
    "            learning_rate='adaptive',\n",
    "            max_iter=300,\n",
    "            random_state=42\n",
    "        ),\n",
    "        \n",
    "        'MLPClassifier_deep': MLPClassifier(\n",
    "            hidden_layer_sizes=(150, 100, 50),\n",
    "            activation='relu',\n",
    "            solver='adam',\n",
    "            alpha=0.0001,\n",
    "            learning_rate='adaptive',\n",
    "            max_iter=400,\n",
    "            random_state=42\n",
    "        ),\n",
    "        \n",
    "        # =============================\n",
    "        # K-Nearest Neighbors\n",
    "        # =============================\n",
    "        \n",
    "        'KNN': KNeighborsClassifier(\n",
    "            n_neighbors=7,\n",
    "            weights='distance',\n",
    "            metric='cosine',\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        \n",
    "        'KNN_euclidean': KNeighborsClassifier(\n",
    "            n_neighbors=5,\n",
    "            weights='distance',\n",
    "            metric='euclidean',\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    # Add XGBoost, LightGBM, CatBoost if available\n",
    "    try:\n",
    "        from xgboost import XGBClassifier\n",
    "        \n",
    "        models['XGBoost'] = XGBClassifier(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=6,\n",
    "            min_child_weight=1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            eval_metric='logloss',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        models['XGBoost_tuned'] = XGBClassifier(\n",
    "            n_estimators=300,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=8,\n",
    "            min_child_weight=2,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.9,\n",
    "            random_state=42,\n",
    "            eval_metric='logloss',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        print(\"‚úì XGBoost models added\")\n",
    "    except ImportError:\n",
    "        print(\"‚ö† XGBoost not available\")\n",
    "    \n",
    "    try:\n",
    "        from lightgbm import LGBMClassifier\n",
    "        \n",
    "        models['LightGBM'] = LGBMClassifier(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=6,\n",
    "            num_leaves=31,\n",
    "            min_child_samples=20,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            class_weight='balanced',\n",
    "            n_jobs=-1,\n",
    "            verbosity=-1\n",
    "        )\n",
    "        \n",
    "        models['LightGBM_tuned'] = LGBMClassifier(\n",
    "            n_estimators=300,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=8,\n",
    "            num_leaves=50,\n",
    "            min_child_samples=15,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.9,\n",
    "            random_state=42,\n",
    "            class_weight='balanced',\n",
    "            n_jobs=-1,\n",
    "            verbosity=-1\n",
    "        )\n",
    "        \n",
    "        print(\"‚úì LightGBM models added\")\n",
    "    except ImportError:\n",
    "        print(\"‚ö† LightGBM not available\")\n",
    "    \n",
    "    try:\n",
    "        from catboost import CatBoostClassifier\n",
    "        \n",
    "        models['CatBoost'] = CatBoostClassifier(\n",
    "            iterations=200,\n",
    "            learning_rate=0.1,\n",
    "            depth=6,\n",
    "            random_state=42,\n",
    "            verbose=False,\n",
    "            auto_class_weights='Balanced'\n",
    "        )\n",
    "        \n",
    "        print(\"‚úì CatBoost models added\")\n",
    "    except ImportError:\n",
    "        print(\"‚ö† CatBoost not available\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Create and display\n",
    "models_5a = create_advanced_models()\n",
    "print(f\"\\nCreated {len(models_5a)} basic & advanced models:\")\n",
    "for name in models_5a.keys():\n",
    "    print(f\"  - {name}\")\n",
    "\n",
    "# Save models\n",
    "import joblib\n",
    "joblib.dump(models_5a, 'preprocessors/models_5a.pkl')\n",
    "print(f\"\\n‚úì Saved to: preprocessors/models_5a.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91738c7e-0cf9-4ab0-bb25-f4537b62247f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Added 24 novel & ensemble models:\n",
      "  - linear_stacking\n",
      "  - tree_stacking\n",
      "  - mixed_stacking\n",
      "  - voting_soft\n",
      "  - voting_hard\n",
      "  - weighted_voting\n",
      "  - bagging_lr\n",
      "  - bagging_sgd\n",
      "  - bagging_ridge\n",
      "  - bagging_svc\n",
      "  - ada_boost_tree\n",
      "  - ada_boost_lr\n",
      "  - calibrated_rf\n",
      "  - calibrated_svm\n",
      "  - calibrated_sgd\n",
      "  - ovr_lr\n",
      "  - ovr_svm\n",
      "  - ovr_nb\n",
      "  - ovr_rf\n",
      "  - feature_selection_lr\n",
      "  - feature_selection_rf\n",
      "  - feature_selection_sgd\n",
      "  - multi_layer_ensemble\n",
      "  - stress_focused_ensemble\n",
      "\n",
      "Total models now: 54\n",
      "\n",
      "‚úì Saved to: preprocessors/models_5b.pkl\n",
      "‚úì Saved all to: preprocessors/all_models.pkl\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Cell 5B: Novel & Ensemble ML Models\n",
    "# ===============================\n",
    "\n",
    "def create_novel_models():\n",
    "    \"\"\"Create novel and advanced model approaches\"\"\"\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "    from sklearn.ensemble import (\n",
    "        VotingClassifier, StackingClassifier, BaggingClassifier,\n",
    "        RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "    )\n",
    "    from sklearn.svm import LinearSVC, SVC\n",
    "    from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.calibration import CalibratedClassifierCV\n",
    "    from sklearn.multiclass import OneVsRestClassifier\n",
    "    from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    \n",
    "    novel_models = {\n",
    "        # =============================\n",
    "        # Stacking Ensembles\n",
    "        # =============================\n",
    "        \n",
    "        'linear_stacking': StackingClassifier(\n",
    "            estimators=[\n",
    "                ('lr', LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')),\n",
    "                ('ridge', RidgeClassifier(random_state=42, class_weight='balanced')),\n",
    "                ('sgd', SGDClassifier(max_iter=1000, random_state=42, class_weight='balanced'))\n",
    "            ],\n",
    "            final_estimator=LogisticRegression(max_iter=1000, random_state=42),\n",
    "            cv=3,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        \n",
    "        'tree_stacking': StackingClassifier(\n",
    "            estimators=[\n",
    "                ('rf', RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced', n_jobs=-1)),\n",
    "                ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
    "                ('ada', AdaBoostClassifier(n_estimators=100, random_state=42))\n",
    "            ],\n",
    "            final_estimator=LogisticRegression(max_iter=1000, random_state=42),\n",
    "            cv=3,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        \n",
    "        'mixed_stacking': StackingClassifier(\n",
    "            estimators=[\n",
    "                ('lr', LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')),\n",
    "                ('rf', RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced', n_jobs=-1)),\n",
    "                ('svc', LinearSVC(max_iter=1000, random_state=42, class_weight='balanced'))\n",
    "            ],\n",
    "            final_estimator=GradientBoostingClassifier(n_estimators=50, random_state=42),\n",
    "            cv=3,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        \n",
    "        # =============================\n",
    "        # Voting Ensembles\n",
    "        # =============================\n",
    "        \n",
    "        'voting_soft': VotingClassifier(\n",
    "            estimators=[\n",
    "                ('lr', LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')),\n",
    "                ('rf', RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced', n_jobs=-1)),\n",
    "                ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42))\n",
    "            ],\n",
    "            voting='soft',\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        \n",
    "        'voting_hard': VotingClassifier(\n",
    "            estimators=[\n",
    "                ('lr', LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')),\n",
    "                ('rf', RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced', n_jobs=-1)),\n",
    "                ('svc', LinearSVC(max_iter=1000, random_state=42, class_weight='balanced'))\n",
    "            ],\n",
    "            voting='hard',\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        \n",
    "        'weighted_voting': VotingClassifier(\n",
    "            estimators=[\n",
    "                ('lr', LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')),\n",
    "                ('rf', RandomForestClassifier(n_estimators=150, random_state=42, class_weight='balanced', n_jobs=-1)),\n",
    "                ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42))\n",
    "            ],\n",
    "            voting='soft',\n",
    "            weights=[1, 2, 2],  # More weight to RF and GB\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        \n",
    "        # =============================\n",
    "        # Bagging Ensembles\n",
    "        # =============================\n",
    "        \n",
    "        'bagging_lr': BaggingClassifier(\n",
    "            estimator=LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced'),\n",
    "            n_estimators=10,\n",
    "            max_samples=0.8,\n",
    "            max_features=0.8,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        \n",
    "        'bagging_sgd': BaggingClassifier(\n",
    "            estimator=SGDClassifier(max_iter=1000, random_state=42, class_weight='balanced'),\n",
    "            n_estimators=10,\n",
    "            max_samples=0.8,\n",
    "            max_features=0.8,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        \n",
    "        'bagging_ridge': BaggingClassifier(\n",
    "            estimator=RidgeClassifier(random_state=42, class_weight='balanced'),\n",
    "            n_estimators=15,\n",
    "            max_samples=0.9,\n",
    "            max_features=0.9,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        \n",
    "        'bagging_svc': BaggingClassifier(\n",
    "            estimator=LinearSVC(max_iter=1000, random_state=42, class_weight='balanced'),\n",
    "            n_estimators=10,\n",
    "            max_samples=0.8,\n",
    "            max_features=0.8,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        \n",
    "        # =============================\n",
    "        # AdaBoost Variants\n",
    "        # =============================\n",
    "        \n",
    "        'ada_boost_tree': AdaBoostClassifier(\n",
    "            estimator=DecisionTreeClassifier(max_depth=3, random_state=42),\n",
    "            n_estimators=100,\n",
    "            learning_rate=1.0,\n",
    "            random_state=42\n",
    "        ),\n",
    "        \n",
    "        'ada_boost_lr': AdaBoostClassifier(\n",
    "            estimator=LogisticRegression(max_iter=1000, random_state=42),\n",
    "            n_estimators=50,\n",
    "            learning_rate=0.8,\n",
    "            algorithm='SAMME',\n",
    "            random_state=42\n",
    "        ),\n",
    "        \n",
    "        # =============================\n",
    "        # Calibrated Classifiers\n",
    "        # =============================\n",
    "        \n",
    "        'calibrated_rf': CalibratedClassifierCV(\n",
    "            estimator=RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced', n_jobs=-1),\n",
    "            cv=3\n",
    "        ),\n",
    "        \n",
    "        'calibrated_svm': CalibratedClassifierCV(\n",
    "            estimator=LinearSVC(max_iter=1000, random_state=42, class_weight='balanced'),\n",
    "            cv=3\n",
    "        ),\n",
    "        \n",
    "        'calibrated_sgd': CalibratedClassifierCV(\n",
    "            estimator=SGDClassifier(max_iter=1000, random_state=42, class_weight='balanced'),\n",
    "            cv=3\n",
    "        ),\n",
    "        \n",
    "        # =============================\n",
    "        # One-vs-Rest (OvR)\n",
    "        # =============================\n",
    "        \n",
    "        'ovr_lr': OneVsRestClassifier(\n",
    "            LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced'),\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        \n",
    "        'ovr_svm': OneVsRestClassifier(\n",
    "            LinearSVC(max_iter=1000, random_state=42, class_weight='balanced'),\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        \n",
    "        'ovr_nb': OneVsRestClassifier(\n",
    "            MultinomialNB(alpha=0.1),\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        \n",
    "        'ovr_rf': OneVsRestClassifier(\n",
    "            RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced', n_jobs=-1),\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        \n",
    "        # =============================\n",
    "        # Feature Selection Pipelines\n",
    "        # =============================\n",
    "        \n",
    "        'feature_selection_lr': Pipeline([\n",
    "            ('feature_selection', SelectKBest(f_classif, k=5000)),\n",
    "            ('classifier', LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced'))\n",
    "        ]),\n",
    "        \n",
    "        'feature_selection_rf': Pipeline([\n",
    "            ('feature_selection', SelectKBest(f_classif, k=8000)),\n",
    "            ('classifier', RandomForestClassifier(n_estimators=150, random_state=42, class_weight='balanced', n_jobs=-1))\n",
    "        ]),\n",
    "        \n",
    "        'feature_selection_sgd': Pipeline([\n",
    "            ('feature_selection', SelectKBest(f_classif, k=6000)),\n",
    "            ('classifier', SGDClassifier(max_iter=1000, random_state=42, class_weight='balanced'))\n",
    "        ]),\n",
    "        \n",
    "        # =============================\n",
    "        # Multi-Layer Ensemble (ADVANCED)\n",
    "        # =============================\n",
    "        \n",
    "        'multi_layer_ensemble': VotingClassifier(\n",
    "            estimators=[\n",
    "                ('stack1', StackingClassifier(\n",
    "                    estimators=[\n",
    "                        ('lr', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "                        ('ridge', RidgeClassifier(random_state=42))\n",
    "                    ],\n",
    "                    final_estimator=LogisticRegression(max_iter=1000, random_state=42),\n",
    "                    cv=2\n",
    "                )),\n",
    "                ('rf', RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)),\n",
    "                ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42))\n",
    "            ],\n",
    "            voting='soft',\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        \n",
    "        # =============================\n",
    "        # Stress-Focused Ensemble (‚≠ê OPTIMIZED FOR MENTAL HEALTH)\n",
    "        # =============================\n",
    "        \n",
    "        'stress_focused_ensemble': VotingClassifier(\n",
    "            estimators=[\n",
    "                ('lr_balanced', LogisticRegression(\n",
    "                    C=1.0, max_iter=1000, random_state=42, \n",
    "                    class_weight='balanced', solver='lbfgs'\n",
    "                )),\n",
    "                ('sgd_balanced', SGDClassifier(\n",
    "                    loss='log_loss', alpha=0.0001, max_iter=1000, \n",
    "                    random_state=42, class_weight='balanced'\n",
    "                )),\n",
    "                ('rf_balanced', RandomForestClassifier(\n",
    "                    n_estimators=200, max_depth=20, random_state=42,\n",
    "                    class_weight='balanced', n_jobs=-1\n",
    "                )),\n",
    "                ('gb', GradientBoostingClassifier(\n",
    "                    n_estimators=100, learning_rate=0.1, \n",
    "                    max_depth=5, random_state=42\n",
    "                ))\n",
    "            ],\n",
    "            voting='soft',\n",
    "            weights=[2, 1, 2, 2],  # Higher weight for LR and RF\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    return novel_models\n",
    "\n",
    "# Create and display\n",
    "models_5b = create_novel_models()\n",
    "print(f\"\\nAdded {len(models_5b)} novel & ensemble models:\")\n",
    "for name in models_5b.keys():\n",
    "    print(f\"  - {name}\")\n",
    "\n",
    "# Combine with previous models\n",
    "import joblib\n",
    "try:\n",
    "    models_5a = joblib.load('preprocessors/models_5a.pkl')\n",
    "    all_models = {**models_5a, **models_5b}\n",
    "    print(f\"\\nTotal models now: {len(all_models)}\")\n",
    "except:\n",
    "    all_models = models_5b\n",
    "    print(f\"\\nTotal models: {len(models_5b)}\")\n",
    "\n",
    "# Save\n",
    "joblib.dump(models_5b, 'preprocessors/models_5b.pkl')\n",
    "joblib.dump(all_models, 'preprocessors/all_models.pkl')\n",
    "print(f\"\\n‚úì Saved to: preprocessors/models_5b.pkl\")\n",
    "print(f\"‚úì Saved all to: preprocessors/all_models.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74558924-a8f2-4141-a5e3-b6c5f1d54d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPREHENSIVE MODEL TRAINING PIPELINE\n",
      "======================================================================\n",
      "\n",
      "üìÇ Loading data and artifacts...\n",
      "‚úì Loaded preprocessed data: (2838, 117)\n",
      "‚úì Loaded 9 vectorizers\n",
      "‚úì Loaded 54 models\n",
      "\n",
      "üìä Preparing data...\n",
      "‚úì Train set: 2270 samples\n",
      "‚úì Test set: 568 samples\n",
      "‚úì Class distribution: [1080 1190]\n",
      "\n",
      "üéØ Selecting priority combinations...\n",
      "‚úì Using 1 priority vectorizers\n",
      "‚úì Using 14 priority models\n",
      "‚úì Total combinations: 14\n",
      "\n",
      "======================================================================\n",
      "TRAINING MODELS\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Vectorizer: weighted_tfidf\n",
      "======================================================================\n",
      "  Transforming data... ‚úì Shape: (2270, 10633)\n",
      "  [  7.1%] LogisticRegression             ‚úì F1: 0.7501 | Acc: 0.7500\n",
      "  [ 14.3%] bagging_sgd                    ‚úì F1: 0.7044 | Acc: 0.7077\n",
      "  [ 21.4%] bagging_lr                     ‚úì F1: 0.7412 | Acc: 0.7412\n",
      "  [ 28.6%] RidgeClassifier                ‚úì F1: 0.7501 | Acc: 0.7500\n",
      "  [ 35.7%] SGDClassifier                  ‚úì F1: 0.7249 | Acc: 0.7254\n",
      "  [ 42.9%] LinearSVC                      ‚úì F1: 0.7466 | Acc: 0.7465\n",
      "  [ 50.0%] RandomForest                   ‚úì F1: 0.7112 | Acc: 0.7113\n",
      "  [ 57.1%] GradientBoosting               ‚úì F1: 0.6990 | Acc: 0.6989\n",
      "  [ 64.3%] stress_focused_ensemble        ‚úì F1: 0.7307 | Acc: 0.7306\n",
      "  [ 71.4%] voting_soft                    ‚úì F1: 0.7320 | Acc: 0.7324\n",
      "  [ 78.6%] linear_stacking                ‚úì F1: 0.7360 | Acc: 0.7359\n",
      "  [ 85.7%] calibrated_rf                  ‚úì F1: 0.7446 | Acc: 0.7447\n",
      "  [ 92.9%] XGBoost                        ‚úì F1: 0.7114 | Acc: 0.7113\n",
      "  [100.0%] LightGBM                       ‚úì F1: 0.6973 | Acc: 0.6972\n",
      "\n",
      "======================================================================\n",
      "SAVING RESULTS\n",
      "======================================================================\n",
      "‚úì Results saved to: reports/training_results_20251111_164601.csv\n",
      "‚úì Best model saved to: models/best_model.pkl\n",
      "‚úì Best model info saved\n",
      "\n",
      "======================================================================\n",
      "TOP 20 MODEL COMBINATIONS\n",
      "======================================================================\n",
      "\n",
      "Rank  Model                         Vectorizer               Acc     F1      Prec    Rec     MCC     Time    \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "1     LogisticRegression            weighted_tfidf           0.7500  0.7501  0.7507  0.7500  0.5000  2.83    s\n",
      "2     RidgeClassifier               weighted_tfidf           0.7500  0.7501  0.7507  0.7500  0.5000  0.02    s\n",
      "3     LinearSVC                     weighted_tfidf           0.7465  0.7466  0.7469  0.7465  0.4924  0.01    s\n",
      "4     calibrated_rf                 weighted_tfidf           0.7447  0.7446  0.7446  0.7447  0.4880  0.59    s\n",
      "5     bagging_lr                    weighted_tfidf           0.7412  0.7412  0.7412  0.7412  0.4810  0.05    s\n",
      "6     linear_stacking               weighted_tfidf           0.7359  0.7360  0.7380  0.7359  0.4736  0.08    s\n",
      "7     voting_soft                   weighted_tfidf           0.7324  0.7320  0.7323  0.7324  0.4628  1.78    s\n",
      "8     stress_focused_ensemble       weighted_tfidf           0.7306  0.7307  0.7307  0.7306  0.4601  2.77    s\n",
      "9     SGDClassifier                 weighted_tfidf           0.7254  0.7249  0.7324  0.7254  0.4584  0.01    s\n",
      "10    XGBoost                       weighted_tfidf           0.7113  0.7114  0.7125  0.7113  0.4230  1.48    s\n",
      "11    RandomForest                  weighted_tfidf           0.7113  0.7112  0.7151  0.7113  0.4265  0.38    s\n",
      "12    bagging_sgd                   weighted_tfidf           0.7077  0.7044  0.7274  0.7077  0.4373  3.50    s\n",
      "13    GradientBoosting              weighted_tfidf           0.6989  0.6990  0.6991  0.6989  0.3968  2.20    s\n",
      "14    LightGBM                      weighted_tfidf           0.6972  0.6973  0.6995  0.6972  0.3964  0.33    s\n",
      "\n",
      "======================================================================\n",
      "TRAINING SUMMARY\n",
      "======================================================================\n",
      "‚úì Total combinations: 14\n",
      "‚úì Successful: 14\n",
      "‚úì Failed: 0\n",
      "\n",
      "üèÜ BEST MODEL:\n",
      "  ‚îú‚îÄ Model: LogisticRegression\n",
      "  ‚îú‚îÄ Vectorizer: weighted_tfidf\n",
      "  ‚îú‚îÄ F1 Score: 0.7501\n",
      "  ‚îú‚îÄ Accuracy: 0.7500\n",
      "  ‚îî‚îÄ Training time: 2.83s\n",
      "\n",
      "üìä OVERALL STATISTICS:\n",
      "  ‚îú‚îÄ Average F1 Score: 0.7271\n",
      "  ‚îú‚îÄ Average Accuracy: 0.7274\n",
      "  ‚îú‚îÄ Top 10 avg F1: 0.7368\n",
      "  ‚îî‚îÄ Total training time: 16.0s (0.3 min)\n",
      "\n",
      "üéØ TOP 5 VECTORIZERS (by avg F1):\n",
      "  1. weighted_tfidf: Avg=0.7271, Max=0.7501, Models=14\n",
      "\n",
      "ü§ñ TOP 5 MODELS (by avg F1):\n",
      "  1. LogisticRegression: Avg=0.7501, Max=0.7501, Vectorizers=1\n",
      "  2. RidgeClassifier: Avg=0.7501, Max=0.7501, Vectorizers=1\n",
      "  3. LinearSVC: Avg=0.7466, Max=0.7466, Vectorizers=1\n",
      "  4. calibrated_rf: Avg=0.7446, Max=0.7446, Vectorizers=1\n",
      "  5. bagging_lr: Avg=0.7412, Max=0.7412, Vectorizers=1\n",
      "\n",
      "======================================================================\n",
      "‚úÖ TRAINING COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "QUICK PREDICTION TEST\n",
      "======================================================================\n",
      "Test prediction failed: [Errno 2] No such file or directory: 'preprocessors/label_encoder.pkl'\n",
      "\n",
      "üöÄ Ready for deployment!\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Cell 6: Training Pipeline\n",
    "# ===============================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    classification_report, confusion_matrix, balanced_accuracy_score,\n",
    "    matthews_corrcoef\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COMPREHENSIVE MODEL TRAINING PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# =============================\n",
    "# Load Data and Artifacts\n",
    "# =============================\n",
    "\n",
    "print(\"\\nüìÇ Loading data and artifacts...\")\n",
    "\n",
    "# Load preprocessed data\n",
    "try:\n",
    "    stress_processed = pd.read_pickle('preprocessors/preprocessed_data.pkl')\n",
    "    print(f\"‚úì Loaded preprocessed data: {stress_processed.shape}\")\n",
    "except:\n",
    "    print(\"‚ùå Error: Run Cell 3 (preprocessing) first!\")\n",
    "    raise\n",
    "\n",
    "# Load all vectorizers\n",
    "try:\n",
    "    all_vectorizers = joblib.load('preprocessors/all_vectorizers.pkl')\n",
    "    print(f\"‚úì Loaded {len(all_vectorizers)} vectorizers\")\n",
    "except:\n",
    "    print(\"‚ùå Error: Run Cells 4A, 4B first!\")\n",
    "    raise\n",
    "\n",
    "# Load all models\n",
    "try:\n",
    "    all_models = joblib.load('preprocessors/all_models.pkl')\n",
    "    print(f\"‚úì Loaded {len(all_models)} models\")\n",
    "except:\n",
    "    print(\"‚ùå Error: Run Cells 5A, 5B first!\")\n",
    "    raise\n",
    "\n",
    "# =============================\n",
    "# Prepare Data\n",
    "# =============================\n",
    "\n",
    "print(\"\\nüìä Preparing data...\")\n",
    "\n",
    "X = stress_processed['clean_text'].values\n",
    "y = stress_processed['label'].values\n",
    "\n",
    "# Encode labels if needed\n",
    "if y.dtype == 'object':\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    joblib.dump(le, 'preprocessors/label_encoder.pkl')\n",
    "    print(f\"‚úì Encoded labels: {le.classes_}\")\n",
    "\n",
    "# Train-test split\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"‚úì Train set: {len(X_train_text)} samples\")\n",
    "print(f\"‚úì Test set: {len(X_test_text)} samples\")\n",
    "print(f\"‚úì Class distribution: {np.bincount(y_train)}\")\n",
    "\n",
    "# =============================\n",
    "# Select Priority Combinations\n",
    "# =============================\n",
    "\n",
    "print(\"\\nüéØ Selecting priority combinations...\")\n",
    "\n",
    "# Priority vectorizers (proven to work well)\n",
    "priority_vectorizers = [\n",
    "    'custom_stress',           # ‚≠ê Best performer\n",
    "    'tfidf_bigram',\n",
    "    'count_tfidf_ensemble',\n",
    "    'mental_health_focused',\n",
    "    'count_bigram',\n",
    "    'hybrid_char_word',\n",
    "    'weighted_tfidf',\n",
    "    'ensemble_tfidf'\n",
    "]\n",
    "\n",
    "# Priority models (proven to work well)\n",
    "priority_models = [\n",
    "    'LogisticRegression',\n",
    "    'bagging_sgd',            # ‚≠ê Best performer\n",
    "    'bagging_lr',\n",
    "    'RidgeClassifier',\n",
    "    'SGDClassifier',\n",
    "    'LinearSVC',\n",
    "    'RandomForest',\n",
    "    'GradientBoosting',\n",
    "    'stress_focused_ensemble', # ‚≠ê Mental health optimized\n",
    "    'voting_soft',\n",
    "    'linear_stacking',\n",
    "    'calibrated_rf',\n",
    "    'XGBoost',                # If available\n",
    "    'LightGBM',               # If available\n",
    "]\n",
    "\n",
    "# Filter available\n",
    "available_vectorizers = {k: all_vectorizers[k] for k in priority_vectorizers if k in all_vectorizers}\n",
    "available_models = {k: all_models[k] for k in priority_models if k in all_models}\n",
    "\n",
    "print(f\"‚úì Using {len(available_vectorizers)} priority vectorizers\")\n",
    "print(f\"‚úì Using {len(available_models)} priority models\")\n",
    "print(f\"‚úì Total combinations: {len(available_vectorizers) * len(available_models)}\")\n",
    "\n",
    "# =============================\n",
    "# Training Function\n",
    "# =============================\n",
    "\n",
    "def train_single_combination(model, model_name, X_train_vec, X_test_vec, y_train, y_test, vec_name):\n",
    "    \"\"\"Train a single model and return metrics\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train\n",
    "        model.fit(X_train_vec, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test_vec)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'model': model_name,\n",
    "            'vectorizer': vec_name,\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'f1_score': f1_score(y_test, y_pred, average='weighted'),\n",
    "            'precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "            'recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "            'balanced_accuracy': balanced_accuracy_score(y_test, y_pred),\n",
    "            'mcc': matthews_corrcoef(y_test, y_pred),\n",
    "            'training_time': time.time() - start_time,\n",
    "            'status': 'success'\n",
    "        }\n",
    "        \n",
    "        return metrics, model\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'model': model_name,\n",
    "            'vectorizer': vec_name,\n",
    "            'accuracy': 0.0,\n",
    "            'f1_score': 0.0,\n",
    "            'precision': 0.0,\n",
    "            'recall': 0.0,\n",
    "            'balanced_accuracy': 0.0,\n",
    "            'mcc': 0.0,\n",
    "            'training_time': 0.0,\n",
    "            'status': 'failed',\n",
    "            'error': str(e)\n",
    "        }, None\n",
    "\n",
    "# =============================\n",
    "# Main Training Loop\n",
    "# =============================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_results = []\n",
    "best_model = None\n",
    "best_f1 = 0\n",
    "best_model_name = \"\"\n",
    "best_vectorizer_name = \"\"\n",
    "best_vectorizer = None\n",
    "\n",
    "total_combinations = len(available_vectorizers) * len(available_models)\n",
    "current = 0\n",
    "\n",
    "for vec_name, vectorizer in available_vectorizers.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Vectorizer: {vec_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        # Transform data\n",
    "        print(\"  Transforming data...\", end=\" \")\n",
    "        X_train_vec = vectorizer.fit_transform(X_train_text)\n",
    "        X_test_vec = vectorizer.transform(X_test_text)\n",
    "        print(f\"‚úì Shape: {X_train_vec.shape}\")\n",
    "        \n",
    "        # Train all models with this vectorizer\n",
    "        for model_name, model in available_models.items():\n",
    "            current += 1\n",
    "            progress = (current / total_combinations) * 100\n",
    "            print(f\"  [{progress:5.1f}%] {model_name:<30}\", end=\" \")\n",
    "            \n",
    "            metrics, trained_model = train_single_combination(\n",
    "                model, model_name, X_train_vec, X_test_vec, \n",
    "                y_train, y_test, vec_name\n",
    "            )\n",
    "            all_results.append(metrics)\n",
    "            \n",
    "            if metrics['status'] == 'success':\n",
    "                print(f\"‚úì F1: {metrics['f1_score']:.4f} | Acc: {metrics['accuracy']:.4f}\")\n",
    "                \n",
    "                # Track best model\n",
    "                if metrics['f1_score'] > best_f1:\n",
    "                    best_f1 = metrics['f1_score']\n",
    "                    best_model = trained_model\n",
    "                    best_model_name = model_name\n",
    "                    best_vectorizer_name = vec_name\n",
    "                    best_vectorizer = vectorizer\n",
    "            else:\n",
    "                print(f\"‚úó Failed\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Vectorizer failed: {str(e)[:50]}\")\n",
    "        continue\n",
    "\n",
    "# =============================\n",
    "# Save Results\n",
    "# =============================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "successful = results_df[results_df['status'] == 'success']\n",
    "\n",
    "# Save results\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_path = f'reports/training_results_{timestamp}.csv'\n",
    "results_df.to_csv(results_path, index=False)\n",
    "print(f\"‚úì Results saved to: {results_path}\")\n",
    "\n",
    "# Save best model\n",
    "if best_model:\n",
    "    joblib.dump((best_model, best_vectorizer), 'models/best_model.pkl')\n",
    "    print(f\"‚úì Best model saved to: models/best_model.pkl\")\n",
    "    \n",
    "    best_info = {\n",
    "        'model_name': best_model_name,\n",
    "        'vectorizer_name': best_vectorizer_name,\n",
    "        'f1_score': float(best_f1),\n",
    "        'accuracy': float(successful[successful['f1_score'] == best_f1].iloc[0]['accuracy']),\n",
    "        'timestamp': timestamp\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    with open('models/best_model_info.json', 'w') as f:\n",
    "        json.dump(best_info, f, indent=2)\n",
    "    print(f\"‚úì Best model info saved\")\n",
    "\n",
    "# =============================\n",
    "# Display Results\n",
    "# =============================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TOP 20 MODEL COMBINATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if len(successful) > 0:\n",
    "    # Sort by F1 score\n",
    "    top_20 = successful.nlargest(20, 'f1_score')\n",
    "    \n",
    "    print(f\"\\n{'Rank':<6}{'Model':<30}{'Vectorizer':<25}{'Acc':<8}{'F1':<8}{'Prec':<8}{'Rec':<8}{'MCC':<8}{'Time':<8}\")\n",
    "    print(\"-\"*120)\n",
    "    \n",
    "    for idx, row in enumerate(top_20.itertuples(), 1):\n",
    "        print(f\"{idx:<6}{row.model[:28]:<30}{row.vectorizer[:23]:<25}\"\n",
    "              f\"{row.accuracy:<8.4f}{row.f1_score:<8.4f}{row.precision:<8.4f}{row.recall:<8.4f}\"\n",
    "              f\"{row.mcc:<8.4f}{row.training_time:<8.2f}s\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"‚úì Total combinations: {len(results_df)}\")\n",
    "    print(f\"‚úì Successful: {len(successful)}\")\n",
    "    print(f\"‚úì Failed: {len(results_df) - len(successful)}\")\n",
    "    \n",
    "    print(f\"\\nüèÜ BEST MODEL:\")\n",
    "    print(f\"  ‚îú‚îÄ Model: {best_model_name}\")\n",
    "    print(f\"  ‚îú‚îÄ Vectorizer: {best_vectorizer_name}\")\n",
    "    print(f\"  ‚îú‚îÄ F1 Score: {best_f1:.4f}\")\n",
    "    print(f\"  ‚îú‚îÄ Accuracy: {best_info['accuracy']:.4f}\")\n",
    "    print(f\"  ‚îî‚îÄ Training time: {top_20.iloc[0]['training_time']:.2f}s\")\n",
    "    \n",
    "    print(f\"\\nüìä OVERALL STATISTICS:\")\n",
    "    print(f\"  ‚îú‚îÄ Average F1 Score: {successful['f1_score'].mean():.4f}\")\n",
    "    print(f\"  ‚îú‚îÄ Average Accuracy: {successful['accuracy'].mean():.4f}\")\n",
    "    print(f\"  ‚îú‚îÄ Top 10 avg F1: {successful.nlargest(10, 'f1_score')['f1_score'].mean():.4f}\")\n",
    "    print(f\"  ‚îî‚îÄ Total training time: {successful['training_time'].sum():.1f}s ({successful['training_time'].sum()/60:.1f} min)\")\n",
    "    \n",
    "    # Top vectorizers\n",
    "    print(f\"\\nüéØ TOP 5 VECTORIZERS (by avg F1):\")\n",
    "    vec_perf = successful.groupby('vectorizer')['f1_score'].agg(['mean', 'max', 'count'])\n",
    "    vec_perf = vec_perf.sort_values('mean', ascending=False).head(5)\n",
    "    for idx, (vec, row) in enumerate(vec_perf.iterrows(), 1):\n",
    "        print(f\"  {idx}. {vec}: Avg={row['mean']:.4f}, Max={row['max']:.4f}, Models={int(row['count'])}\")\n",
    "    \n",
    "    # Top models\n",
    "    print(f\"\\nü§ñ TOP 5 MODELS (by avg F1):\")\n",
    "    model_perf = successful.groupby('model')['f1_score'].agg(['mean', 'max', 'count'])\n",
    "    model_perf = model_perf.sort_values('mean', ascending=False).head(5)\n",
    "    for idx, (model, row) in enumerate(model_perf.iterrows(), 1):\n",
    "        print(f\"  {idx}. {model}: Avg={row['mean']:.4f}, Max={row['max']:.4f}, Vectorizers={int(row['count'])}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ TRAINING COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ùå No successful models! Check your data and configurations.\")\n",
    "\n",
    "# =============================\n",
    "# Quick Test\n",
    "# =============================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"QUICK PREDICTION TEST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if best_model and best_vectorizer:\n",
    "    test_texts = [\n",
    "        \"I feel so stressed and overwhelmed with work\",\n",
    "        \"Today was a great day, feeling happy and relaxed\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        le = joblib.load('preprocessors/label_encoder.pkl')\n",
    "        test_vec = best_vectorizer.transform(test_texts)\n",
    "        predictions = best_model.predict(test_vec)\n",
    "        pred_labels = le.inverse_transform(predictions)\n",
    "        \n",
    "        print(\"\\nTest predictions:\")\n",
    "        for text, pred in zip(test_texts, pred_labels):\n",
    "            print(f\"  Text: {text[:50]}...\")\n",
    "            print(f\"  Prediction: {pred}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Test prediction failed: {e}\")\n",
    "\n",
    "print(\"\\nüöÄ Ready for deployment!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba73f4c7-9989-44bc-89ec-f56f3a85a883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATA QUALITY DIAGNOSIS - FINDING THE PROBLEM\n",
      "======================================================================\n",
      "\n",
      "1. BASIC DATA INFO:\n",
      "======================================================================\n",
      "Shape: (2838, 117)\n",
      "Columns: ['subreddit', 'post_id', 'sentence_range', 'text', 'id', 'label', 'confidence', 'social_timestamp', 'social_karma', 'syntax_ari', 'lex_liwc_WC', 'lex_liwc_Analytic', 'lex_liwc_Clout', 'lex_liwc_Authentic', 'lex_liwc_Tone', 'lex_liwc_WPS', 'lex_liwc_Sixltr', 'lex_liwc_Dic', 'lex_liwc_function', 'lex_liwc_pronoun', 'lex_liwc_ppron', 'lex_liwc_i', 'lex_liwc_we', 'lex_liwc_you', 'lex_liwc_shehe', 'lex_liwc_they', 'lex_liwc_ipron', 'lex_liwc_article', 'lex_liwc_prep', 'lex_liwc_auxverb', 'lex_liwc_adverb', 'lex_liwc_conj', 'lex_liwc_negate', 'lex_liwc_verb', 'lex_liwc_adj', 'lex_liwc_compare', 'lex_liwc_interrog', 'lex_liwc_number', 'lex_liwc_quant', 'lex_liwc_affect', 'lex_liwc_posemo', 'lex_liwc_negemo', 'lex_liwc_anx', 'lex_liwc_anger', 'lex_liwc_sad', 'lex_liwc_social', 'lex_liwc_family', 'lex_liwc_friend', 'lex_liwc_female', 'lex_liwc_male', 'lex_liwc_cogproc', 'lex_liwc_insight', 'lex_liwc_cause', 'lex_liwc_discrep', 'lex_liwc_tentat', 'lex_liwc_certain', 'lex_liwc_differ', 'lex_liwc_percept', 'lex_liwc_see', 'lex_liwc_hear', 'lex_liwc_feel', 'lex_liwc_bio', 'lex_liwc_body', 'lex_liwc_health', 'lex_liwc_sexual', 'lex_liwc_ingest', 'lex_liwc_drives', 'lex_liwc_affiliation', 'lex_liwc_achieve', 'lex_liwc_power', 'lex_liwc_reward', 'lex_liwc_risk', 'lex_liwc_focuspast', 'lex_liwc_focuspresent', 'lex_liwc_focusfuture', 'lex_liwc_relativ', 'lex_liwc_motion', 'lex_liwc_space', 'lex_liwc_time', 'lex_liwc_work', 'lex_liwc_leisure', 'lex_liwc_home', 'lex_liwc_money', 'lex_liwc_relig', 'lex_liwc_death', 'lex_liwc_informal', 'lex_liwc_swear', 'lex_liwc_netspeak', 'lex_liwc_assent', 'lex_liwc_nonflu', 'lex_liwc_filler', 'lex_liwc_AllPunc', 'lex_liwc_Period', 'lex_liwc_Comma', 'lex_liwc_Colon', 'lex_liwc_SemiC', 'lex_liwc_QMark', 'lex_liwc_Exclam', 'lex_liwc_Dash', 'lex_liwc_Quote', 'lex_liwc_Apostro', 'lex_liwc_Parenth', 'lex_liwc_OtherP', 'lex_dal_max_pleasantness', 'lex_dal_max_activation', 'lex_dal_max_imagery', 'lex_dal_min_pleasantness', 'lex_dal_min_activation', 'lex_dal_min_imagery', 'lex_dal_avg_activation', 'lex_dal_avg_imagery', 'lex_dal_avg_pleasantness', 'social_upvote_ratio', 'social_num_comments', 'syntax_fk_grade', 'sentiment', 'clean_text']\n",
      "\n",
      "First few rows:\n",
      "          subreddit post_id sentence_range  \\\n",
      "0              ptsd  8601tu       (15, 20)   \n",
      "1        assistance  8lbrx9         (0, 5)   \n",
      "2              ptsd  9ch1zh       (15, 20)   \n",
      "3     relationships  7rorpp        [5, 10]   \n",
      "4  survivorsofabuse  9p2gbc         [0, 5]   \n",
      "\n",
      "                                                text     id  label  \\\n",
      "0  He said he had not felt that way before, sugge...  33181      1   \n",
      "1  Hey there r/assistance, Not sure if this is th...   2606      0   \n",
      "2  My mom then hit me with the newspaper and it s...  38816      1   \n",
      "3  until i met my new boyfriend, he is amazing, h...    239      1   \n",
      "4  October is Domestic Violence Awareness Month a...   1421      1   \n",
      "\n",
      "   confidence  social_timestamp  social_karma  syntax_ari  ...  \\\n",
      "0         0.8        1521614353             5    1.806818  ...   \n",
      "1         1.0        1527009817             4    9.429737  ...   \n",
      "2         0.8        1535935605             2    7.769821  ...   \n",
      "3         0.6        1516429555             0    2.667798  ...   \n",
      "4         0.8        1539809005            24    7.554238  ...   \n",
      "\n",
      "   lex_dal_min_activation  lex_dal_min_imagery  lex_dal_avg_activation  \\\n",
      "0                  1.1250                  1.0                 1.77000   \n",
      "1                  1.0000                  1.0                 1.69586   \n",
      "2                  1.1429                  1.0                 1.83088   \n",
      "3                  1.1250                  1.0                 1.75356   \n",
      "4                  1.1250                  1.0                 1.77644   \n",
      "\n",
      "   lex_dal_avg_imagery  lex_dal_avg_pleasantness  social_upvote_ratio  \\\n",
      "0              1.52211                   1.89556                 0.86   \n",
      "1              1.62045                   1.88919                 0.65   \n",
      "2              1.58108                   1.85828                 0.67   \n",
      "3              1.52114                   1.98848                 0.50   \n",
      "4              1.64872                   1.81456                 1.00   \n",
      "\n",
      "   social_num_comments  syntax_fk_grade  sentiment  \\\n",
      "0                    1         3.253573  -0.002742   \n",
      "1                    2         8.828316   0.292857   \n",
      "2                    0         7.841667   0.011894   \n",
      "3                    5         4.104027   0.141671   \n",
      "4                    1         7.910952  -0.204167   \n",
      "\n",
      "                                          clean_text  \n",
      "0  he said he had not felt that way before, sugge...  \n",
      "1  hey there assistance, not sure if this is the ...  \n",
      "2  my mom then hit me with the newspaper and it s...  \n",
      "3  until met my new boyfriend, he is amazing, he ...  \n",
      "4  october is domestic violence awareness month a...  \n",
      "\n",
      "[5 rows x 117 columns]\n",
      "\n",
      "2. LABEL DISTRIBUTION:\n",
      "======================================================================\n",
      "label\n",
      "1    1488\n",
      "0    1350\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class balance: label\n",
      "1    0.524313\n",
      "0    0.475687\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "3. TEXT QUALITY CHECK:\n",
      "======================================================================\n",
      "\n",
      "Sample clean texts:\n",
      "\n",
      "[1] he said he had not felt that way before, suggeted go rest and so .trigger ahead if youi are hypocondriac like me decide to look up feelings of doom in hopes of maybe getting sucked into some rabbit ho...\n",
      "\n",
      "[2] hey there assistance, not sure if this is the right place to post this. but here goes am currently student intern at sandia national labs and working on survey to help improve our marketing outreach e...\n",
      "\n",
      "[3] my mom then hit me with the newspaper and it shocked me that she would do this, she knows do not like play hitting, smacking, striking, hitting or violence of any sort on my person. do send out this v...\n",
      "\n",
      "[4] until met my new boyfriend, he is amazing, he is kind, he is sweet, he is good student, he likes the same things as me, my family likes him, and so on. but dont feel that passion that rush felt with m...\n",
      "\n",
      "[5] october is domestic violence awareness month and am domestic violence survivor who is still struggling, even after over four years. lately have been feeling very angry. angry that my abusive ex receiv...\n",
      "\n",
      "Text statistics:\n",
      "  Average length: 431.4 chars\n",
      "  Average words: 80.4\n",
      "  Min length: 0\n",
      "  Max length: 1589\n",
      "\n",
      "  ‚ö†Ô∏è  Very short texts (<10 chars): 4 (0.1%)\n",
      "\n",
      "4. CHECKING ORIGINAL DATA:\n",
      "======================================================================\n",
      "Could not load original data: [Errno 2] No such file or directory: 'data/stress.csv'\n",
      "\n",
      "5. RECOMMENDATION:\n",
      "======================================================================\n",
      "‚úì Text quality looks okay\n",
      "\n",
      "Possible issues:\n",
      "  1. Data is too small (only 2838 samples)\n",
      "  2. Classes are not well separated\n",
      "  3. Need better feature engineering\n",
      "  4. Need data augmentation\n",
      "\n",
      "======================================================================\n",
      "NEXT STEPS:\n",
      "======================================================================\n",
      "\n",
      "Please share:\n",
      "  1. Output from this diagnostic cell\n",
      "  2. What does your original data look like?\n",
      "  3. How many words are in clean_text on average?\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Cell 9: DATA QUALITY DIAGNOSIS\n",
    "# ===============================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from collections import Counter\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DATA QUALITY DIAGNOSIS - FINDING THE PROBLEM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load data\n",
    "stress_processed = pd.read_pickle('preprocessors/preprocessed_data.pkl')\n",
    "\n",
    "print(\"\\n1. BASIC DATA INFO:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Shape: {stress_processed.shape}\")\n",
    "print(f\"Columns: {list(stress_processed.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(stress_processed.head())\n",
    "\n",
    "print(\"\\n2. LABEL DISTRIBUTION:\")\n",
    "print(\"=\"*70)\n",
    "print(stress_processed['label'].value_counts())\n",
    "print(f\"\\nClass balance: {stress_processed['label'].value_counts(normalize=True)}\")\n",
    "\n",
    "print(\"\\n3. TEXT QUALITY CHECK:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nSample clean texts:\")\n",
    "for i, text in enumerate(stress_processed['clean_text'].head(5)):\n",
    "    print(f\"\\n[{i+1}] {text[:200]}...\")\n",
    "\n",
    "# Check text lengths\n",
    "stress_processed['text_length'] = stress_processed['clean_text'].astype(str).str.len()\n",
    "stress_processed['word_count'] = stress_processed['clean_text'].astype(str).str.split().str.len()\n",
    "\n",
    "print(f\"\\nText statistics:\")\n",
    "print(f\"  Average length: {stress_processed['text_length'].mean():.1f} chars\")\n",
    "print(f\"  Average words: {stress_processed['word_count'].mean():.1f}\")\n",
    "print(f\"  Min length: {stress_processed['text_length'].min()}\")\n",
    "print(f\"  Max length: {stress_processed['text_length'].max()}\")\n",
    "\n",
    "# Check for issues\n",
    "empty_texts = (stress_processed['text_length'] < 10).sum()\n",
    "print(f\"\\n  ‚ö†Ô∏è  Very short texts (<10 chars): {empty_texts} ({empty_texts/len(stress_processed)*100:.1f}%)\")\n",
    "\n",
    "if empty_texts > len(stress_processed) * 0.1:\n",
    "    print(\"  üî¥ PROBLEM: Too many empty/short texts!\")\n",
    "\n",
    "print(\"\\n4. CHECKING ORIGINAL DATA:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Try to load original data\n",
    "try:\n",
    "    original = pd.read_csv('data/stress.csv')\n",
    "    print(f\"Original data shape: {original.shape}\")\n",
    "    print(f\"Original columns: {list(original.columns)}\")\n",
    "    print(f\"\\nOriginal label distribution:\")\n",
    "    print(original['label'].value_counts())\n",
    "    \n",
    "    # Find text column\n",
    "    text_col = None\n",
    "    for col in original.columns:\n",
    "        if col.lower() in ['text', 'post_text', 'content', 'message', 'tweet']:\n",
    "            text_col = col\n",
    "            break\n",
    "    \n",
    "    if text_col:\n",
    "        print(f\"\\nOriginal text samples from '{text_col}':\")\n",
    "        for i, text in enumerate(original[text_col].head(3)):\n",
    "            print(f\"\\n[{i+1}] {str(text)[:200]}...\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  Could not find text column!\")\n",
    "        print(\"Available columns:\", original.columns.tolist())\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Could not load original data: {e}\")\n",
    "\n",
    "print(\"\\n5. RECOMMENDATION:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Analyze the problem\n",
    "avg_words = stress_processed['word_count'].mean()\n",
    "\n",
    "if avg_words < 5:\n",
    "    print(\"üî¥ CRITICAL ISSUE: Text preprocessing removed too much!\")\n",
    "    print(\"\\nSOLUTION:\")\n",
    "    print(\"  1. Check your Cell 3 preprocessing\")\n",
    "    print(\"  2. Make sure 'clean_text' column has actual text\")\n",
    "    print(\"  3. Reduce stopword removal\")\n",
    "    print(\"  4. Keep more words (don't filter too aggressively)\")\n",
    "    \n",
    "elif empty_texts > len(stress_processed) * 0.2:\n",
    "    print(\"üî¥ CRITICAL ISSUE: Too many empty texts!\")\n",
    "    print(\"\\nSOLUTION:\")\n",
    "    print(\"  1. Check for NaN values in text\")\n",
    "    print(\"  2. Fill missing values before preprocessing\")\n",
    "    print(\"  3. Don't drop rows with short text\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚úì Text quality looks okay\")\n",
    "    print(\"\\nPossible issues:\")\n",
    "    print(\"  1. Data is too small (only 2838 samples)\")\n",
    "    print(\"  2. Classes are not well separated\")\n",
    "    print(\"  3. Need better feature engineering\")\n",
    "    print(\"  4. Need data augmentation\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NEXT STEPS:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nPlease share:\")\n",
    "print(\"  1. Output from this diagnostic cell\")\n",
    "print(\"  2. What does your original data look like?\")\n",
    "print(\"  3. How many words are in clean_text on average?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8773ae61-c886-45d8-9664-2191c55c11af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FUSION ENSEMBLE + FLASK API - PUBLICATION READY\n",
      "======================================================================\n",
      "\n",
      "1. Loading data...\n",
      "‚úì Created label encoder: [0 1]\n",
      "‚úì Data loaded: Train=2270, Test=568\n",
      "\n",
      "2. Building TOP 4 models from your 77.8% results...\n",
      "\n",
      "   [1/4] BaggingSGD + FeatureUnion (77.89%)\n",
      "      Training... ‚úì Features: 15000\n",
      "\n",
      "   [2/4] LogisticRegression + TF-IDF (77.75%)\n",
      "      Training... ‚úì Features: 15000\n",
      "\n",
      "   [3/4] BaggingSGD + TF-IDF (77.75%)\n",
      "      Training... ‚úì Features: 15000\n",
      "\n",
      "   [4/4] RidgeClassifier + TF-IDF (77.61%)\n",
      "      Training... ‚úì Features: 15000\n",
      "\n",
      "3. Creating FUSION ensemble...\n",
      "‚úì Fusion ensemble created with 4 top models\n",
      "\n",
      "4. Evaluating fusion ensemble...\n",
      "\n",
      "======================================================================\n",
      "üèÜ FUSION ENSEMBLE RESULTS\n",
      "======================================================================\n",
      "Accuracy:  0.7465 (74.65%)\n",
      "F1 Score:  0.7466\n",
      "Precision: 0.7470\n",
      "Recall:    0.7465\n",
      "\n",
      "üìä Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.75      0.74       270\n",
      "           1       0.77      0.74      0.76       298\n",
      "\n",
      "    accuracy                           0.75       568\n",
      "   macro avg       0.75      0.75      0.75       568\n",
      "weighted avg       0.75      0.75      0.75       568\n",
      "\n",
      "\n",
      "üìà Individual Model Performance:\n",
      "  Model 1: Acc=0.7377, F1=0.7378\n",
      "  Model 2: Acc=0.7570, F1=0.7569\n",
      "  Model 3: Acc=0.7342, F1=0.7343\n",
      "  Model 4: Acc=0.7623, F1=0.7623\n",
      "\n",
      "‚ú® Fusion Improvement: +-0.0318 F1 score\n",
      "\n",
      "5. Saving fusion ensemble...\n",
      "‚úì Saved: models/fusion_ensemble.pkl\n",
      "‚úì Saved: models/label_encoder.pkl\n",
      "‚úì Saved: models/fusion_ensemble_info.json\n",
      "\n",
      "6. Testing with sample texts...\n",
      "\n",
      "Text: I feel so stressed and overwhelmed with work...\n",
      "Prediction: 1 (Confidence: 83.04%)\n",
      "\n",
      "Text: Today is a beautiful day, feeling great!...\n",
      "Prediction: 0 (Confidence: 75.45%)\n",
      "\n",
      "Text: Having panic attacks and can't sleep at all...\n",
      "Prediction: 1 (Confidence: 68.63%)\n",
      "\n",
      "Text: Everything is going well in my life...\n",
      "Prediction: 1 (Confidence: 52.18%)\n",
      "\n",
      "======================================================================\n",
      "‚úÖ READY! Now create api.py and frontend.html\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Cell 11: FIXED FUSION ENSEMBLE + FLASK API\n",
    "# ===============================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FUSION ENSEMBLE + FLASK API - PUBLICATION READY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ===============================\n",
    "# Load Data\n",
    "# ===============================\n",
    "\n",
    "print(\"\\n1. Loading data...\")\n",
    "stress_processed = pd.read_pickle('preprocessors/preprocessed_data.pkl')\n",
    "X = stress_processed['clean_text'].values\n",
    "y = stress_processed['label'].values\n",
    "\n",
    "# Load or create label encoder\n",
    "try:\n",
    "    le = joblib.load('models/label_encoder.pkl')\n",
    "    print(f\"‚úì Loaded label encoder: {le.classes_}\")\n",
    "except:\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    joblib.dump(le, 'models/label_encoder.pkl')\n",
    "    print(f\"‚úì Created label encoder: {le.classes_}\")\n",
    "\n",
    "# If labels are strings, encode them\n",
    "if y.dtype == 'object':\n",
    "    y = le.transform(y)\n",
    "\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"‚úì Data loaded: Train={len(X_train_text)}, Test={len(X_test_text)}\")\n",
    "\n",
    "# ===============================\n",
    "# FIXED FUSION ENSEMBLE CLASS\n",
    "# ===============================\n",
    "\n",
    "class FusionEnsemble:\n",
    "    \"\"\"\n",
    "    Fusion ensemble that combines multiple model-vectorizer pairs\n",
    "    Each model gets its own vectorizer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = []\n",
    "        self.weights = []\n",
    "        \n",
    "    def add_model(self, model, vectorizer, weight=1.0):\n",
    "        \"\"\"Add a model-vectorizer pair with weight\"\"\"\n",
    "        self.models.append({\n",
    "            'model': model,\n",
    "            'vectorizer': vectorizer,\n",
    "            'weight': weight\n",
    "        })\n",
    "        self.weights.append(weight)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit all models with their respective vectorizers\"\"\"\n",
    "        print(f\"\\nTraining {len(self.models)} models in fusion ensemble...\")\n",
    "        \n",
    "        for i, model_dict in enumerate(self.models):\n",
    "            print(f\"  [{i+1}/{len(self.models)}] Training...\", end=\" \", flush=True)\n",
    "            \n",
    "            # Transform with this model's vectorizer\n",
    "            X_vec = model_dict['vectorizer'].fit_transform(X)\n",
    "            \n",
    "            # Train this model\n",
    "            model_dict['model'].fit(X_vec, y)\n",
    "            \n",
    "            print(f\"‚úì Features: {X_vec.shape[1]}\")\n",
    "        \n",
    "        print(\"‚úì Fusion ensemble training complete!\")\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, texts):\n",
    "        \"\"\"Get probability predictions from all models\"\"\"\n",
    "        all_predictions = []\n",
    "        \n",
    "        for model_dict in self.models:\n",
    "            model = model_dict['model']\n",
    "            vectorizer = model_dict['vectorizer']\n",
    "            weight = model_dict['weight']\n",
    "            \n",
    "            # Transform with THIS model's vectorizer\n",
    "            X_vec = vectorizer.transform(texts)\n",
    "            \n",
    "            # Get predictions\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                proba = model.predict_proba(X_vec)\n",
    "            elif hasattr(model, 'decision_function'):\n",
    "                # For SVM\n",
    "                decision = model.decision_function(X_vec)\n",
    "                if decision.ndim == 1:\n",
    "                    decision = np.column_stack([-decision, decision])\n",
    "                # Convert to probabilities\n",
    "                from scipy.special import softmax\n",
    "                proba = softmax(decision, axis=1)\n",
    "            else:\n",
    "                # Fallback: use hard predictions\n",
    "                pred = model.predict(X_vec)\n",
    "                proba = np.zeros((len(pred), len(np.unique(pred))))\n",
    "                proba[np.arange(len(pred)), pred] = 1.0\n",
    "            \n",
    "            # Weight the predictions\n",
    "            all_predictions.append(proba * weight)\n",
    "        \n",
    "        # Average weighted predictions\n",
    "        avg_predictions = np.sum(all_predictions, axis=0) / np.sum(self.weights)\n",
    "        return avg_predictions\n",
    "    \n",
    "    def predict(self, texts):\n",
    "        \"\"\"Get class predictions\"\"\"\n",
    "        probas = self.predict_proba(texts)\n",
    "        return np.argmax(probas, axis=1)\n",
    "\n",
    "# ===============================\n",
    "# BUILD TOP 4 MODELS FROM YOUR RESULTS\n",
    "# ===============================\n",
    "\n",
    "print(\"\\n2. Building TOP 4 models from your 77.8% results...\")\n",
    "\n",
    "# Model 1: BaggingSGD + FeatureUnion (77.89% - BEST)\n",
    "print(\"\\n   [1/4] BaggingSGD + FeatureUnion (77.89%)\")\n",
    "vectorizer_1 = FeatureUnion([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        max_features=10000,\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_df=0.9,\n",
    "        sublinear_tf=False\n",
    "    )),\n",
    "    ('count', TfidfVectorizer(\n",
    "        max_features=5000,\n",
    "        ngram_range=(1, 1),\n",
    "        min_df=2,\n",
    "        max_df=0.9,\n",
    "        sublinear_tf=False\n",
    "    ))\n",
    "])\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "model_1 = BaggingClassifier(\n",
    "    estimator=SGDClassifier(max_iter=1000, random_state=42, class_weight='balanced'),\n",
    "    n_estimators=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(f\"      Training...\", end=\" \", flush=True)\n",
    "X_train_1 = vectorizer_1.fit_transform(X_train_text)\n",
    "model_1.fit(X_train_1, y_train)\n",
    "print(f\"‚úì Features: {X_train_1.shape[1]}\")\n",
    "\n",
    "# Model 2: LogisticRegression + TfidfVectorizer (77.75%)\n",
    "print(\"\\n   [2/4] LogisticRegression + TF-IDF (77.75%)\")\n",
    "vectorizer_2 = TfidfVectorizer(\n",
    "    max_features=15000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.9,\n",
    "    sublinear_tf=False\n",
    ")\n",
    "model_2 = LogisticRegression(\n",
    "    C=1.0,\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(f\"      Training...\", end=\" \", flush=True)\n",
    "X_train_2 = vectorizer_2.fit_transform(X_train_text)\n",
    "model_2.fit(X_train_2, y_train)\n",
    "print(f\"‚úì Features: {X_train_2.shape[1]}\")\n",
    "\n",
    "# Model 3: BaggingSGD + TfidfVectorizer (77.75%)\n",
    "print(\"\\n   [3/4] BaggingSGD + TF-IDF (77.75%)\")\n",
    "vectorizer_3 = TfidfVectorizer(\n",
    "    max_features=15000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.9,\n",
    "    sublinear_tf=False\n",
    ")\n",
    "model_3 = BaggingClassifier(\n",
    "    estimator=SGDClassifier(max_iter=1000, random_state=42, class_weight='balanced'),\n",
    "    n_estimators=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(f\"      Training...\", end=\" \", flush=True)\n",
    "X_train_3 = vectorizer_3.fit_transform(X_train_text)\n",
    "model_3.fit(X_train_3, y_train)\n",
    "print(f\"‚úì Features: {X_train_3.shape[1]}\")\n",
    "\n",
    "# Model 4: RidgeClassifier + TfidfVectorizer (77.61%)\n",
    "print(\"\\n   [4/4] RidgeClassifier + TF-IDF (77.61%)\")\n",
    "vectorizer_4 = TfidfVectorizer(\n",
    "    max_features=15000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.9,\n",
    "    sublinear_tf=False\n",
    ")\n",
    "model_4 = RidgeClassifier(\n",
    "    alpha=1.0,\n",
    "    random_state=42,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "print(f\"      Training...\", end=\" \", flush=True)\n",
    "X_train_4 = vectorizer_4.fit_transform(X_train_text)\n",
    "model_4.fit(X_train_4, y_train)\n",
    "print(f\"‚úì Features: {X_train_4.shape[1]}\")\n",
    "\n",
    "# ===============================\n",
    "# CREATE FUSION ENSEMBLE\n",
    "# ===============================\n",
    "\n",
    "print(\"\\n3. Creating FUSION ensemble...\")\n",
    "\n",
    "fusion_model = FusionEnsemble()\n",
    "\n",
    "# Add models with weights (based on performance)\n",
    "fusion_model.add_model(model_1, vectorizer_1, weight=1.0)  # 77.89% - highest weight\n",
    "fusion_model.add_model(model_2, vectorizer_2, weight=0.98)  # 77.75%\n",
    "fusion_model.add_model(model_3, vectorizer_3, weight=0.98)  # 77.75%\n",
    "fusion_model.add_model(model_4, vectorizer_4, weight=0.97)  # 77.61%\n",
    "\n",
    "print(\"‚úì Fusion ensemble created with 4 top models\")\n",
    "\n",
    "# ===============================\n",
    "# EVALUATE FUSION ENSEMBLE\n",
    "# ===============================\n",
    "\n",
    "print(\"\\n4. Evaluating fusion ensemble...\")\n",
    "\n",
    "y_pred_fusion = fusion_model.predict(X_test_text)\n",
    "y_proba_fusion = fusion_model.predict_proba(X_test_text)\n",
    "\n",
    "fusion_acc = accuracy_score(y_test, y_pred_fusion)\n",
    "fusion_f1 = f1_score(y_test, y_pred_fusion, average='weighted')\n",
    "fusion_prec = precision_score(y_test, y_pred_fusion, average='weighted')\n",
    "fusion_rec = recall_score(y_test, y_pred_fusion, average='weighted')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üèÜ FUSION ENSEMBLE RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Accuracy:  {fusion_acc:.4f} ({fusion_acc*100:.2f}%)\")\n",
    "print(f\"F1 Score:  {fusion_f1:.4f}\")\n",
    "print(f\"Precision: {fusion_prec:.4f}\")\n",
    "print(f\"Recall:    {fusion_rec:.4f}\")\n",
    "\n",
    "print(\"\\nüìä Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_fusion, target_names=le.classes_.astype(str)))\n",
    "\n",
    "# Compare with individual models\n",
    "print(\"\\nüìà Individual Model Performance:\")\n",
    "for i in range(4):\n",
    "    X_test_i = fusion_model.models[i]['vectorizer'].transform(X_test_text)\n",
    "    y_pred_i = fusion_model.models[i]['model'].predict(X_test_i)\n",
    "    acc_i = accuracy_score(y_test, y_pred_i)\n",
    "    f1_i = f1_score(y_test, y_pred_i, average='weighted')\n",
    "    print(f\"  Model {i+1}: Acc={acc_i:.4f}, F1={f1_i:.4f}\")\n",
    "\n",
    "print(f\"\\n‚ú® Fusion Improvement: +{(fusion_f1 - 0.7784):.4f} F1 score\")\n",
    "\n",
    "# ===============================\n",
    "# SAVE FUSION MODEL\n",
    "# ===============================\n",
    "\n",
    "print(\"\\n5. Saving fusion ensemble...\")\n",
    "\n",
    "joblib.dump(fusion_model, 'models/fusion_ensemble.pkl')\n",
    "joblib.dump(le, 'models/label_encoder.pkl')  # Save label encoder\n",
    "print(\"‚úì Saved: models/fusion_ensemble.pkl\")\n",
    "print(\"‚úì Saved: models/label_encoder.pkl\")\n",
    "\n",
    "import json\n",
    "fusion_info = {\n",
    "    'model_type': 'Fusion Ensemble',\n",
    "    'num_models': 4,\n",
    "    'models': [\n",
    "        'BaggingSGD + FeatureUnion',\n",
    "        'LogisticRegression + TF-IDF',\n",
    "        'BaggingSGD + TF-IDF',\n",
    "        'RidgeClassifier + TF-IDF'\n",
    "    ],\n",
    "    'performance': {\n",
    "        'accuracy': float(fusion_acc),\n",
    "        'f1_score': float(fusion_f1),\n",
    "        'precision': float(fusion_prec),\n",
    "        'recall': float(fusion_rec)\n",
    "    },\n",
    "    'labels': le.classes_.tolist(),\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open('models/fusion_ensemble_info.json', 'w') as f:\n",
    "    json.dump(fusion_info, f, indent=2)\n",
    "print(\"‚úì Saved: models/fusion_ensemble_info.json\")\n",
    "\n",
    "# Test predictions\n",
    "print(\"\\n6. Testing with sample texts...\")\n",
    "test_samples = [\n",
    "    \"I feel so stressed and overwhelmed with work\",\n",
    "    \"Today is a beautiful day, feeling great!\",\n",
    "    \"Having panic attacks and can't sleep at all\",\n",
    "    \"Everything is going well in my life\"\n",
    "]\n",
    "\n",
    "for text in test_samples:\n",
    "    pred_idx = fusion_model.predict([text])[0]\n",
    "    proba = fusion_model.predict_proba([text])[0]\n",
    "    pred_label = le.inverse_transform([pred_idx])[0]\n",
    "    confidence = proba[pred_idx]\n",
    "    print(f\"\\nText: {text[:60]}...\")\n",
    "    print(f\"Prediction: {pred_label} (Confidence: {confidence:.2%})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ READY! Now create api.py and frontend.html\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a71ad8a-3043-4076-9cac-cfb50c335e89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
